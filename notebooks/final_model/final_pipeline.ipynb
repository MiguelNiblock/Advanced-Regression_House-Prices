{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "[1. Loading Data](#1.-Loading-Data)\n",
    "\n",
    "[2. Exploratory Data Analysis](#2.-Exploratory-Data-Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading Data\n",
    "\n",
    "- Get data into notebook in the best form possible for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "import cyrtranslit\n",
    "from sklearn import preprocessing, model_selection, metrics, feature_selection, ensemble, linear_model, cross_decomposition, feature_extraction, decomposition, compose\n",
    "from scipy import stats\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "color = sns.color_palette()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis\n",
    "\n",
    "- What’s happening with data.\n",
    "    - It's big.\n",
    "    - It's in Russian with Cyrillic alphabet.\n",
    "\n",
    "- Why it’s interesting\n",
    "    - Outcome variable is very non-normal. There's three distinct groups. (Zero Range, Lower Range, Upper Range)\n",
    "    \n",
    "- What features you intend to take advantage of for your modeling.\n",
    "    - Titles and descriptions offer endless NLP opportunities.\n",
    "    - Plenty of categorical data to binarize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pipeline\n",
    "\n",
    "- Linked stages.\n",
    "- Efficient and easy to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Cross-Decomposition of TF-IDF Vectors with BiGrams\n",
    "\n",
    "This process consists of extracting term frequency vectors using the text in each ad as a document. Tokens for unigrams and bigrams will be included in this stage. Lastly, the resulting matrix will be reduced to the smallest number of components that retain all potential predictive power. Perform onto both titles and descriptions and retain separate components for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and describe engineered features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Discretized Vector Cross-Decomposition\n",
    "\n",
    "This consists of splitting the dependent variable into discrete ranges and creating a vocabulary for each range. Then vectorize and cross-decompose each vocabulary independently. Resulting components for each vocabulary will reflect the presence of terms common in a certain discrete range of target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and describe engineered features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Discretized Vector Sums\n",
    "\n",
    "Similar to previous procedure, vocabularies are created for discrete ranges of target. However instead of decomposing the vectors of those vocabularies, you simply sum their frequencies along the row axis of the term frequency matrix. This results in a single variable for each vocabulary, which represents the aggregate frequency of a vocabulary's terms per ad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and describe engineered features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Categorical Features\n",
    "\n",
    "Categorical variables with small numbers of unique values are easy to process. However, several variables in this data have thousands of unique values which would increase the dimensional space unreasonably. Based on the potential predictive power of each categorical variable, a dependent number of its features can be selected based on their tree-derived `feature_importance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and describe engineered features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Other Features\n",
    "\n",
    "### Sentiment Analysis\n",
    "\n",
    "An NLP library called `polyglot` offers multi-language tools, such as Sentiment-Analysis and Named-Entity-Recognition in Russian.\n",
    "\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and describe engineered features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation\n",
    "\n",
    "- Evaluation and comparison of multiple models via robust analysis of residuals and error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Product\n",
    "\n",
    "- Why chose it.\n",
    "- Why works.\n",
    "- What problem it solves.\n",
    "- How will it run in a production environment.\n",
    "- What to do to maintain it going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
