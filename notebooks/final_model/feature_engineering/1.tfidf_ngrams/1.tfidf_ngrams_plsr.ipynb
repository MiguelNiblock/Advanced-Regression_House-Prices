{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Decomposition of TF-IDF Vectors with BiGrams\n",
    "\n",
    "- Apply to train and test data.\n",
    "- Retain separate sets of components for titles and descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn import preprocessing, model_selection, metrics, feature_selection, ensemble, linear_model, cross_decomposition, feature_extraction, decomposition\n",
    "import time\n",
    "from sklearn.externals import joblib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../../train.pkl',compression='zip')\n",
    "test = pd.read_pickle('../../test.pkl',compression='zip')\n",
    "# Russian stopwords\n",
    "ru_stop = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'title'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N tokens: 67138\n"
     ]
    }
   ],
   "source": [
    "vec = feature_extraction.text.TfidfVectorizer(\n",
    "    stop_words=ru_stop,\n",
    "    lowercase=False,\n",
    "    ngram_range=(1,2),\n",
    "    min_df=0.000005)\n",
    "# Fitting on train and test as merged lists\n",
    "vec.fit(train[var].astype(str).tolist() + test[var].astype(str).tolist())\n",
    "print('N tokens:',len(vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### vectors for train and test\n",
    "counts_train = vec.transform(train[var].astype(str).tolist())\n",
    "counts_test = vec.transform(test[var].astype(str).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### To start from zero...\n",
    "reduced_train = pd.DataFrame(index=train.index)\n",
    "reduced_test = pd.DataFrame(index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: 0-1500\n",
      "Prelim score for column range: 0.009183509675312895\n",
      "Aggregate cv: [0.00505399 0.00456241 0.00495345 0.0049719 ]\n",
      "Columns: 1500-3000\n",
      "Prelim score for column range: 0.020489688454986776\n",
      "Aggregate cv: [0.01907627 0.01772109 0.01797458 0.01845972]\n",
      "Columns: 3000-4500\n",
      "Prelim score for column range: 0.009821800559869809\n",
      "Aggregate cv: [0.02258087 0.02117297 0.02089647 0.02195212]\n",
      "Columns: 4500-6000\n",
      "Prelim score for column range: 0.00799019550929092\n",
      "Aggregate cv: [0.02426916 0.02302251 0.02233436 0.02385494]\n",
      "Columns: 6000-7500\n",
      "Prelim score for column range: 0.00797682269629596\n",
      "Aggregate cv: [0.02591112 0.02443876 0.02357445 0.02554189]\n",
      "Columns: 7500-9000\n",
      "Prelim score for column range: 0.008547466768183076\n",
      "Aggregate cv: [0.02748956 0.02611181 0.02527265 0.02740797]\n",
      "Columns: 9000-10500\n",
      "Prelim score for column range: 0.012916359682723089\n",
      "Aggregate cv: [0.0305337  0.02883387 0.02845827 0.0305729 ]\n",
      "Columns: 10500-12000\n",
      "Prelim score for column range: 0.008462633644602868\n",
      "Aggregate cv: [0.03168803 0.02980779 0.02961256 0.03163865]\n",
      "Columns: 12000-13500\n",
      "Prelim score for column range: 0.008490065554429549\n",
      "Aggregate cv: [0.03301736 0.03130514 0.03103282 0.03304503]\n",
      "Columns: 13500-15000\n",
      "Prelim score for column range: 0.011135035494032053\n",
      "Aggregate cv: [0.03476746 0.03236469 0.03266986 0.03466146]\n",
      "Decomposing Aggregate...\n",
      "Aggregate cv after decomposition: [0.03482965 0.03245686 0.03275864 0.03476629]\n",
      "Columns: 15000-16500\n",
      "Prelim score for column range: 0.007388591659455823\n",
      "Aggregate cv: [0.0358007  0.03356699 0.03397097 0.03591676]\n",
      "Columns: 16500-18000\n",
      "Prelim score for column range: 0.010393200573272419\n",
      "Aggregate cv: [0.03752404 0.03556725 0.03576039 0.03757676]\n",
      "Columns: 18000-19500\n",
      "Prelim score for column range: 0.010654428093213042\n",
      "Aggregate cv: [0.04111547 0.03876667 0.03942854 0.04072825]\n",
      "Columns: 19500-21000\n",
      "Prelim score for column range: 0.010501658131136016\n",
      "Aggregate cv: [0.04656357 0.04396358 0.04454363 0.0462981 ]\n",
      "Columns: 21000-22500\n",
      "Prelim score for column range: 0.023039427139547985\n",
      "Aggregate cv: [0.05458914 0.05260322 0.05317933 0.05458534]\n",
      "Columns: 22500-24000\n",
      "Prelim score for column range: 0.016670177516514784\n",
      "Aggregate cv: [0.06406939 0.06252613 0.06277965 0.06557126]\n",
      "Columns: 24000-25500\n",
      "Prelim score for column range: 0.010801068151430225\n",
      "Aggregate cv: [0.0693559  0.06736187 0.06869506 0.07133906]\n",
      "Columns: 25500-27000\n",
      "Prelim score for column range: 0.00959854171017005\n",
      "Aggregate cv: [0.07319274 0.07087992 0.0728836  0.07537004]\n",
      "Columns: 27000-28500\n",
      "Prelim score for column range: 0.014218613693783189\n",
      "Aggregate cv: [0.08195507 0.07949673 0.08184757 0.08386216]\n",
      "Columns: 28500-30000\n",
      "Prelim score for column range: 0.010127308536992287\n",
      "Aggregate cv: [0.0869508  0.0842824  0.08717573 0.08879253]\n",
      "Decomposing Aggregate...\n",
      "Aggregate cv after decomposition: [0.0870656  0.08437072 0.08732279 0.08892246]\n",
      "Columns: 30000-31500\n",
      "Prelim score for column range: 0.012492693210385708\n",
      "Aggregate cv: [0.09366693 0.09109017 0.09389078 0.09528359]\n",
      "Columns: 31500-33000\n",
      "Prelim score for column range: 0.0114370403540196\n",
      "Aggregate cv: [0.0989882  0.09597521 0.09893727 0.10035367]\n",
      "Columns: 33000-34500\n",
      "Prelim score for column range: 0.015520510126784348\n",
      "Aggregate cv: [0.1098589  0.1055464  0.10969782 0.11099103]\n",
      "Columns: 34500-36000\n",
      "Prelim score for column range: 0.013873719249604788\n",
      "Aggregate cv: [0.11693373 0.11229655 0.11550842 0.11761275]\n",
      "Columns: 36000-37500\n",
      "Prelim score for column range: 0.012482170215886268\n",
      "Aggregate cv: [0.1222626  0.11839156 0.12144507 0.12368712]\n",
      "Columns: 37500-39000\n",
      "Prelim score for column range: 0.01722340985160764\n",
      "Aggregate cv: [0.1334775  0.12932858 0.13277596 0.13427954]\n",
      "Columns: 39000-40500\n",
      "Prelim score for column range: 0.014011738484536074\n",
      "Aggregate cv: [0.14103444 0.13719613 0.14077655 0.1430063 ]\n",
      "Columns: 40500-42000\n",
      "Prelim score for column range: 0.01519516188666703\n",
      "Aggregate cv: [0.14990887 0.14566457 0.1492338  0.15211848]\n",
      "Columns: 42000-43500\n",
      "Prelim score for column range: 0.013063989206937188\n",
      "Aggregate cv: [0.15721558 0.15193626 0.15602163 0.15859548]\n",
      "Columns: 43500-45000\n",
      "Prelim score for column range: 0.01371027683149817\n",
      "Aggregate cv: [0.16172656 0.15618745 0.16095266 0.1625418 ]\n",
      "Decomposing Aggregate...\n",
      "Aggregate cv after decomposition: [0.16183964 0.15630355 0.16109195 0.1626313 ]\n",
      "Columns: 45000-46500\n",
      "Prelim score for column range: 0.01264989341755929\n",
      "Aggregate cv: [0.16347885 0.15796932 0.16242392 0.16397939]\n",
      "Columns: 46500-48000\n",
      "Prelim score for column range: 0.015752052779369752\n",
      "Aggregate cv: [0.16562693 0.16036178 0.16407036 0.16611367]\n",
      "Columns: 48000-49500\n",
      "Prelim score for column range: 0.014863108617689469\n",
      "Aggregate cv: [0.16691836 0.16173835 0.16533615 0.16758908]\n",
      "Columns: 49500-51000\n",
      "Prelim score for column range: 0.010893484448548763\n",
      "Aggregate cv: [0.16787968 0.16270568 0.16635041 0.16869543]\n",
      "Columns: 51000-52500\n",
      "Prelim score for column range: 0.014966592606427964\n",
      "Aggregate cv: [0.16951242 0.16432215 0.16829723 0.17018276]\n",
      "Columns: 52500-54000\n",
      "Prelim score for column range: 0.0126772842860855\n",
      "Aggregate cv: [0.17098906 0.16557951 0.16977481 0.17156093]\n",
      "Columns: 54000-55500\n",
      "Prelim score for column range: 0.014626934110094414\n",
      "Aggregate cv: [0.17253042 0.16690256 0.17135673 0.17273024]\n",
      "Columns: 55500-57000\n",
      "Prelim score for column range: 0.010519407141330596\n",
      "Aggregate cv: [0.17350601 0.16784023 0.17234459 0.17373717]\n",
      "Columns: 57000-58500\n",
      "Prelim score for column range: 0.011745056832714429\n",
      "Aggregate cv: [0.1743532  0.16876392 0.17314099 0.17476779]\n",
      "Columns: 58500-60000\n",
      "Prelim score for column range: 0.013682106190634482\n",
      "Aggregate cv: [0.17550535 0.17028256 0.1746411  0.17607826]\n",
      "Decomposing Aggregate...\n",
      "Aggregate cv after decomposition: [0.17558695 0.17035819 0.17474684 0.17617805]\n",
      "Columns: 60000-61500\n",
      "Prelim score for column range: 0.015491674094736951\n",
      "Aggregate cv: [0.17681423 0.17126189 0.17561774 0.17746547]\n",
      "Columns: 61500-63000\n",
      "Prelim score for column range: 0.010685719787845938\n",
      "Aggregate cv: [0.17766326 0.17203679 0.17633762 0.17824416]\n",
      "Columns: 63000-64500\n",
      "Prelim score for column range: 0.014578818745921374\n",
      "Aggregate cv: [0.17883687 0.17353379 0.17777242 0.17950047]\n",
      "Columns: 64500-66000\n",
      "Prelim score for column range: 0.011591843160968396\n",
      "Aggregate cv: [0.18006346 0.17490881 0.1788604  0.1805137 ]\n",
      "Columns: 66000-67138\n",
      "Prelim score for column range: 0.009172436403075301\n",
      "Aggregate cv: [0.18092212 0.17574777 0.179737   0.18130798]\n",
      "Decomposing Aggregate...\n",
      "Aggregate cv after decomposition: [0.18097245 0.1757806  0.1797841  0.18141601]\n",
      "Minutes: 46.37753241459529\n"
     ]
    }
   ],
   "source": [
    "# Reduce all CSR values in batches\n",
    "t = time.time()\n",
    "start_col = 0\n",
    "varname = var[:5]\n",
    "compname = 'idfngram'\n",
    "##########################################\n",
    "n_cols = counts_train.shape[1]\n",
    "col_step = 1500\n",
    "col_end = n_cols + col_step\n",
    "##########################################\n",
    "# Start iteration with columns\n",
    "low_col = start_col\n",
    "for col in np.arange(0,col_end,col_step):\n",
    "    # Limiting the edge case of the last values\n",
    "    if col > n_cols:\n",
    "        col = n_cols\n",
    "    up_col = col\n",
    "    \n",
    "    if up_col > low_col:\n",
    "        ###########################################\n",
    "        # Train PLSR on a large sample of train vectors\n",
    "        print('Columns: {}-{}'.format(low_col,up_col))\n",
    "        index = np.random.choice(len(train),size=int(4e5))\n",
    "        sample = counts_train[index,low_col:up_col].toarray()\n",
    "        reduce = cross_decomposition.PLSRegression(n_components=5)\n",
    "        reduce.fit(sample,train.iloc[index].deal_probability)\n",
    "        print('Prelim score for column range:',reduce.score(sample,train.iloc[index].deal_probability))\n",
    "        ##########################################\n",
    "        # (TRAIN) Nested indexes iteration\n",
    "        # Initial values:\n",
    "        n_rows = len(train)\n",
    "        row_step = int(2.5e5)\n",
    "        row_end = n_rows + row_step\n",
    "        components = pd.DataFrame()\n",
    "        low_idx = 0\n",
    "        ###########\n",
    "        for idx in np.arange(0,row_end,row_step):\n",
    "            # Limiting the edge case of the last values\n",
    "            if idx > n_rows:\n",
    "                idx = n_rows\n",
    "            up_idx = idx\n",
    "\n",
    "            if up_idx > low_idx:\n",
    "                sample = counts_train[low_idx:up_idx,low_col:up_col].toarray()\n",
    "                sample = reduce.transform(sample)\n",
    "                components = components.append(pd.DataFrame(sample))\n",
    "                low_idx = idx\n",
    "        components.reset_index(drop=True,inplace=True)\n",
    "        components.columns = ['col_{}-{}_{}'.format(low_col,up_col,i) for i in range(0,5)]\n",
    "        reduced_train = reduced_train.join(components)\n",
    "        print('Aggregate cv:',model_selection.cross_val_score(\n",
    "            cv=4,estimator=linear_model.LinearRegression(),\n",
    "            X=reduced_train,y=train.deal_probability))\n",
    "        ###########################################\n",
    "        # (TEST) Nested indexes iteration\n",
    "        # Initial values:\n",
    "        n_rows = len(test)\n",
    "        row_step = int(2e5)\n",
    "        row_end = n_rows + row_step\n",
    "        components = pd.DataFrame()\n",
    "        low_idx = 0\n",
    "        ###########\n",
    "        for idx in np.arange(0,row_end,row_step):\n",
    "            if idx > n_rows:\n",
    "                idx = n_rows\n",
    "            up_idx = idx\n",
    "\n",
    "            if up_idx > low_idx:\n",
    "                sample = counts_test[low_idx:up_idx,low_col:up_col].toarray()\n",
    "                sample = reduce.transform(sample)\n",
    "                components = components.append(pd.DataFrame(sample))\n",
    "                low_idx = idx\n",
    "        components.reset_index(drop=True,inplace=True)\n",
    "        components.columns = ['col_{}-{}_{}'.format(low_col,up_col,i) for i in range(0,5)]\n",
    "        reduced_test = reduced_test.join(components)\n",
    "        #####################################\n",
    "        # Prepare for next column range\n",
    "        low_col = col     \n",
    "        #####################################    \n",
    "        # Decompose aggregate every n steps\n",
    "        if up_col%(col_step*10) == 0:\n",
    "            print('Decomposing Aggregate...')\n",
    "            reduce = cross_decomposition.PLSRegression(n_components=5)\n",
    "            reduce.fit(reduced_train,train.deal_probability)\n",
    "            reduced_train = pd.DataFrame(\n",
    "                reduce.transform(reduced_train),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,5)])\n",
    "            reduced_test = pd.DataFrame(\n",
    "                reduce.transform(reduced_test),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,5)])\n",
    "            print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "                cv=4,estimator=linear_model.LinearRegression(),\n",
    "                X=reduced_train,y=train.deal_probability))\n",
    "        #####################################    \n",
    "        # Save progress every n steps\n",
    "        if up_col%(col_step*5) == 0:\n",
    "            joblib.dump(reduced_train,'train_{}_{}.sav'.format(varname,compname))\n",
    "            joblib.dump(reduced_test,'test_{}_{}.sav'.format(varname,compname))   \n",
    "#####################################\n",
    "# One final round of decomposition to n components\n",
    "if reduced_train.shape[1] > 5:\n",
    "    print('Decomposing Aggregate...')\n",
    "    reduce = cross_decomposition.PLSRegression(n_components=5)\n",
    "    reduce.fit(reduced_train,train.deal_probability)\n",
    "    reduced_train = pd.DataFrame(\n",
    "        reduce.transform(reduced_train),\n",
    "        columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,5)])\n",
    "    reduced_test = pd.DataFrame(\n",
    "        reduce.transform(reduced_test),\n",
    "        columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,5)])\n",
    "    print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "        cv=4,estimator=linear_model.LinearRegression(),\n",
    "        X=reduced_train,y=train.deal_probability))\n",
    "#########\n",
    "# Last save to disk\n",
    "joblib.dump(reduced_train,'train_{}_{}.sav'.format(varname,compname))\n",
    "joblib.dump(reduced_test,'test_{}_{}.sav'.format(varname,compname))\n",
    "#########\n",
    "print('Minutes:',(time.time()-t)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    ">**YOU MAY RESTART THE KERNEL AT THIS POINT**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn import preprocessing, model_selection, metrics, feature_selection, ensemble, linear_model, cross_decomposition, feature_extraction, decomposition\n",
    "import time\n",
    "from sklearn.externals import joblib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../../train.pkl',compression='zip')\n",
    "test = pd.read_pickle('../../test.pkl',compression='zip')\n",
    "# Russian stopwords\n",
    "ru_stop = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'description'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N tokens: 64727\n"
     ]
    }
   ],
   "source": [
    "vec = feature_extraction.text.TfidfVectorizer(\n",
    "    stop_words=ru_stop,\n",
    "    lowercase=False,\n",
    "    ngram_range=(1,2),\n",
    "    min_df=0.00005)\n",
    "# Fitting on train and test as merged lists\n",
    "vec.fit(train[var].astype(str).tolist() + test[var].astype(str).tolist())\n",
    "print('N tokens:',len(vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### vectors for train and test\n",
    "counts_train = vec.transform(train[var].astype(str).tolist())\n",
    "counts_test = vec.transform(test[var].astype(str).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### To start from zero...\n",
    "reduced_train = pd.DataFrame(index=train.index)\n",
    "reduced_test = pd.DataFrame(index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: 0-1500\n",
      "Prelim score for column range: 0.01596561611580327\n",
      "Aggregate cv: [0.00961727 0.00979394 0.01005502 0.01009222]\n",
      "Columns: 1500-3000\n",
      "Prelim score for column range: 0.016929697326530047\n",
      "Aggregate cv: [0.0194269  0.01890985 0.01950827 0.01975085]\n",
      "Columns: 3000-4500\n",
      "Prelim score for column range: 0.012037765672628264\n",
      "Aggregate cv: [0.02341009 0.02303537 0.02318495 0.02416345]\n",
      "Columns: 4500-6000\n",
      "Prelim score for column range: 0.011475811444744853\n",
      "Aggregate cv: [0.02598355 0.02583479 0.02544778 0.02682626]\n",
      "Columns: 6000-7500\n",
      "Prelim score for column range: 0.02646974533710711\n",
      "Aggregate cv: [0.04077638 0.03936563 0.0406893  0.04193133]\n",
      "Columns: 7500-9000\n",
      "Prelim score for column range: 0.02310270429387984\n",
      "Aggregate cv: [0.05109357 0.04957796 0.05046476 0.05235976]\n",
      "Columns: 9000-10500\n",
      "Prelim score for column range: 0.02320084077895901\n",
      "Aggregate cv: [0.05817159 0.0564372  0.05811968 0.05922264]\n",
      "Columns: 10500-12000\n",
      "Prelim score for column range: 0.021032878066735594\n",
      "Aggregate cv: [0.06538714 0.06338197 0.06503895 0.06652223]\n",
      "Columns: 12000-13500\n",
      "Prelim score for column range: 0.019510602969580407\n",
      "Aggregate cv: [0.07097038 0.06885019 0.07005512 0.07162886]\n",
      "Columns: 13500-15000\n",
      "Prelim score for column range: 0.023584366270354207\n",
      "Aggregate cv: [0.07784183 0.07598597 0.07701587 0.07899712]\n",
      "Decomposing Aggregate...\n",
      "Aggregate cv after decomposition: [0.07785686 0.07597613 0.07702163 0.07900377]\n",
      "Columns: 15000-16500\n",
      "Prelim score for column range: 0.024074945589900528\n",
      "Aggregate cv: [0.08394229 0.08167667 0.08318555 0.08513233]\n",
      "Columns: 16500-18000\n",
      "Prelim score for column range: 0.037234600073262625\n",
      "Aggregate cv: [0.09651384 0.09462321 0.09674713 0.09858013]\n",
      "Columns: 18000-19500\n",
      "Prelim score for column range: 0.028181473538509083\n",
      "Aggregate cv: [0.10415572 0.10171513 0.10342172 0.1054183 ]\n",
      "Columns: 19500-21000\n",
      "Prelim score for column range: 0.020746279608265028\n",
      "Aggregate cv: [0.10824969 0.10534395 0.10666146 0.10891425]\n",
      "Columns: 21000-22500\n",
      "Prelim score for column range: 0.0197362930420707\n",
      "Aggregate cv: [0.11156262 0.10875677 0.10951947 0.11207396]\n",
      "Columns: 22500-24000\n",
      "Prelim score for column range: 0.02661339763356108\n",
      "Aggregate cv: [0.11512404 0.11197118 0.11329188 0.11576173]\n",
      "Columns: 24000-25500\n",
      "Prelim score for column range: 0.02651627491263253\n",
      "Aggregate cv: [0.11937222 0.11610572 0.1176932  0.12020828]\n",
      "Columns: 25500-27000\n",
      "Prelim score for column range: 0.0282207510303043\n",
      "Aggregate cv: [0.1230464  0.11987967 0.12184084 0.12430982]\n",
      "Columns: 27000-28500\n",
      "Prelim score for column range: 0.026864075435401032\n",
      "Aggregate cv: [0.12652415 0.12337581 0.12547176 0.128222  ]\n",
      "Columns: 28500-30000\n",
      "Prelim score for column range: 0.030899185791002926\n",
      "Aggregate cv: [0.13048437 0.12712524 0.12906938 0.13201376]\n",
      "Decomposing Aggregate...\n",
      "Aggregate cv after decomposition: [0.13044584 0.12706735 0.12907177 0.13199093]\n",
      "Columns: 30000-31500\n",
      "Prelim score for column range: 0.02068478456069167\n",
      "Aggregate cv: [0.13172947 0.12825445 0.13056159 0.13330329]\n",
      "Columns: 31500-33000\n",
      "Prelim score for column range: 0.023154452868762387\n",
      "Aggregate cv: [0.13305594 0.12943694 0.13188712 0.13441355]\n",
      "Columns: 33000-34500\n",
      "Prelim score for column range: 0.025599124976988907\n",
      "Aggregate cv: [0.13502862 0.13147526 0.13376575 0.13613249]\n",
      "Columns: 34500-36000\n",
      "Prelim score for column range: 0.03282393932273331\n",
      "Aggregate cv: [0.138289   0.13465957 0.13759922 0.13941965]\n",
      "Columns: 36000-37500\n",
      "Prelim score for column range: 0.02721987802660919\n",
      "Aggregate cv: [0.14007858 0.13644348 0.13955578 0.1411031 ]\n",
      "Columns: 37500-39000\n",
      "Prelim score for column range: 0.03353102857632784\n",
      "Aggregate cv: [0.14206471 0.13813521 0.14150346 0.14296244]\n",
      "Columns: 39000-40500\n",
      "Prelim score for column range: 0.027585967957008184\n",
      "Aggregate cv: [0.14378568 0.14007389 0.14354792 0.14511794]\n",
      "Columns: 40500-42000\n",
      "Prelim score for column range: 0.02125927763193325\n",
      "Aggregate cv: [0.14476837 0.1411102  0.14444689 0.1459409 ]\n",
      "Columns: 42000-43500\n",
      "Prelim score for column range: 0.023424001316971865\n",
      "Aggregate cv: [0.14594603 0.14227939 0.1456687  0.1469266 ]\n",
      "Columns: 43500-45000\n",
      "Prelim score for column range: 0.02194819684982041\n",
      "Aggregate cv: [0.14674765 0.14313616 0.14651528 0.14789298]\n",
      "Decomposing Aggregate...\n",
      "Aggregate cv after decomposition: [0.14658981 0.14297087 0.14643391 0.14765321]\n",
      "Columns: 45000-46500\n",
      "Prelim score for column range: 0.027600196428473445\n",
      "Aggregate cv: [0.14791697 0.14455664 0.14801427 0.14928319]\n",
      "Columns: 46500-48000\n",
      "Prelim score for column range: 0.0252383501859349\n",
      "Aggregate cv: [0.14862882 0.14518505 0.14854468 0.14994566]\n",
      "Columns: 48000-49500\n",
      "Prelim score for column range: 0.02005144690927685\n",
      "Aggregate cv: [0.14929684 0.14570116 0.14888645 0.15058832]\n",
      "Columns: 49500-51000\n",
      "Prelim score for column range: 0.021471297870328754\n",
      "Aggregate cv: [0.15004345 0.14642022 0.14981208 0.15116972]\n",
      "Columns: 51000-52500\n",
      "Prelim score for column range: 0.03531723623908234\n",
      "Aggregate cv: [0.1512313  0.14786234 0.15115503 0.15243024]\n",
      "Columns: 52500-54000\n",
      "Prelim score for column range: 0.03320544516317625\n",
      "Aggregate cv: [0.15243262 0.14904121 0.15241438 0.153824  ]\n",
      "Columns: 54000-55500\n",
      "Prelim score for column range: 0.028799014787419308\n",
      "Aggregate cv: [0.15311563 0.14960602 0.15294098 0.15447241]\n",
      "Columns: 55500-57000\n",
      "Prelim score for column range: 0.02226682710847927\n",
      "Aggregate cv: [0.1541467  0.15045144 0.15403692 0.15555746]\n",
      "Columns: 57000-58500\n",
      "Prelim score for column range: 0.030229015771169543\n",
      "Aggregate cv: [0.15506931 0.15143161 0.15486442 0.15635497]\n",
      "Columns: 58500-60000\n",
      "Prelim score for column range: 0.02861493890222322\n",
      "Aggregate cv: [0.15614226 0.1525276  0.15588742 0.15745265]\n",
      "Decomposing Aggregate...\n",
      "Aggregate cv after decomposition: [0.15586816 0.15224031 0.15564593 0.15722519]\n",
      "Columns: 60000-61500\n",
      "Prelim score for column range: 0.02354291139848519\n",
      "Aggregate cv: [0.15619575 0.15258486 0.15594146 0.15767157]\n",
      "Columns: 61500-63000\n",
      "Prelim score for column range: 0.028130434492499298\n",
      "Aggregate cv: [0.15750618 0.1541103  0.15733016 0.15903754]\n",
      "Columns: 63000-64500\n",
      "Prelim score for column range: 0.026301747151896926\n",
      "Aggregate cv: [0.15835492 0.15477208 0.15809114 0.15968282]\n",
      "Columns: 64500-64727\n",
      "Prelim score for column range: 0.002898063501226789\n",
      "Aggregate cv: [0.15850157 0.15489741 0.15819596 0.15976458]\n",
      "Decomposing Aggregate...\n",
      "Aggregate cv after decomposition: [0.15852626 0.15491565 0.15820387 0.15977386]\n",
      "Minutes: 47.0519127925237\n"
     ]
    }
   ],
   "source": [
    "# Reduce all CSR values in batches\n",
    "t = time.time()\n",
    "start_col = 0\n",
    "varname = var[:5]\n",
    "compname = 'idfngram'\n",
    "##########################################\n",
    "n_cols = counts_train.shape[1]\n",
    "col_step = 1500\n",
    "col_end = n_cols + col_step\n",
    "##########################################\n",
    "# Start iteration with columns\n",
    "low_col = start_col\n",
    "for col in np.arange(0,col_end,col_step):\n",
    "    # Limiting the edge case of the last values\n",
    "    if col > n_cols:\n",
    "        col = n_cols\n",
    "    up_col = col\n",
    "    \n",
    "    if up_col > low_col:\n",
    "        ###########################################\n",
    "        # Train PLSR on a large sample of train vectors\n",
    "        print('Columns: {}-{}'.format(low_col,up_col))\n",
    "        index = np.random.choice(len(train),size=int(4e5))\n",
    "        sample = counts_train[index,low_col:up_col].toarray()\n",
    "        reduce = cross_decomposition.PLSRegression(n_components=5)\n",
    "        reduce.fit(sample,train.iloc[index].deal_probability)\n",
    "        print('Prelim score for column range:',reduce.score(sample,train.iloc[index].deal_probability))\n",
    "        ##########################################\n",
    "        # (TRAIN) Nested indexes iteration\n",
    "        # Initial values:\n",
    "        n_rows = len(train)\n",
    "        row_step = int(2.5e5)\n",
    "        row_end = n_rows + row_step\n",
    "        components = pd.DataFrame()\n",
    "        low_idx = 0\n",
    "        ###########\n",
    "        for idx in np.arange(0,row_end,row_step):\n",
    "            # Limiting the edge case of the last values\n",
    "            if idx > n_rows:\n",
    "                idx = n_rows\n",
    "            up_idx = idx\n",
    "\n",
    "            if up_idx > low_idx:\n",
    "                sample = counts_train[low_idx:up_idx,low_col:up_col].toarray()\n",
    "                sample = reduce.transform(sample)\n",
    "                components = components.append(pd.DataFrame(sample))\n",
    "                low_idx = idx\n",
    "        components.reset_index(drop=True,inplace=True)\n",
    "        components.columns = ['col_{}-{}_{}'.format(low_col,up_col,i) for i in range(0,5)]\n",
    "        reduced_train = reduced_train.join(components)\n",
    "        print('Aggregate cv:',model_selection.cross_val_score(\n",
    "            cv=4,estimator=linear_model.LinearRegression(),\n",
    "            X=reduced_train,y=train.deal_probability))\n",
    "        ###########################################\n",
    "        # (TEST) Nested indexes iteration\n",
    "        # Initial values:\n",
    "        n_rows = len(test)\n",
    "        row_step = int(2e5)\n",
    "        row_end = n_rows + row_step\n",
    "        components = pd.DataFrame()\n",
    "        low_idx = 0\n",
    "        ###########\n",
    "        for idx in np.arange(0,row_end,row_step):\n",
    "            if idx > n_rows:\n",
    "                idx = n_rows\n",
    "            up_idx = idx\n",
    "\n",
    "            if up_idx > low_idx:\n",
    "                sample = counts_test[low_idx:up_idx,low_col:up_col].toarray()\n",
    "                sample = reduce.transform(sample)\n",
    "                components = components.append(pd.DataFrame(sample))\n",
    "                low_idx = idx\n",
    "        components.reset_index(drop=True,inplace=True)\n",
    "        components.columns = ['col_{}-{}_{}'.format(low_col,up_col,i) for i in range(0,5)]\n",
    "        reduced_test = reduced_test.join(components)\n",
    "        #####################################\n",
    "        # Prepare for next column range\n",
    "        low_col = col     \n",
    "        #####################################    \n",
    "        # Decompose aggregate every n steps\n",
    "        if up_col%(col_step*10) == 0:\n",
    "            print('Decomposing Aggregate...')\n",
    "            reduce = cross_decomposition.PLSRegression(n_components=5)\n",
    "            reduce.fit(reduced_train,train.deal_probability)\n",
    "            reduced_train = pd.DataFrame(\n",
    "                reduce.transform(reduced_train),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,5)])\n",
    "            reduced_test = pd.DataFrame(\n",
    "                reduce.transform(reduced_test),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,5)])\n",
    "            print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "                cv=4,estimator=linear_model.LinearRegression(),\n",
    "                X=reduced_train,y=train.deal_probability))\n",
    "        #####################################    \n",
    "        # Save progress every n steps\n",
    "        if up_col%(col_step*5) == 0:\n",
    "            joblib.dump(reduced_train,'train_{}_{}.sav'.format(varname,compname))\n",
    "            joblib.dump(reduced_test,'test_{}_{}.sav'.format(varname,compname))   \n",
    "#####################################\n",
    "# One final round of decomposition to n components\n",
    "if reduced_train.shape[1] > 5:\n",
    "    print('Decomposing Aggregate...')\n",
    "    reduce = cross_decomposition.PLSRegression(n_components=5)\n",
    "    reduce.fit(reduced_train,train.deal_probability)\n",
    "    reduced_train = pd.DataFrame(\n",
    "        reduce.transform(reduced_train),\n",
    "        columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,5)])\n",
    "    reduced_test = pd.DataFrame(\n",
    "        reduce.transform(reduced_test),\n",
    "        columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,5)])\n",
    "    print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "        cv=4,estimator=linear_model.LinearRegression(),\n",
    "        X=reduced_train,y=train.deal_probability))\n",
    "#########\n",
    "# Last save to disk\n",
    "joblib.dump(reduced_train,'train_{}_{}.sav'.format(varname,compname))\n",
    "joblib.dump(reduced_test,'test_{}_{}.sav'.format(varname,compname))\n",
    "#########\n",
    "print('Minutes:',(time.time()-t)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
