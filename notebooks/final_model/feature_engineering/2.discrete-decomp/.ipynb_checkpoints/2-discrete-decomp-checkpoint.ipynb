{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Vocabularies for Target Ranges\n",
    "\n",
    "- Include Count and IDF vocabularies.\n",
    "- Get vocabs for titles and descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title Vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn import preprocessing, model_selection, metrics, feature_selection, ensemble, linear_model, cross_decomposition, feature_extraction, decomposition\n",
    "import time\n",
    "from sklearn.externals import joblib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../../train.pkl',compression='zip')\n",
    "test = pd.read_pickle('../../test.pkl',compression='zip')\n",
    "# Russian stopwords\n",
    "ru_stop = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define discrete target boundaries\n",
    "i_0 = train[train.deal_probability==0].index.tolist()\n",
    "i_low = train[(train.deal_probability>0)&(train.deal_probability<0.65)].index.tolist()\n",
    "i_up = train[train.deal_probability>=0.65].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make count and idf vocabularies for target ranges on a given variable\n",
    "var = 'title'\n",
    "#####################################\n",
    "upper_str = ' '.join(train.loc[i_up,var].astype(str).values)\n",
    "lower_str = ' '.join(train.loc[i_low,var].astype(str).values)\n",
    "zeroes_str = ' '.join(train.loc[i_0,var].astype(str).values)\n",
    "\n",
    "# Get dictionaries from both count and idf vectorizers.\n",
    "count = feature_extraction.text.CountVectorizer(\n",
    "    stop_words=ru_stop,\n",
    "    lowercase=False)\n",
    "idf = feature_extraction.text.TfidfVectorizer(\n",
    "    stop_words=ru_stop,\n",
    "    lowercase=False)\n",
    "\n",
    "vecs ={'count':count,'idf':idf}\n",
    "#####################\n",
    "for key in vecs:\n",
    "    vec = vecs[key]\n",
    "    vec.fit([zeroes_str,lower_str,upper_str])\n",
    "    counts = vec.transform([zeroes_str,lower_str,upper_str])\n",
    "\n",
    "    # Convert CSR into DataFrame and Transpose. Now terms are on the index\n",
    "    counts = pd.DataFrame(counts.toarray()).T\n",
    "\n",
    "    # Extract terms from vocabulary, sort by index and add to df index\n",
    "    vocab = vec.vocabulary_\n",
    "    terms = [f for f in vocab]\n",
    "    terms = pd.DataFrame(terms)\n",
    "    terms['index'] = [vocab[k] for k in vocab]\n",
    "    terms.sort_values(by='index',inplace=True)\n",
    "    terms = terms[0].values.tolist()\n",
    "    counts.index = terms\n",
    "    \n",
    "    # Make an indicator of where the highest frequency is for each term\n",
    "    group = []\n",
    "    for i in np.arange(len(counts)):\n",
    "        group.append(np.argmax(counts.iloc[i].values))      \n",
    "    counts['group'] = group\n",
    "    \n",
    "    zero_vocab = counts[counts.group == 0].sort_values(by=0,ascending=False).index.tolist()\n",
    "    lower_vocab = counts[counts.group == 1].sort_values(by=1,ascending=False).index.tolist()\n",
    "    upper_vocab = counts[counts.group == 2].sort_values(by=2,ascending=False).index.tolist()\n",
    "\n",
    "    vocabs = [zero_vocab,lower_vocab,upper_vocab]\n",
    "    vocabs = pd.DataFrame(vocabs)\n",
    "    vocabs = vocabs.T\n",
    "    vocabs.columns=['zero_voc','low_voc','up_voc']\n",
    "    \n",
    "    vocabs.to_pickle('{}_vocabs_{}.pkl'.format(var[:5],key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> RESTART KERNEL\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description Vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn import preprocessing, model_selection, metrics, feature_selection, ensemble, linear_model, cross_decomposition, feature_extraction, decomposition\n",
    "import time\n",
    "from sklearn.externals import joblib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../../train.pkl',compression='zip')\n",
    "test = pd.read_pickle('../../test.pkl',compression='zip')\n",
    "# Russian stopwords\n",
    "ru_stop = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define discrete target boundaries\n",
    "i_0 = train[train.deal_probability==0].index.tolist()\n",
    "i_low = train[(train.deal_probability>0)&(train.deal_probability<0.65)].index.tolist()\n",
    "i_up = train[train.deal_probability>=0.65].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make count and idf vocabularies for target ranges on a given variable\n",
    "var = 'description'\n",
    "#####################################\n",
    "upper_str = ' '.join(train.loc[i_up,var].astype(str).values)\n",
    "lower_str = ' '.join(train.loc[i_low,var].astype(str).values)\n",
    "zeroes_str = ' '.join(train.loc[i_0,var].astype(str).values)\n",
    "\n",
    "# Get dictionaries from both count and idf vectorizers.\n",
    "count = feature_extraction.text.CountVectorizer(\n",
    "    stop_words=ru_stop,\n",
    "    lowercase=False)\n",
    "idf = feature_extraction.text.TfidfVectorizer(\n",
    "    stop_words=ru_stop,\n",
    "    lowercase=False)\n",
    "\n",
    "vecs ={'count':count,'idf':idf}\n",
    "#####################\n",
    "for key in vecs:\n",
    "    vec = vecs[key]\n",
    "    vec.fit([zeroes_str,lower_str,upper_str])\n",
    "    counts = vec.transform([zeroes_str,lower_str,upper_str])\n",
    "\n",
    "    # Convert CSR into DataFrame and Transpose. Now terms are on the index\n",
    "    counts = pd.DataFrame(counts.toarray()).T\n",
    "\n",
    "    # Extract terms from vocabulary, sort by index and add to df index\n",
    "    vocab = vec.vocabulary_\n",
    "    terms = [f for f in vocab]\n",
    "    terms = pd.DataFrame(terms)\n",
    "    terms['index'] = [vocab[k] for k in vocab]\n",
    "    terms.sort_values(by='index',inplace=True)\n",
    "    terms = terms[0].values.tolist()\n",
    "    counts.index = terms\n",
    "    \n",
    "    # Make an indicator of where the highest frequency is for each term\n",
    "    group = []\n",
    "    for i in np.arange(len(counts)):\n",
    "        group.append(np.argmax(counts.iloc[i].values))      \n",
    "    counts['group'] = group\n",
    "    \n",
    "    zero_vocab = counts[counts.group == 0].sort_values(by=0,ascending=False).index.tolist()\n",
    "    lower_vocab = counts[counts.group == 1].sort_values(by=1,ascending=False).index.tolist()\n",
    "    upper_vocab = counts[counts.group == 2].sort_values(by=2,ascending=False).index.tolist()\n",
    "\n",
    "    vocabs = [zero_vocab,lower_vocab,upper_vocab]\n",
    "    vocabs = pd.DataFrame(vocabs)\n",
    "    vocabs = vocabs.T\n",
    "    vocabs.columns=['zero_voc','low_voc','up_voc']\n",
    "    \n",
    "    vocabs.to_pickle('{}_vocabs_{}.pkl'.format(var[:5],key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> RESTART THE KERNEL\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Decomposition per Vocab\n",
    "\n",
    "I can only decompose one vocabulary at a time, for memory limitation reasons. So every time define:\n",
    "- the variable\n",
    "- the vocabulary kind\n",
    "- the target range it represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title var- Count voc- Zero Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn import preprocessing, model_selection, metrics, feature_selection, ensemble, linear_model, cross_decomposition, feature_extraction, decomposition\n",
    "import time\n",
    "from sklearn.externals import joblib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../../../train.pkl',compression='zip')\n",
    "test = pd.read_pickle('../../../test.pkl',compression='zip')\n",
    "# Russian stopwords\n",
    "ru_stop = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define discrete target boundaries\n",
    "i_0 = train[train.deal_probability==0].index.tolist()\n",
    "i_low = train[(train.deal_probability>0)&(train.deal_probability<0.65)].index.tolist()\n",
    "i_up = train[train.deal_probability>=0.65].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variable, index-range, discrete-range, voc-origin, and component-name\n",
    "var = 'title' # title or description\n",
    "irange = i_0 # pick from above cell\n",
    "rnge = 'zero' # zero, low or up\n",
    "voc_kind = 'count' # count or idf\n",
    "compname = 'zerocnt' # custom name for component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the desired vocabulary onto a list of limited length\n",
    "voc = pd.read_pickle('{}_vocabs_{}.pkl'.format(var[:5],voc_kind))['{}_voc'.format(rnge)].dropna()[:67000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N tokens: 67000\n"
     ]
    }
   ],
   "source": [
    "vec = feature_extraction.text.TfidfVectorizer(\n",
    "    stop_words=ru_stop,\n",
    "    lowercase=False,\n",
    "    vocabulary=voc)\n",
    "# Fitting on train and test as merged lists\n",
    "vec.fit(train[var].astype(str).tolist() + test[var].astype(str).tolist())\n",
    "print('N tokens:',len(vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### vectors for train and test\n",
    "counts_train = vec.transform(train[var].astype(str).tolist())\n",
    "counts_test = vec.transform(test[var].astype(str).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### To start from zero...\n",
    "reduced_train = pd.DataFrame(index=train.index)\n",
    "reduced_test = pd.DataFrame(index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: 0-1500\n",
      "Prelim score for column range: 0.056647761693727094\n",
      "Aggregate cv: [0.0437396  0.04204362 0.04367361 0.04482865]\n",
      "Aggregate score for up range: [0.08124249 0.07623606 0.07767721 0.07850478]\n",
      "(1503424, 0) (508438, 0)\n",
      "Columns: 1500-3000\n",
      "Prelim score for column range: 0.011968047589463904\n",
      "Aggregate cv: [0.04687835 0.0450409  0.04649784 0.04759228]\n",
      "Aggregate score for up range: [0.08203729 0.07668296 0.07864323 0.07990906]\n",
      "(1503424, 2) (508438, 2)\n",
      "Columns: 3000-4500\n",
      "Prelim score for column range: 0.00641642638614326\n",
      "Aggregate cv: [0.04818664 0.04643281 0.04793703 0.04907903]\n",
      "Aggregate score for up range: [0.0824903  0.07709868 0.07954522 0.08001705]\n",
      "(1503424, 4) (508438, 4)\n",
      "Columns: 4500-6000\n",
      "Prelim score for column range: 0.007745084219598074\n",
      "Aggregate cv: [0.04973886 0.04806677 0.049751   0.05075626]\n",
      "Aggregate score for up range: [0.08318783 0.07793074 0.08104477 0.0807392 ]\n",
      "(1503424, 6) (508438, 6)\n",
      "Columns: 6000-7500\n",
      "Prelim score for column range: 0.0067777154659836025\n",
      "Aggregate cv: [0.05144193 0.0493584  0.05101778 0.05233962]\n",
      "Aggregate score for up range: [0.08392135 0.07784635 0.08138418 0.08150062]\n",
      "(1503424, 8) (508438, 8)\n",
      "Columns: 7500-9000\n",
      "Prelim score for column range: 0.006987419742598977\n",
      "Aggregate cv: [0.05273216 0.05077756 0.05257169 0.05380414]\n",
      "Aggregate score for up range: [0.08427017 0.07823867 0.08236086 0.0824333 ]\n",
      "(1503424, 10) (508438, 10)\n",
      "Columns: 9000-10500\n",
      "Prelim score for column range: 0.005894594160555711\n",
      "Aggregate cv: [0.05399073 0.05200129 0.05398157 0.05533756]\n",
      "Aggregate score for up range: [0.08424785 0.07825832 0.08234758 0.08245303]\n",
      "(1503424, 12) (508438, 12)\n",
      "Columns: 10500-12000\n",
      "Prelim score for column range: 0.006372188606543538\n",
      "Aggregate cv: [0.05521872 0.05338252 0.05502756 0.05659624]\n",
      "Aggregate score for up range: [0.08421206 0.07828075 0.08236441 0.0824345 ]\n",
      "(1503424, 14) (508438, 14)\n",
      "Columns: 12000-13500\n",
      "Prelim score for column range: 0.006076355742526075\n",
      "Aggregate cv: [0.05641762 0.05470564 0.05642855 0.05805593]\n",
      "Aggregate score for up range: [0.08423319 0.07824337 0.08239025 0.08243593]\n",
      "(1503424, 16) (508438, 16)\n",
      "Columns: 13500-15000\n",
      "Prelim score for column range: 0.006432396558075371\n",
      "Aggregate cv: [0.05773034 0.0561916  0.05787685 0.0595435 ]\n",
      "Aggregate score for up range: [0.08429718 0.07832365 0.08245345 0.08232928]\n",
      "(1503424, 18) (508438, 18)\n",
      "Interval decomposition...\n",
      "(1503424, 20) (508438, 20)\n",
      "Aggregate cv after decomposition: [0.057753   0.05619984 0.05850618 0.05956529]\n",
      "Columns: 15000-16262\n",
      "Prelim score for column range: 0.004721482742708227\n",
      "Aggregate cv: [0.05899784 0.05739623 0.05934332 0.06030592]\n",
      "Aggregate score for up range: [0.06390983 0.0583199  0.06157603 0.06018849]\n",
      "(1503424, 2) (508438, 2)\n",
      "Aggregate cv after decomposition: [0.05899843 0.05740616 0.05934397 0.06041749]\n",
      "Minutes: 7.507353901863098\n"
     ]
    }
   ],
   "source": [
    "# Reduce all CSR values in batches\n",
    "t = time.time()\n",
    "start_col = 0\n",
    "varname = var[:5]\n",
    "##########################################\n",
    "n_cols = counts_train.shape[1]\n",
    "col_step = 1500\n",
    "col_end = n_cols + col_step\n",
    "##########################################\n",
    "# Start iteration with columns\n",
    "low_col = start_col\n",
    "corrupted = False\n",
    "for col in np.arange(0,col_end,col_step):\n",
    "    # Limiting the edge case of the last values\n",
    "    if col > n_cols:\n",
    "        col = n_cols\n",
    "    up_col = col\n",
    "\n",
    "    if up_col > low_col:\n",
    "        ###########################################\n",
    "        # Train PLSR on a large sample of train vectors\n",
    "        print('Columns: {}-{}'.format(low_col,up_col))\n",
    "        index = np.random.choice(len(train),size=int(4e5))\n",
    "        sample = counts_train[index,low_col:up_col].toarray()\n",
    "        reduce = cross_decomposition.PLSRegression(n_components=2)\n",
    "        reduce.fit(sample,train.iloc[index].deal_probability)\n",
    "        print('Prelim score for column range:',reduce.score(sample,train.iloc[index].deal_probability))\n",
    "        ##########################################\n",
    "        # (TRAIN) Nested indexes iteration\n",
    "        # Initial values:\n",
    "        n_rows = len(train)\n",
    "        row_step = int(2.5e5)\n",
    "        row_end = n_rows + row_step\n",
    "        components = pd.DataFrame()\n",
    "        low_idx = 0\n",
    "        ###########\n",
    "        for idx in np.arange(0,row_end,row_step):\n",
    "            # Limiting the edge case of the last values\n",
    "            if idx > n_rows:\n",
    "                idx = n_rows\n",
    "            up_idx = idx\n",
    "\n",
    "            if up_idx > low_idx:\n",
    "                sample = counts_train[low_idx:up_idx,low_col:up_col].toarray()\n",
    "                sample = reduce.transform(sample)\n",
    "                components = components.append(pd.DataFrame(sample))\n",
    "                low_idx = idx\n",
    "        components.reset_index(drop=True,inplace=True)\n",
    "        components.columns = ['col_{}-{}_{}'.format(low_col,up_col,i) for i in range(0,2)]\n",
    "\n",
    "        # Cross-validate and check for corruptions before joining\n",
    "        cv = model_selection.cross_val_score(\n",
    "            cv=4,estimator=linear_model.LinearRegression(),\n",
    "            X=reduced_train.join(components),y=train.deal_probability)\n",
    "\n",
    "        print('Aggregate cv:',cv)\n",
    "        print('Aggregate score for {} range:'.format(rnge),model_selection.cross_val_score(\n",
    "            cv=4,estimator=linear_model.LinearRegression(),\n",
    "            X=reduced_train.join(components).iloc[irange],y=train.iloc[irange].deal_probability))\n",
    "        \n",
    "        \n",
    "        if sum([score < 0 for score in cv]) > 0:\n",
    "            print('Reached corruption.\\n Final decomposition without joining...')\n",
    "            print(reduced_train.shape,reduced_test.shape)\n",
    "            reduce = cross_decomposition.PLSRegression(n_components=2)\n",
    "            reduce.fit(reduced_train,train.deal_probability)\n",
    "            reduced_train = pd.DataFrame(\n",
    "                reduce.transform(reduced_train),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "            reduced_test = pd.DataFrame(\n",
    "                reduce.transform(reduced_test),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "            print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "                cv=4,estimator=linear_model.LinearRegression(),\n",
    "                X=reduced_train,y=train.deal_probability))\n",
    "            print('Minutes:',(time.time()-t)/60)\n",
    "            break\n",
    "        \n",
    "        # Join if it wasn't corrupted..    \n",
    "        reduced_train = reduced_train.join(components)\n",
    "        ###########################################\n",
    "        # (TEST) Nested indexes iteration\n",
    "        # Initial values:\n",
    "        n_rows = len(test)\n",
    "        row_step = int(2e5)\n",
    "        row_end = n_rows + row_step\n",
    "        components = pd.DataFrame()\n",
    "        low_idx = 0\n",
    "        ###########\n",
    "        for idx in np.arange(0,row_end,row_step):\n",
    "            if idx > n_rows:\n",
    "                idx = n_rows\n",
    "            up_idx = idx\n",
    "\n",
    "            if up_idx > low_idx:\n",
    "                sample = counts_test[low_idx:up_idx,low_col:up_col].toarray()\n",
    "                sample = reduce.transform(sample)\n",
    "                components = components.append(pd.DataFrame(sample))\n",
    "                low_idx = idx\n",
    "        components.reset_index(drop=True,inplace=True)\n",
    "        components.columns = ['col_{}-{}_{}'.format(low_col,up_col,i) for i in range(0,2)]\n",
    "        reduced_test = reduced_test.join(components)\n",
    "        print(reduced_train.shape,reduced_test.shape)\n",
    "        #####################################\n",
    "        # Prepare for next column range\n",
    "        low_col = col     \n",
    "        #####################################    \n",
    "        # Interval decompositions\n",
    "        if up_col%(col_step*10) == 0:\n",
    "            print('Interval decomposition...')\n",
    "            print(reduced_train.shape,reduced_test.shape)\n",
    "            reduce = cross_decomposition.PLSRegression(n_components=2)\n",
    "            reduce.fit(reduced_train,train.deal_probability)\n",
    "            reduced_train = pd.DataFrame(\n",
    "                reduce.transform(reduced_train),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "            reduced_test = pd.DataFrame(\n",
    "                reduce.transform(reduced_test),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "            print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "                cv=4,estimator=linear_model.LinearRegression(),\n",
    "                X=reduced_train,y=train.deal_probability))\n",
    "        #####################################    \n",
    "        # Save progress every n steps\n",
    "        if up_col%(col_step*5) == 0:\n",
    "            print('Interval save to disk...')\n",
    "            joblib.dump(reduced_train,'train_{}_{}.sav'.format(varname,compname))\n",
    "            joblib.dump(reduced_test,'test_{}_{}.sav'.format(varname,compname))   \n",
    "#####################################\n",
    "# Final round of decomposition\n",
    "print('Final decomposition...')\n",
    "reduce = cross_decomposition.PLSRegression(n_components=2)\n",
    "reduce.fit(reduced_train,train.deal_probability)\n",
    "reduced_train = pd.DataFrame(\n",
    "    reduce.transform(reduced_train),\n",
    "    columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "reduced_test = pd.DataFrame(\n",
    "    reduce.transform(reduced_test),\n",
    "    columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "    cv=4,estimator=linear_model.LinearRegression(),\n",
    "    X=reduced_train,y=train.deal_probability))\n",
    "#########\n",
    "# Last save to disk\n",
    "joblib.dump(reduced_train,'train_{}_{}.sav'.format(varname,compname))\n",
    "joblib.dump(reduced_test,'test_{}_{}.sav'.format(varname,compname))\n",
    "#########\n",
    "print('Minutes:',(time.time()-t)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> RESTART KERNEL\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title var- Count voc- Lower Range\n",
    "\n",
    "Learning from the corruption in the previous iteration, I've eliminated the interval decompositions and the final decomposition, while reducing the number of components to 2 per column step. This will allow the freedom to trim corrupted components before doing a final decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn import preprocessing, model_selection, metrics, feature_selection, ensemble, linear_model, cross_decomposition, feature_extraction, decomposition\n",
    "import time\n",
    "from sklearn.externals import joblib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../../../train.pkl',compression='zip')\n",
    "test = pd.read_pickle('../../../test.pkl',compression='zip')\n",
    "# Russian stopwords\n",
    "ru_stop = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define discrete target boundaries\n",
    "i_0 = train[train.deal_probability==0].index.tolist()\n",
    "i_low = train[(train.deal_probability>0)&(train.deal_probability<0.65)].index.tolist()\n",
    "i_up = train[train.deal_probability>=0.65].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variable, index-range, discrete-range, voc-origin, and component-name\n",
    "var = 'title' # title or description\n",
    "irange = i_low # pick from above cell\n",
    "rnge = 'low' # zero, low or up\n",
    "voc_kind = 'count' # count or idf\n",
    "compname = 'lowcnt' # custom name for component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the desired vocabulary onto a list of limited length\n",
    "voc = pd.read_pickle('{}_vocabs_{}.pkl'.format(var[:5],voc_kind))['{}_voc'.format(rnge)].dropna()[:67000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N tokens: 41218\n"
     ]
    }
   ],
   "source": [
    "vec = feature_extraction.text.TfidfVectorizer(\n",
    "    stop_words=ru_stop,\n",
    "    lowercase=False,\n",
    "    vocabulary=voc)\n",
    "# Fitting on train and test as merged lists\n",
    "vec.fit(train[var].astype(str).tolist() + test[var].astype(str).tolist())\n",
    "print('N tokens:',len(vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### vectors for train and test\n",
    "counts_train = vec.transform(train[var].astype(str).tolist())\n",
    "counts_test = vec.transform(test[var].astype(str).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### To start from zero...\n",
    "reduced_train = pd.DataFrame(index=train.index)\n",
    "reduced_test = pd.DataFrame(index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: 0-1500\n",
      "Prelim score for column range: 0.056647761693727094\n",
      "Aggregate cv: [0.0437396  0.04204362 0.04367361 0.04482865]\n",
      "Aggregate score for up range: [0.08124249 0.07623606 0.07767721 0.07850478]\n",
      "(1503424, 0) (508438, 0)\n",
      "Columns: 1500-3000\n",
      "Prelim score for column range: 0.011968047589463904\n",
      "Aggregate cv: [0.04687835 0.0450409  0.04649784 0.04759228]\n",
      "Aggregate score for up range: [0.08203729 0.07668296 0.07864323 0.07990906]\n",
      "(1503424, 2) (508438, 2)\n",
      "Columns: 3000-4500\n",
      "Prelim score for column range: 0.00641642638614326\n",
      "Aggregate cv: [0.04818664 0.04643281 0.04793703 0.04907903]\n",
      "Aggregate score for up range: [0.0824903  0.07709868 0.07954522 0.08001705]\n",
      "(1503424, 4) (508438, 4)\n",
      "Columns: 4500-6000\n",
      "Prelim score for column range: 0.007745084219598074\n",
      "Aggregate cv: [0.04973886 0.04806677 0.049751   0.05075626]\n",
      "Aggregate score for up range: [0.08318783 0.07793074 0.08104477 0.0807392 ]\n",
      "(1503424, 6) (508438, 6)\n",
      "Columns: 6000-7500\n",
      "Prelim score for column range: 0.0067777154659836025\n",
      "Aggregate cv: [0.05144193 0.0493584  0.05101778 0.05233962]\n",
      "Aggregate score for up range: [0.08392135 0.07784635 0.08138418 0.08150062]\n",
      "(1503424, 8) (508438, 8)\n",
      "Columns: 7500-9000\n",
      "Prelim score for column range: 0.006987419742598977\n",
      "Aggregate cv: [0.05273216 0.05077756 0.05257169 0.05380414]\n",
      "Aggregate score for up range: [0.08427017 0.07823867 0.08236086 0.0824333 ]\n",
      "(1503424, 10) (508438, 10)\n",
      "Columns: 9000-10500\n",
      "Prelim score for column range: 0.005894594160555711\n",
      "Aggregate cv: [0.05399073 0.05200129 0.05398157 0.05533756]\n",
      "Aggregate score for up range: [0.08424785 0.07825832 0.08234758 0.08245303]\n",
      "(1503424, 12) (508438, 12)\n",
      "Columns: 10500-12000\n",
      "Prelim score for column range: 0.006372188606543538\n",
      "Aggregate cv: [0.05521872 0.05338252 0.05502756 0.05659624]\n",
      "Aggregate score for up range: [0.08421206 0.07828075 0.08236441 0.0824345 ]\n",
      "(1503424, 14) (508438, 14)\n",
      "Columns: 12000-13500\n",
      "Prelim score for column range: 0.006076355742526075\n",
      "Aggregate cv: [0.05641762 0.05470564 0.05642855 0.05805593]\n",
      "Aggregate score for up range: [0.08423319 0.07824337 0.08239025 0.08243593]\n",
      "(1503424, 16) (508438, 16)\n",
      "Columns: 13500-15000\n",
      "Prelim score for column range: 0.006432396558075371\n",
      "Aggregate cv: [0.05773034 0.0561916  0.05787685 0.0595435 ]\n",
      "Aggregate score for up range: [0.08429718 0.07832365 0.08245345 0.08232928]\n",
      "(1503424, 18) (508438, 18)\n",
      "Interval decomposition...\n",
      "(1503424, 20) (508438, 20)\n",
      "Aggregate cv after decomposition: [0.057753   0.05619984 0.05850618 0.05956529]\n",
      "Columns: 15000-16262\n",
      "Prelim score for column range: 0.004721482742708227\n",
      "Aggregate cv: [0.05899784 0.05739623 0.05934332 0.06030592]\n",
      "Aggregate score for up range: [0.06390983 0.0583199  0.06157603 0.06018849]\n",
      "(1503424, 2) (508438, 2)\n",
      "Aggregate cv after decomposition: [0.05899843 0.05740616 0.05934397 0.06041749]\n",
      "Minutes: 7.507353901863098\n"
     ]
    }
   ],
   "source": [
    "# Reduce all CSR values in batches\n",
    "t = time.time()\n",
    "start_col = 0\n",
    "varname = var[:5]\n",
    "##########################################\n",
    "n_cols = counts_train.shape[1]\n",
    "col_step = 1500\n",
    "col_end = n_cols + col_step\n",
    "##########################################\n",
    "# Start iteration with columns\n",
    "low_col = start_col\n",
    "corrupted = False\n",
    "for col in np.arange(0,col_end,col_step):\n",
    "    # Limiting the edge case of the last values\n",
    "    if col > n_cols:\n",
    "        col = n_cols\n",
    "    up_col = col\n",
    "\n",
    "    if up_col > low_col:\n",
    "        ###########################################\n",
    "        # Train PLSR on a large sample of train vectors\n",
    "        print('Columns: {}-{}'.format(low_col,up_col))\n",
    "        index = np.random.choice(len(train),size=int(4e5))\n",
    "        sample = counts_train[index,low_col:up_col].toarray()\n",
    "        reduce = cross_decomposition.PLSRegression(n_components=2)\n",
    "        reduce.fit(sample,train.iloc[index].deal_probability)\n",
    "        print('Prelim score for column range:',reduce.score(sample,train.iloc[index].deal_probability))\n",
    "        ##########################################\n",
    "        # (TRAIN) Nested indexes iteration\n",
    "        # Initial values:\n",
    "        n_rows = len(train)\n",
    "        row_step = int(2.5e5)\n",
    "        row_end = n_rows + row_step\n",
    "        components = pd.DataFrame()\n",
    "        low_idx = 0\n",
    "        ###########\n",
    "        for idx in np.arange(0,row_end,row_step):\n",
    "            # Limiting the edge case of the last values\n",
    "            if idx > n_rows:\n",
    "                idx = n_rows\n",
    "            up_idx = idx\n",
    "\n",
    "            if up_idx > low_idx:\n",
    "                sample = counts_train[low_idx:up_idx,low_col:up_col].toarray()\n",
    "                sample = reduce.transform(sample)\n",
    "                components = components.append(pd.DataFrame(sample))\n",
    "                low_idx = idx\n",
    "        components.reset_index(drop=True,inplace=True)\n",
    "        components.columns = ['col_{}-{}_{}'.format(low_col,up_col,i) for i in range(0,2)]\n",
    "\n",
    "        # Cross-validate and check for corruptions before joining\n",
    "        cv = model_selection.cross_val_score(\n",
    "            cv=4,estimator=linear_model.LinearRegression(),\n",
    "            X=reduced_train.join(components),y=train.deal_probability)\n",
    "\n",
    "        print('Aggregate cv:',cv)\n",
    "        print('Aggregate score for {} range:'.format(rnge),model_selection.cross_val_score(\n",
    "            cv=4,estimator=linear_model.LinearRegression(),\n",
    "            X=reduced_train.join(components).iloc[irange],y=train.iloc[irange].deal_probability))\n",
    "        \n",
    "        \n",
    "        if sum([score < 0 for score in cv]) > 0:\n",
    "            print('Reached corruption.\\n Final decomposition without joining...')\n",
    "            print(reduced_train.shape,reduced_test.shape)\n",
    "            reduce = cross_decomposition.PLSRegression(n_components=2)\n",
    "            reduce.fit(reduced_train,train.deal_probability)\n",
    "            reduced_train = pd.DataFrame(\n",
    "                reduce.transform(reduced_train),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "            reduced_test = pd.DataFrame(\n",
    "                reduce.transform(reduced_test),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "            print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "                cv=4,estimator=linear_model.LinearRegression(),\n",
    "                X=reduced_train,y=train.deal_probability))\n",
    "            print('Minutes:',(time.time()-t)/60)\n",
    "            break\n",
    "        \n",
    "        # Join if it wasn't corrupted..    \n",
    "        reduced_train = reduced_train.join(components)\n",
    "        ###########################################\n",
    "        # (TEST) Nested indexes iteration\n",
    "        # Initial values:\n",
    "        n_rows = len(test)\n",
    "        row_step = int(2e5)\n",
    "        row_end = n_rows + row_step\n",
    "        components = pd.DataFrame()\n",
    "        low_idx = 0\n",
    "        ###########\n",
    "        for idx in np.arange(0,row_end,row_step):\n",
    "            if idx > n_rows:\n",
    "                idx = n_rows\n",
    "            up_idx = idx\n",
    "\n",
    "            if up_idx > low_idx:\n",
    "                sample = counts_test[low_idx:up_idx,low_col:up_col].toarray()\n",
    "                sample = reduce.transform(sample)\n",
    "                components = components.append(pd.DataFrame(sample))\n",
    "                low_idx = idx\n",
    "        components.reset_index(drop=True,inplace=True)\n",
    "        components.columns = ['col_{}-{}_{}'.format(low_col,up_col,i) for i in range(0,2)]\n",
    "        reduced_test = reduced_test.join(components)\n",
    "        print(reduced_train.shape,reduced_test.shape)\n",
    "        #####################################\n",
    "        # Prepare for next column range\n",
    "        low_col = col     \n",
    "        #####################################    \n",
    "        # Interval decompositions\n",
    "        if up_col%(col_step*10) == 0:\n",
    "            print('Interval decomposition...')\n",
    "            print(reduced_train.shape,reduced_test.shape)\n",
    "            reduce = cross_decomposition.PLSRegression(n_components=2)\n",
    "            reduce.fit(reduced_train,train.deal_probability)\n",
    "            reduced_train = pd.DataFrame(\n",
    "                reduce.transform(reduced_train),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "            reduced_test = pd.DataFrame(\n",
    "                reduce.transform(reduced_test),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "            print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "                cv=4,estimator=linear_model.LinearRegression(),\n",
    "                X=reduced_train,y=train.deal_probability))\n",
    "        #####################################    \n",
    "        # Save progress every n steps\n",
    "        if up_col%(col_step*5) == 0:\n",
    "            print('Interval save to disk...')\n",
    "            joblib.dump(reduced_train,'train_{}_{}.sav'.format(varname,compname))\n",
    "            joblib.dump(reduced_test,'test_{}_{}.sav'.format(varname,compname))   \n",
    "#####################################\n",
    "# Final round of decomposition\n",
    "print('Final decomposition...')\n",
    "reduce = cross_decomposition.PLSRegression(n_components=2)\n",
    "reduce.fit(reduced_train,train.deal_probability)\n",
    "reduced_train = pd.DataFrame(\n",
    "    reduce.transform(reduced_train),\n",
    "    columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "reduced_test = pd.DataFrame(\n",
    "    reduce.transform(reduced_test),\n",
    "    columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "    cv=4,estimator=linear_model.LinearRegression(),\n",
    "    X=reduced_train,y=train.deal_probability))\n",
    "#########\n",
    "# Last save to disk\n",
    "joblib.dump(reduced_train,'train_{}_{}.sav'.format(varname,compname))\n",
    "joblib.dump(reduced_test,'test_{}_{}.sav'.format(varname,compname))\n",
    "#########\n",
    "print('Minutes:',(time.time()-t)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> RESTART THE KERNEL\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title var- Count voc- Upper Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn import preprocessing, model_selection, metrics, feature_selection, ensemble, linear_model, cross_decomposition, feature_extraction, decomposition\n",
    "import time\n",
    "from sklearn.externals import joblib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../../../train.pkl',compression='zip')\n",
    "test = pd.read_pickle('../../../test.pkl',compression='zip')\n",
    "# Russian stopwords\n",
    "ru_stop = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define discrete target boundaries\n",
    "i_0 = train[train.deal_probability==0].index.tolist()\n",
    "i_low = train[(train.deal_probability>0)&(train.deal_probability<0.65)].index.tolist()\n",
    "i_up = train[train.deal_probability>=0.65].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variable, index-range, discrete-range, voc-origin, and component-name\n",
    "var = 'title' # title or description\n",
    "irange = i_up # pick from above cell\n",
    "rnge = 'up' # zero, low or up\n",
    "voc_kind = 'count' # count or idf\n",
    "compname = 'upcnt' # custom name for component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the desired vocabulary onto a list of limited length\n",
    "voc = pd.read_pickle('{}_vocabs_{}.pkl'.format(var[:5],voc_kind))['{}_voc'.format(rnge)].dropna()[:67000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N tokens: 16262\n"
     ]
    }
   ],
   "source": [
    "vec = feature_extraction.text.TfidfVectorizer(\n",
    "    stop_words=ru_stop,\n",
    "    lowercase=False,\n",
    "    vocabulary=voc)\n",
    "# Fitting on train and test as merged lists\n",
    "vec.fit(train[var].astype(str).tolist() + test[var].astype(str).tolist())\n",
    "print('N tokens:',len(vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### vectors for train and test\n",
    "counts_train = vec.transform(train[var].astype(str).tolist())\n",
    "counts_test = vec.transform(test[var].astype(str).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### To start from zero...\n",
    "reduced_train = pd.DataFrame(index=train.index)\n",
    "reduced_test = pd.DataFrame(index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce all CSR values in batches\n",
    "t = time.time()\n",
    "start_col = 0\n",
    "varname = var[:5]\n",
    "##########################################\n",
    "n_cols = counts_train.shape[1]\n",
    "col_step = 1500\n",
    "col_end = n_cols + col_step\n",
    "##########################################\n",
    "# Start iteration with columns\n",
    "low_col = start_col\n",
    "corrupted = False\n",
    "for col in np.arange(0,col_end,col_step):\n",
    "    # Limiting the edge case of the last values\n",
    "    if col > n_cols:\n",
    "        col = n_cols\n",
    "    up_col = col\n",
    "\n",
    "    if up_col > low_col:\n",
    "        ###########################################\n",
    "        # Train PLSR on a large sample of train vectors\n",
    "        print('Columns: {}-{}'.format(low_col,up_col))\n",
    "        index = np.random.choice(len(train),size=int(4e5))\n",
    "        sample = counts_train[index,low_col:up_col].toarray()\n",
    "        reduce = cross_decomposition.PLSRegression(n_components=2)\n",
    "        reduce.fit(sample,train.iloc[index].deal_probability)\n",
    "        print('Prelim score for column range:',reduce.score(sample,train.iloc[index].deal_probability))\n",
    "        ##########################################\n",
    "        # (TRAIN) Nested indexes iteration\n",
    "        # Initial values:\n",
    "        n_rows = len(train)\n",
    "        row_step = int(2.5e5)\n",
    "        row_end = n_rows + row_step\n",
    "        components = pd.DataFrame()\n",
    "        low_idx = 0\n",
    "        ###########\n",
    "        for idx in np.arange(0,row_end,row_step):\n",
    "            # Limiting the edge case of the last values\n",
    "            if idx > n_rows:\n",
    "                idx = n_rows\n",
    "            up_idx = idx\n",
    "\n",
    "            if up_idx > low_idx:\n",
    "                sample = counts_train[low_idx:up_idx,low_col:up_col].toarray()\n",
    "                sample = reduce.transform(sample)\n",
    "                components = components.append(pd.DataFrame(sample))\n",
    "                low_idx = idx\n",
    "        components.reset_index(drop=True,inplace=True)\n",
    "        components.columns = ['col_{}-{}_{}'.format(low_col,up_col,i) for i in range(0,2)]\n",
    "\n",
    "        # Cross-validate and check for corruptions before joining\n",
    "        cv = model_selection.cross_val_score(\n",
    "            cv=4,estimator=linear_model.LinearRegression(),\n",
    "            X=reduced_train.join(components),y=train.deal_probability)\n",
    "\n",
    "        print('Aggregate cv:',cv)\n",
    "        print('Aggregate score for {} range:'.format(rnge),model_selection.cross_val_score(\n",
    "            cv=4,estimator=linear_model.LinearRegression(),\n",
    "            X=reduced_train.join(components).iloc[irange],y=train.iloc[irange].deal_probability))\n",
    "        \n",
    "        \n",
    "        if sum([score < 0 for score in cv]) > 0:\n",
    "            print('Reached corruption.\\n Final decomposition without joining...')\n",
    "            print(reduced_train.shape,reduced_test.shape)\n",
    "            reduce = cross_decomposition.PLSRegression(n_components=2)\n",
    "            reduce.fit(reduced_train,train.deal_probability)\n",
    "            reduced_train = pd.DataFrame(\n",
    "                reduce.transform(reduced_train),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "            reduced_test = pd.DataFrame(\n",
    "                reduce.transform(reduced_test),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "            print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "                cv=4,estimator=linear_model.LinearRegression(),\n",
    "                X=reduced_train,y=train.deal_probability))\n",
    "            print('Minutes:',(time.time()-t)/60)\n",
    "            break\n",
    "        \n",
    "        # Join if it wasn't corrupted..    \n",
    "        reduced_train = reduced_train.join(components)\n",
    "        ###########################################\n",
    "        # (TEST) Nested indexes iteration\n",
    "        # Initial values:\n",
    "        n_rows = len(test)\n",
    "        row_step = int(2e5)\n",
    "        row_end = n_rows + row_step\n",
    "        components = pd.DataFrame()\n",
    "        low_idx = 0\n",
    "        ###########\n",
    "        for idx in np.arange(0,row_end,row_step):\n",
    "            if idx > n_rows:\n",
    "                idx = n_rows\n",
    "            up_idx = idx\n",
    "\n",
    "            if up_idx > low_idx:\n",
    "                sample = counts_test[low_idx:up_idx,low_col:up_col].toarray()\n",
    "                sample = reduce.transform(sample)\n",
    "                components = components.append(pd.DataFrame(sample))\n",
    "                low_idx = idx\n",
    "        components.reset_index(drop=True,inplace=True)\n",
    "        components.columns = ['col_{}-{}_{}'.format(low_col,up_col,i) for i in range(0,2)]\n",
    "        reduced_test = reduced_test.join(components)\n",
    "        print(reduced_train.shape,reduced_test.shape)\n",
    "        #####################################\n",
    "        # Prepare for next column range\n",
    "        low_col = col     \n",
    "        #####################################    \n",
    "        # Interval decompositions\n",
    "        if up_col%(col_step*10) == 0:\n",
    "            print('Interval decomposition...')\n",
    "            print(reduced_train.shape,reduced_test.shape)\n",
    "            reduce = cross_decomposition.PLSRegression(n_components=2)\n",
    "            reduce.fit(reduced_train,train.deal_probability)\n",
    "            reduced_train = pd.DataFrame(\n",
    "                reduce.transform(reduced_train),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "            reduced_test = pd.DataFrame(\n",
    "                reduce.transform(reduced_test),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "            print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "                cv=4,estimator=linear_model.LinearRegression(),\n",
    "                X=reduced_train,y=train.deal_probability))\n",
    "        #####################################    \n",
    "        # Save progress every n steps\n",
    "        if up_col%(col_step*5) == 0:\n",
    "            print('Interval save to disk...')\n",
    "            joblib.dump(reduced_train,'train_{}_{}.sav'.format(varname,compname))\n",
    "            joblib.dump(reduced_test,'test_{}_{}.sav'.format(varname,compname))   \n",
    "#####################################\n",
    "# Final round of decomposition\n",
    "print('Final decomposition...')\n",
    "reduce = cross_decomposition.PLSRegression(n_components=2)\n",
    "reduce.fit(reduced_train,train.deal_probability)\n",
    "reduced_train = pd.DataFrame(\n",
    "    reduce.transform(reduced_train),\n",
    "    columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "reduced_test = pd.DataFrame(\n",
    "    reduce.transform(reduced_test),\n",
    "    columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "    cv=4,estimator=linear_model.LinearRegression(),\n",
    "    X=reduced_train,y=train.deal_probability))\n",
    "#########\n",
    "# Last save to disk\n",
    "joblib.dump(reduced_train,'train_{}_{}.sav'.format(varname,compname))\n",
    "joblib.dump(reduced_test,'test_{}_{}.sav'.format(varname,compname))\n",
    "#########\n",
    "print('Minutes:',(time.time()-t)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> RESTART THE KERNEL\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title var- IDF voc- Zero Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn import preprocessing, model_selection, metrics, feature_selection, ensemble, linear_model, cross_decomposition, feature_extraction, decomposition\n",
    "import time\n",
    "from sklearn.externals import joblib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../../../train.pkl',compression='zip')\n",
    "test = pd.read_pickle('../../../test.pkl',compression='zip')\n",
    "# Russian stopwords\n",
    "ru_stop = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define discrete target boundaries\n",
    "i_0 = train[train.deal_probability==0].index.tolist()\n",
    "i_low = train[(train.deal_probability>0)&(train.deal_probability<0.65)].index.tolist()\n",
    "i_up = train[train.deal_probability>=0.65].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variable, index-range, discrete-range, voc-origin, and component-name\n",
    "var = 'title' # title or description\n",
    "irange = i_0 # pick from above cell\n",
    "rnge = 'zero' # zero, low or up\n",
    "voc_kind = 'idf' # count or idf\n",
    "compname = 'zeroidf' # custom name for component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the desired vocabulary onto a list of limited length\n",
    "voc = pd.read_pickle('{}_vocabs_{}.pkl'.format(var[:5],voc_kind))['{}_voc'.format(rnge)].dropna()[:67000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N tokens: 67000\n"
     ]
    }
   ],
   "source": [
    "vec = feature_extraction.text.TfidfVectorizer(\n",
    "    stop_words=ru_stop,\n",
    "    lowercase=False,\n",
    "    vocabulary=voc)\n",
    "# Fitting on train and test as merged lists\n",
    "vec.fit(train[var].astype(str).tolist() + test[var].astype(str).tolist())\n",
    "print('N tokens:',len(vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### vectors for train and test\n",
    "counts_train = vec.transform(train[var].astype(str).tolist())\n",
    "counts_test = vec.transform(test[var].astype(str).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### To start from zero...\n",
    "reduced_train = pd.DataFrame(index=train.index)\n",
    "reduced_test = pd.DataFrame(index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: 0-1500\n",
      "Prelim score for column range: 0.0767375171127994\n",
      "Aggregate cv: [0.07237674 0.06944764 0.07174497 0.07169012]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 2) (508438, 2)\n",
      "Columns: 1500-3000\n",
      "Prelim score for column range: 0.009651776421301548\n",
      "Aggregate cv: [0.07873665 0.07578788 0.07831682 0.07825956]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 4) (508438, 4)\n",
      "Columns: 3000-4500\n",
      "Prelim score for column range: 0.00703418889581886\n",
      "Aggregate cv: [0.08269613 0.0796202  0.08203387 0.08189988]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 6) (508438, 6)\n",
      "Columns: 4500-6000\n",
      "Prelim score for column range: 0.005639163197510787\n",
      "Aggregate cv: [0.08480908 0.08165625 0.08423592 0.08424105]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 8) (508438, 8)\n",
      "Columns: 6000-7500\n",
      "Prelim score for column range: 0.004841451820319587\n",
      "Aggregate cv: [0.08648068 0.08319056 0.08568377 0.08581158]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 10) (508438, 10)\n",
      "Interval save to disk...\n",
      "Columns: 7500-9000\n",
      "Prelim score for column range: 0.004075381511198906\n",
      "Aggregate cv: [0.08803598 0.08458707 0.08717538 0.08726127]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 12) (508438, 12)\n",
      "Columns: 9000-10500\n",
      "Prelim score for column range: 0.0035924560021023404\n",
      "Aggregate cv: [0.08956725 0.08618066 0.08869554 0.08888446]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 14) (508438, 14)\n",
      "Columns: 10500-12000\n",
      "Prelim score for column range: 0.003381385928280745\n",
      "Aggregate cv: [0.09036748 0.08704891 0.08959081 0.08962093]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 16) (508438, 16)\n",
      "Columns: 12000-13500\n",
      "Prelim score for column range: 0.002743128436629827\n",
      "Aggregate cv: [0.0910299  0.08787254 0.09040308 0.0903644 ]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 18) (508438, 18)\n",
      "Columns: 13500-15000\n",
      "Prelim score for column range: 0.0030014689618005352\n",
      "Aggregate cv: [0.09165006 0.08850953 0.09103495 0.0908536 ]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 20) (508438, 20)\n",
      "Interval decomposition...\n",
      "(1503424, 20) (508438, 20)\n",
      "Aggregate cv after decomposition: [0.09166284 0.088533   0.09105628 0.09088151]\n",
      "Interval save to disk...\n",
      "Columns: 15000-16500\n",
      "Prelim score for column range: 0.0021924440280229884\n",
      "Aggregate cv: [0.09269196 0.08967242 0.09213322 0.0920512 ]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 4) (508438, 4)\n",
      "Columns: 16500-18000\n",
      "Prelim score for column range: 0.0022670386952901023\n",
      "Aggregate cv: [0.09339808 0.09038893 0.09274746 0.09279605]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 6) (508438, 6)\n",
      "Columns: 18000-19500\n",
      "Prelim score for column range: 0.002327368383554518\n",
      "Aggregate cv: [0.09394092 0.09091268 0.09329037 0.09337291]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 8) (508438, 8)\n",
      "Columns: 19500-21000\n",
      "Prelim score for column range: 0.0021249196332621123\n",
      "Aggregate cv: [0.09441776 0.09128396 0.09376048 0.0937782 ]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 10) (508438, 10)\n",
      "Columns: 21000-22500\n",
      "Prelim score for column range: 0.0018239819584326966\n",
      "Aggregate cv: [0.09503626 0.09187843 0.0942085  0.09436388]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 12) (508438, 12)\n",
      "Interval save to disk...\n",
      "Columns: 22500-24000\n",
      "Prelim score for column range: 0.001935061037752761\n",
      "Aggregate cv: [0.09539307 0.09218011 0.09458854 0.0947698 ]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 14) (508438, 14)\n",
      "Columns: 24000-25500\n",
      "Prelim score for column range: 0.0016939155934314611\n",
      "Aggregate cv: [0.0956198  0.09233827 0.09486727 0.09498037]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 16) (508438, 16)\n",
      "Columns: 25500-27000\n",
      "Prelim score for column range: 0.001094471984473655\n",
      "Aggregate cv: [0.09647405 0.09328583 0.09566542 0.09586299]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 18) (508438, 18)\n",
      "Columns: 27000-28500\n",
      "Prelim score for column range: 0.0012932713991687939\n",
      "Aggregate cv: [0.09714941 0.09395571 0.09632315 0.09648004]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 20) (508438, 20)\n",
      "Columns: 28500-30000\n",
      "Prelim score for column range: 0.002263021436476831\n",
      "Aggregate cv: [0.09728262 0.09406236 0.09648665 0.0966157 ]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 22) (508438, 22)\n",
      "Interval decomposition...\n",
      "(1503424, 22) (508438, 22)\n",
      "Aggregate cv after decomposition: [0.09728943 0.09407648 0.09650973 0.09662297]\n",
      "Interval save to disk...\n",
      "Columns: 30000-31500\n",
      "Prelim score for column range: 0.0010830322537299653\n",
      "Aggregate cv: [0.09770832 0.0944749  0.09695758 0.0969311 ]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 4) (508438, 4)\n",
      "Columns: 31500-33000\n",
      "Prelim score for column range: 0.0009063775325102164\n",
      "Aggregate cv: [0.0982239  0.09502588 0.09752337 0.0974557 ]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 6) (508438, 6)\n",
      "Columns: 33000-34500\n",
      "Prelim score for column range: 0.0008564531812332855\n",
      "Aggregate cv: [0.09880402 0.09558637 0.09807549 0.09804969]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 8) (508438, 8)\n",
      "Columns: 34500-36000\n",
      "Prelim score for column range: 0.0008836398550606805\n",
      "Aggregate cv: [0.09940396 0.0960797  0.09869118 0.09861584]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 10) (508438, 10)\n",
      "Columns: 36000-37500\n",
      "Prelim score for column range: 0.0008385548685351241\n",
      "Aggregate cv: [0.09998139 0.09662355 0.09921817 0.09917305]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 12) (508438, 12)\n",
      "Interval save to disk...\n",
      "Columns: 37500-39000\n",
      "Prelim score for column range: 0.0008565167012658524\n",
      "Aggregate cv: [0.10026445 0.09694202 0.09952311 0.09945093]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 14) (508438, 14)\n",
      "Columns: 39000-40500\n",
      "Prelim score for column range: 0.00082721454622392\n",
      "Aggregate cv: [0.10051727 0.09710685 0.09969227 0.09963048]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 16) (508438, 16)\n",
      "Columns: 40500-42000\n",
      "Prelim score for column range: 0.0007323142867099453\n",
      "Aggregate cv: [0.10077418 0.0973306  0.09993206 0.09980639]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 18) (508438, 18)\n",
      "Columns: 42000-43500\n",
      "Prelim score for column range: 0.0005942602887952786\n",
      "Aggregate cv: [0.10108643 0.09764609 0.10021566 0.10011892]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 20) (508438, 20)\n",
      "Columns: 43500-45000\n",
      "Prelim score for column range: 0.0005449565150439373\n",
      "Aggregate cv: [0.10137099 0.09791755 0.10055102 0.10042603]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 22) (508438, 22)\n",
      "Interval decomposition...\n",
      "(1503424, 22) (508438, 22)\n",
      "Aggregate cv after decomposition: [0.10138094 0.0979223  0.10055429 0.10043331]\n",
      "Interval save to disk...\n",
      "Columns: 45000-46500\n",
      "Prelim score for column range: 0.0005742359621313531\n",
      "Aggregate cv: [0.1016537  0.09822395 0.10086647 0.1006904 ]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 4) (508438, 4)\n",
      "Columns: 46500-48000\n",
      "Prelim score for column range: 0.0005774424181180793\n",
      "Aggregate cv: [0.10193788 0.09851325 0.10117516 0.10097504]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 6) (508438, 6)\n",
      "Columns: 48000-49500\n",
      "Prelim score for column range: 0.0005643783214434039\n",
      "Aggregate cv: [0.10218118 0.09881314 0.10145953 0.10128088]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 8) (508438, 8)\n",
      "Columns: 49500-51000\n",
      "Prelim score for column range: 0.0005600627405796255\n",
      "Aggregate cv: [0.10243756 0.09906285 0.10175324 0.10155378]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 10) (508438, 10)\n",
      "Columns: 51000-52500\n",
      "Prelim score for column range: 0.0005364824252233058\n",
      "Aggregate cv: [0.10271242 0.09930908 0.10200347 0.10182572]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 12) (508438, 12)\n",
      "Interval save to disk...\n",
      "Columns: 52500-54000\n",
      "Prelim score for column range: 0.0005975817288994545\n",
      "Aggregate cv: [0.10301698 0.0996001  0.10228813 0.10210065]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 14) (508438, 14)\n",
      "Columns: 54000-55500\n",
      "Prelim score for column range: 0.0005617296806829097\n",
      "Aggregate cv: [0.10330231 0.09989665 0.10260625 0.10237786]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 16) (508438, 16)\n",
      "Columns: 55500-57000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prelim score for column range: 0.0005509756879108485\n",
      "Aggregate cv: [0.10360386 0.10016776 0.10291351 0.10263695]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 18) (508438, 18)\n",
      "Columns: 57000-58500\n",
      "Prelim score for column range: 0.0005654461256056065\n",
      "Aggregate cv: [0.10388842 0.10042094 0.10320837 0.10291276]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 20) (508438, 20)\n",
      "Columns: 58500-60000\n",
      "Prelim score for column range: 0.0006925368200940696\n",
      "Aggregate cv: [0.10397295 0.1005211  0.10331266 0.10298287]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 22) (508438, 22)\n",
      "Interval decomposition...\n",
      "(1503424, 22) (508438, 22)\n",
      "Aggregate cv after decomposition: [0.10397968 0.10052284 0.10331499 0.10298807]\n",
      "Interval save to disk...\n",
      "Columns: 60000-61500\n",
      "Prelim score for column range: 0.0006882795972371626\n",
      "Aggregate cv: [0.10406794 0.10061817 0.10338086 0.10305505]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 4) (508438, 4)\n",
      "Columns: 61500-63000\n",
      "Prelim score for column range: 0.0006537095907440671\n",
      "Aggregate cv: [0.10417073 0.10069719 0.10344813 0.10319538]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 6) (508438, 6)\n",
      "Columns: 63000-64500\n",
      "Prelim score for column range: 0.0003295134752381301\n",
      "Aggregate cv: [0.10423723 0.10077312 0.10350932 0.10326964]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 8) (508438, 8)\n",
      "Columns: 64500-66000\n",
      "Prelim score for column range: 0.00028380089999846536\n",
      "Aggregate cv: [0.10430797 0.10084823 0.10361674 0.10335059]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 10) (508438, 10)\n",
      "Columns: 66000-67000\n",
      "Prelim score for column range: 0.00020969842511653347\n",
      "Aggregate cv: [0.10436384 0.10091084 0.10368008 0.10341752]\n",
      "Aggregate score for zero range: [1. 1. 1. 1.]\n",
      "(1503424, 12) (508438, 12)\n",
      "Final decomposition...\n",
      "Aggregate cv after decomposition: [0.10436675 0.10091361 0.10368187 0.10342137]\n",
      "Minutes: 34.346240131060284\n"
     ]
    }
   ],
   "source": [
    "# Reduce all CSR values in batches\n",
    "t = time.time()\n",
    "start_col = 0\n",
    "varname = var[:5]\n",
    "##########################################\n",
    "n_cols = counts_train.shape[1]\n",
    "col_step = 1500\n",
    "col_end = n_cols + col_step\n",
    "##########################################\n",
    "# Start iteration with columns\n",
    "low_col = start_col\n",
    "corrupted = False\n",
    "for col in np.arange(0,col_end,col_step):\n",
    "    # Limiting the edge case of the last values\n",
    "    if col > n_cols:\n",
    "        col = n_cols\n",
    "    up_col = col\n",
    "\n",
    "    if up_col > low_col:\n",
    "        ###########################################\n",
    "        # Train PLSR on a large sample of train vectors\n",
    "        print('Columns: {}-{}'.format(low_col,up_col))\n",
    "        index = np.random.choice(len(train),size=int(4e5))\n",
    "        sample = counts_train[index,low_col:up_col].toarray()\n",
    "        reduce = cross_decomposition.PLSRegression(n_components=2)\n",
    "        reduce.fit(sample,train.iloc[index].deal_probability)\n",
    "        print('Prelim score for column range:',reduce.score(sample,train.iloc[index].deal_probability))\n",
    "        ##########################################\n",
    "        # (TRAIN) Nested indexes iteration\n",
    "        # Initial values:\n",
    "        n_rows = len(train)\n",
    "        row_step = int(2.5e5)\n",
    "        row_end = n_rows + row_step\n",
    "        components = pd.DataFrame()\n",
    "        low_idx = 0\n",
    "        ###########\n",
    "        for idx in np.arange(0,row_end,row_step):\n",
    "            # Limiting the edge case of the last values\n",
    "            if idx > n_rows:\n",
    "                idx = n_rows\n",
    "            up_idx = idx\n",
    "\n",
    "            if up_idx > low_idx:\n",
    "                sample = counts_train[low_idx:up_idx,low_col:up_col].toarray()\n",
    "                sample = reduce.transform(sample)\n",
    "                components = components.append(pd.DataFrame(sample))\n",
    "                low_idx = idx\n",
    "        components.reset_index(drop=True,inplace=True)\n",
    "        components.columns = ['col_{}-{}_{}'.format(low_col,up_col,i) for i in range(0,2)]\n",
    "\n",
    "        # Cross-validate and check for corruptions before joining\n",
    "        cv = model_selection.cross_val_score(\n",
    "            cv=4,estimator=linear_model.LinearRegression(),\n",
    "            X=reduced_train.join(components),y=train.deal_probability)\n",
    "\n",
    "        print('Aggregate cv:',cv)\n",
    "        print('Aggregate score for {} range:'.format(rnge),model_selection.cross_val_score(\n",
    "            cv=4,estimator=linear_model.LinearRegression(),\n",
    "            X=reduced_train.join(components).iloc[irange],y=train.iloc[irange].deal_probability))\n",
    "        \n",
    "        \n",
    "        if sum([score < 0 for score in cv]) > 0:\n",
    "            print('Reached corruption.\\n Final decomposition without joining...')\n",
    "            print(reduced_train.shape,reduced_test.shape)\n",
    "            reduce = cross_decomposition.PLSRegression(n_components=2)\n",
    "            reduce.fit(reduced_train,train.deal_probability)\n",
    "            reduced_train = pd.DataFrame(\n",
    "                reduce.transform(reduced_train),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "            reduced_test = pd.DataFrame(\n",
    "                reduce.transform(reduced_test),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "            print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "                cv=4,estimator=linear_model.LinearRegression(),\n",
    "                X=reduced_train,y=train.deal_probability))\n",
    "            print('Minutes:',(time.time()-t)/60)\n",
    "            break\n",
    "        \n",
    "        # Join if it wasn't corrupted..    \n",
    "        reduced_train = reduced_train.join(components)\n",
    "        ###########################################\n",
    "        # (TEST) Nested indexes iteration\n",
    "        # Initial values:\n",
    "        n_rows = len(test)\n",
    "        row_step = int(2e5)\n",
    "        row_end = n_rows + row_step\n",
    "        components = pd.DataFrame()\n",
    "        low_idx = 0\n",
    "        ###########\n",
    "        for idx in np.arange(0,row_end,row_step):\n",
    "            if idx > n_rows:\n",
    "                idx = n_rows\n",
    "            up_idx = idx\n",
    "\n",
    "            if up_idx > low_idx:\n",
    "                sample = counts_test[low_idx:up_idx,low_col:up_col].toarray()\n",
    "                sample = reduce.transform(sample)\n",
    "                components = components.append(pd.DataFrame(sample))\n",
    "                low_idx = idx\n",
    "        components.reset_index(drop=True,inplace=True)\n",
    "        components.columns = ['col_{}-{}_{}'.format(low_col,up_col,i) for i in range(0,2)]\n",
    "        reduced_test = reduced_test.join(components)\n",
    "        print(reduced_train.shape,reduced_test.shape)\n",
    "        #####################################\n",
    "        # Prepare for next column range\n",
    "        low_col = col     \n",
    "        #####################################    \n",
    "        # Interval decompositions\n",
    "        if up_col%(col_step*10) == 0:\n",
    "            print('Interval decomposition...')\n",
    "            print(reduced_train.shape,reduced_test.shape)\n",
    "            reduce = cross_decomposition.PLSRegression(n_components=2)\n",
    "            reduce.fit(reduced_train,train.deal_probability)\n",
    "            reduced_train = pd.DataFrame(\n",
    "                reduce.transform(reduced_train),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "            reduced_test = pd.DataFrame(\n",
    "                reduce.transform(reduced_test),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "            print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "                cv=4,estimator=linear_model.LinearRegression(),\n",
    "                X=reduced_train,y=train.deal_probability))\n",
    "        #####################################    \n",
    "        # Save progress every n steps\n",
    "        if up_col%(col_step*5) == 0:\n",
    "            print('Interval save to disk...')\n",
    "            joblib.dump(reduced_train,'train_{}_{}.sav'.format(varname,compname))\n",
    "            joblib.dump(reduced_test,'test_{}_{}.sav'.format(varname,compname))   \n",
    "#####################################\n",
    "# Final round of decomposition\n",
    "print('Final decomposition...')\n",
    "reduce = cross_decomposition.PLSRegression(n_components=2)\n",
    "reduce.fit(reduced_train,train.deal_probability)\n",
    "reduced_train = pd.DataFrame(\n",
    "    reduce.transform(reduced_train),\n",
    "    columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "reduced_test = pd.DataFrame(\n",
    "    reduce.transform(reduced_test),\n",
    "    columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "    cv=4,estimator=linear_model.LinearRegression(),\n",
    "    X=reduced_train,y=train.deal_probability))\n",
    "#########\n",
    "# Last save to disk\n",
    "joblib.dump(reduced_train,'train_{}_{}.sav'.format(varname,compname))\n",
    "joblib.dump(reduced_test,'test_{}_{}.sav'.format(varname,compname))\n",
    "#########\n",
    "print('Minutes:',(time.time()-t)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> RESTART THE KERNEL\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title var- IDF voc- Lower Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn import preprocessing, model_selection, metrics, feature_selection, ensemble, linear_model, cross_decomposition, feature_extraction, decomposition\n",
    "import time\n",
    "from sklearn.externals import joblib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../../../train.pkl',compression='zip')\n",
    "test = pd.read_pickle('../../../test.pkl',compression='zip')\n",
    "# Russian stopwords\n",
    "ru_stop = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define discrete target boundaries\n",
    "i_0 = train[train.deal_probability==0].index.tolist()\n",
    "i_low = train[(train.deal_probability>0)&(train.deal_probability<0.65)].index.tolist()\n",
    "i_up = train[train.deal_probability>=0.65].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variable, index-range, discrete-range, voc-origin, and component-name\n",
    "var = 'title' # title or description\n",
    "irange = i_low # pick from above cell\n",
    "rnge = 'low' # zero, low or up\n",
    "voc_kind = 'idf' # count or idf\n",
    "compname = 'lowidf' # custom name for component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the desired vocabulary onto a list of limited length\n",
    "voc = pd.read_pickle('{}_vocabs_{}.pkl'.format(var[:5],voc_kind))['{}_voc'.format(rnge)].dropna()[:67000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N tokens: 35363\n"
     ]
    }
   ],
   "source": [
    "vec = feature_extraction.text.TfidfVectorizer(\n",
    "    stop_words=ru_stop,\n",
    "    lowercase=False,\n",
    "    vocabulary=voc)\n",
    "# Fitting on train and test as merged lists\n",
    "vec.fit(train[var].astype(str).tolist() + test[var].astype(str).tolist())\n",
    "print('N tokens:',len(vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### vectors for train and test\n",
    "counts_train = vec.transform(train[var].astype(str).tolist())\n",
    "counts_test = vec.transform(test[var].astype(str).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### To start from zero...\n",
    "reduced_train = pd.DataFrame(index=train.index)\n",
    "reduced_test = pd.DataFrame(index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: 0-1500\n",
      "Prelim score for column range: 0.005390740582564391\n",
      "Aggregate cv: [0.0026094  0.00277017 0.00251395 0.00236   ]\n",
      "Aggregate score for low range: [0.00591272 0.00544688 0.00489253 0.00374499]\n",
      "(1503424, 2) (508438, 2)\n",
      "Columns: 1500-3000\n",
      "Prelim score for column range: 0.0010543684435517786\n",
      "Aggregate cv: [0.00275112 0.00280599 0.00263428 0.00246088]\n",
      "Aggregate score for low range: [0.00766209 0.00633327 0.00639631 0.00487442]\n",
      "(1503424, 4) (508438, 4)\n",
      "Columns: 3000-4500\n",
      "Prelim score for column range: 0.000679906829489374\n",
      "Aggregate cv: [0.00294102 0.00296647 0.00279493 0.0026183 ]\n",
      "Aggregate score for low range: [0.00917883 0.00750882 0.00759257 0.00604975]\n",
      "(1503424, 6) (508438, 6)\n",
      "Columns: 4500-6000\n",
      "Prelim score for column range: 0.0007005460428868293\n",
      "Aggregate cv: [0.00294864 0.00301088 0.0028186  0.00265559]\n",
      "Aggregate score for low range: [0.00940135 0.00839502 0.00830197 0.0068365 ]\n",
      "(1503424, 8) (508438, 8)\n",
      "Columns: 6000-7500\n",
      "Prelim score for column range: 0.00036082500646450466\n",
      "Aggregate cv: [0.00296995 0.00305365 0.00288699 0.00270513]\n",
      "Aggregate score for low range: [0.00978118 0.00888113 0.00901283 0.00749133]\n",
      "(1503424, 10) (508438, 10)\n",
      "Interval save to disk...\n",
      "Columns: 7500-9000\n",
      "Prelim score for column range: 0.0002962190046008617\n",
      "Aggregate cv: [0.00301335 0.00312344 0.00293776 0.00278425]\n",
      "Aggregate score for low range: [0.01021882 0.00952979 0.00942234 0.0082348 ]\n",
      "(1503424, 12) (508438, 12)\n",
      "Columns: 9000-10500\n",
      "Prelim score for column range: 0.00034702816176579443\n",
      "Aggregate cv: [0.00308238 0.00320447 0.00304324 0.00284867]\n",
      "Aggregate score for low range: [0.01085486 0.01022646 0.01042005 0.00886509]\n",
      "(1503424, 14) (508438, 14)\n",
      "Columns: 10500-12000\n",
      "Prelim score for column range: 0.000392714048885967\n",
      "Aggregate cv: [0.00316458 0.00331623 0.00311681 0.00290669]\n",
      "Aggregate score for low range: [0.01167928 0.01126309 0.01107104 0.00936325]\n",
      "(1503424, 16) (508438, 16)\n",
      "Columns: 12000-13500\n",
      "Prelim score for column range: 0.00043624371125783323\n",
      "Aggregate cv: [0.0032634  0.00342991 0.00317526 0.00300357]\n",
      "Aggregate score for low range: [0.01266339 0.01233823 0.01159984 0.01036489]\n",
      "(1503424, 18) (508438, 18)\n",
      "Columns: 13500-15000\n",
      "Prelim score for column range: 0.0003066872314246849\n",
      "Aggregate cv: [0.00332783 0.00348189 0.00324324 0.00307115]\n",
      "Aggregate score for low range: [-7.04363687  0.01277185  0.01215121  0.01095893]\n",
      "(1503424, 20) (508438, 20)\n",
      "Interval decomposition...\n",
      "(1503424, 20) (508438, 20)\n",
      "Aggregate cv after decomposition: [0.00334546 0.00349035 0.00324623 0.00307761]\n",
      "Interval save to disk...\n",
      "Columns: 15000-16500\n",
      "Prelim score for column range: 0.00039360352393658093\n",
      "Aggregate cv: [0.0033866  0.0036024  0.00332723 0.00317486]\n",
      "Aggregate score for low range: [0.01286972 0.01280002 0.01147956 0.0105036 ]\n",
      "(1503424, 4) (508438, 4)\n",
      "Columns: 16500-18000\n",
      "Prelim score for column range: 0.0004179822146326284\n",
      "Aggregate cv: [0.00348149 0.00370115 0.00344157 0.00324495]\n",
      "Aggregate score for low range: [0.01376753 0.01379432 0.01247028 0.01119725]\n",
      "(1503424, 6) (508438, 6)\n",
      "Columns: 18000-19500\n",
      "Prelim score for column range: 0.0004659851199545484\n",
      "Aggregate cv: [0.00360058 0.00378936 0.00353425 0.00332172]\n",
      "Aggregate score for low range: [0.01487648 0.01455352 0.0132829  0.01194589]\n",
      "(1503424, 8) (508438, 8)\n",
      "Columns: 19500-21000\n",
      "Prelim score for column range: 0.0003525076555626061\n",
      "Aggregate cv: [0.00370804 0.00387553 0.00357895 0.00336729]\n",
      "Aggregate score for low range: [0.01589446 0.01537602 0.01375632 0.0123501 ]\n",
      "(1503424, 10) (508438, 10)\n",
      "Columns: 21000-22500\n",
      "Prelim score for column range: 0.0002277769998483281\n",
      "Aggregate cv: [0.00376435 0.00396059 0.00362225 0.00340607]\n",
      "Aggregate score for low range: [0.01637944 0.01626324 0.01417656 0.01271193]\n",
      "(1503424, 12) (508438, 12)\n",
      "Interval save to disk...\n",
      "Columns: 22500-24000\n",
      "Prelim score for column range: 0.00023659938777820244\n",
      "Aggregate cv: [0.00380526 0.00402554 0.00367955 0.00345924]\n",
      "Aggregate score for low range: [0.01679066 0.01687058 0.01475551 0.01314951]\n",
      "(1503424, 14) (508438, 14)\n",
      "Columns: 24000-25500\n",
      "Prelim score for column range: 0.0002758079120560586\n",
      "Aggregate cv: [0.00386234 0.00409062 0.0037475  0.00352125]\n",
      "Aggregate score for low range: [0.01725639 0.01751676 0.01542593 0.0137856 ]\n",
      "(1503424, 16) (508438, 16)\n",
      "Columns: 25500-27000\n",
      "Prelim score for column range: 0.0002527031983571293\n",
      "Aggregate cv: [0.00392931 0.00415149 0.0037951  0.00357624]\n",
      "Aggregate score for low range: [0.01796385 0.01809526 0.01586794 0.01437079]\n",
      "(1503424, 18) (508438, 18)\n",
      "Columns: 27000-28500\n",
      "Prelim score for column range: 0.0002863945733991846\n",
      "Aggregate cv: [0.00399427 0.00420052 0.00385706 0.00363448]\n",
      "Aggregate score for low range: [0.0185417  0.01853142 0.01641316 0.01500405]\n",
      "(1503424, 20) (508438, 20)\n",
      "Columns: 28500-30000\n",
      "Prelim score for column range: 0.0004199886434951283\n",
      "Aggregate cv: [0.0040838  0.00431713 0.00394802 0.00368246]\n",
      "Aggregate score for low range: [0.01936861 0.01966058 0.01728474 0.0154364 ]\n",
      "(1503424, 22) (508438, 22)\n",
      "Interval decomposition...\n",
      "(1503424, 22) (508438, 22)\n",
      "Aggregate cv after decomposition: [0.00408528 0.00431899 0.00394871 0.0036848 ]\n",
      "Interval save to disk...\n",
      "Columns: 30000-31500\n",
      "Prelim score for column range: 0.0003443033581439492\n",
      "Aggregate cv: [0.00414409 0.00438915 0.00401353 0.00375626]\n",
      "Aggregate score for low range: [0.01957326 0.01986336 0.01771398 0.0156404 ]\n",
      "(1503424, 4) (508438, 4)\n",
      "Columns: 31500-33000\n",
      "Prelim score for column range: 0.0003177537623633153\n",
      "Aggregate cv: [0.00423168 0.00444899 0.00406776 0.00383653]\n",
      "Aggregate score for low range: [0.02035636 0.02031659 0.0182295  0.01636161]\n",
      "(1503424, 6) (508438, 6)\n",
      "Columns: 33000-34500\n",
      "Prelim score for column range: 0.0004061095537423176\n",
      "Aggregate cv: [0.00429698 0.00450852 0.00413484 0.00395868]\n",
      "Aggregate score for low range: [0.02101298 0.02078771 0.01883106 0.01761067]\n",
      "(1503424, 8) (508438, 8)\n",
      "Columns: 34500-35363\n",
      "Prelim score for column range: 0.00020577274812705326\n",
      "Aggregate cv: [0.00432782 0.00454547 0.00419645 0.0039866 ]\n",
      "Aggregate score for low range: [0.02139702 0.02121539 0.01949251 0.018057  ]\n",
      "(1503424, 10) (508438, 10)\n",
      "Final decomposition...\n",
      "Aggregate cv after decomposition: [0.00432857 0.00454585 0.0041971  0.00398768]\n",
      "Minutes: 17.498896185557047\n"
     ]
    }
   ],
   "source": [
    "# Reduce all CSR values in batches\n",
    "t = time.time()\n",
    "start_col = 0\n",
    "varname = var[:5]\n",
    "##########################################\n",
    "n_cols = counts_train.shape[1]\n",
    "col_step = 1500\n",
    "col_end = n_cols + col_step\n",
    "##########################################\n",
    "# Start iteration with columns\n",
    "low_col = start_col\n",
    "corrupted = False\n",
    "for col in np.arange(0,col_end,col_step):\n",
    "    # Limiting the edge case of the last values\n",
    "    if col > n_cols:\n",
    "        col = n_cols\n",
    "    up_col = col\n",
    "\n",
    "    if up_col > low_col:\n",
    "        ###########################################\n",
    "        # Train PLSR on a large sample of train vectors\n",
    "        print('Columns: {}-{}'.format(low_col,up_col))\n",
    "        index = np.random.choice(len(train),size=int(4e5))\n",
    "        sample = counts_train[index,low_col:up_col].toarray()\n",
    "        reduce = cross_decomposition.PLSRegression(n_components=2)\n",
    "        reduce.fit(sample,train.iloc[index].deal_probability)\n",
    "        print('Prelim score for column range:',reduce.score(sample,train.iloc[index].deal_probability))\n",
    "        ##########################################\n",
    "        # (TRAIN) Nested indexes iteration\n",
    "        # Initial values:\n",
    "        n_rows = len(train)\n",
    "        row_step = int(2.5e5)\n",
    "        row_end = n_rows + row_step\n",
    "        components = pd.DataFrame()\n",
    "        low_idx = 0\n",
    "        ###########\n",
    "        for idx in np.arange(0,row_end,row_step):\n",
    "            # Limiting the edge case of the last values\n",
    "            if idx > n_rows:\n",
    "                idx = n_rows\n",
    "            up_idx = idx\n",
    "\n",
    "            if up_idx > low_idx:\n",
    "                sample = counts_train[low_idx:up_idx,low_col:up_col].toarray()\n",
    "                sample = reduce.transform(sample)\n",
    "                components = components.append(pd.DataFrame(sample))\n",
    "                low_idx = idx\n",
    "        components.reset_index(drop=True,inplace=True)\n",
    "        components.columns = ['col_{}-{}_{}'.format(low_col,up_col,i) for i in range(0,2)]\n",
    "\n",
    "        # Cross-validate and check for corruptions before joining\n",
    "        cv = model_selection.cross_val_score(\n",
    "            cv=4,estimator=linear_model.LinearRegression(),\n",
    "            X=reduced_train.join(components),y=train.deal_probability)\n",
    "\n",
    "        print('Aggregate cv:',cv)\n",
    "        print('Aggregate score for {} range:'.format(rnge),model_selection.cross_val_score(\n",
    "            cv=4,estimator=linear_model.LinearRegression(),\n",
    "            X=reduced_train.join(components).iloc[irange],y=train.iloc[irange].deal_probability))\n",
    "        \n",
    "        \n",
    "        if sum([score < 0 for score in cv]) > 0:\n",
    "            print('Reached corruption.\\n Final decomposition without joining...')\n",
    "            print(reduced_train.shape,reduced_test.shape)\n",
    "            reduce = cross_decomposition.PLSRegression(n_components=2)\n",
    "            reduce.fit(reduced_train,train.deal_probability)\n",
    "            reduced_train = pd.DataFrame(\n",
    "                reduce.transform(reduced_train),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "            reduced_test = pd.DataFrame(\n",
    "                reduce.transform(reduced_test),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "            print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "                cv=4,estimator=linear_model.LinearRegression(),\n",
    "                X=reduced_train,y=train.deal_probability))\n",
    "            print('Minutes:',(time.time()-t)/60)\n",
    "            break\n",
    "        \n",
    "        # Join if it wasn't corrupted..    \n",
    "        reduced_train = reduced_train.join(components)\n",
    "        ###########################################\n",
    "        # (TEST) Nested indexes iteration\n",
    "        # Initial values:\n",
    "        n_rows = len(test)\n",
    "        row_step = int(2e5)\n",
    "        row_end = n_rows + row_step\n",
    "        components = pd.DataFrame()\n",
    "        low_idx = 0\n",
    "        ###########\n",
    "        for idx in np.arange(0,row_end,row_step):\n",
    "            if idx > n_rows:\n",
    "                idx = n_rows\n",
    "            up_idx = idx\n",
    "\n",
    "            if up_idx > low_idx:\n",
    "                sample = counts_test[low_idx:up_idx,low_col:up_col].toarray()\n",
    "                sample = reduce.transform(sample)\n",
    "                components = components.append(pd.DataFrame(sample))\n",
    "                low_idx = idx\n",
    "        components.reset_index(drop=True,inplace=True)\n",
    "        components.columns = ['col_{}-{}_{}'.format(low_col,up_col,i) for i in range(0,2)]\n",
    "        reduced_test = reduced_test.join(components)\n",
    "        print(reduced_train.shape,reduced_test.shape)\n",
    "        #####################################\n",
    "        # Prepare for next column range\n",
    "        low_col = col     \n",
    "        #####################################    \n",
    "        # Interval decompositions\n",
    "        if up_col%(col_step*10) == 0:\n",
    "            print('Interval decomposition...')\n",
    "            print(reduced_train.shape,reduced_test.shape)\n",
    "            reduce = cross_decomposition.PLSRegression(n_components=2)\n",
    "            reduce.fit(reduced_train,train.deal_probability)\n",
    "            reduced_train = pd.DataFrame(\n",
    "                reduce.transform(reduced_train),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "            reduced_test = pd.DataFrame(\n",
    "                reduce.transform(reduced_test),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "            print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "                cv=4,estimator=linear_model.LinearRegression(),\n",
    "                X=reduced_train,y=train.deal_probability))\n",
    "        #####################################    \n",
    "        # Save progress every n steps\n",
    "        if up_col%(col_step*5) == 0:\n",
    "            print('Interval save to disk...')\n",
    "            joblib.dump(reduced_train,'train_{}_{}.sav'.format(varname,compname))\n",
    "            joblib.dump(reduced_test,'test_{}_{}.sav'.format(varname,compname))   \n",
    "#####################################\n",
    "# Final round of decomposition\n",
    "print('Final decomposition...')\n",
    "reduce = cross_decomposition.PLSRegression(n_components=2)\n",
    "reduce.fit(reduced_train,train.deal_probability)\n",
    "reduced_train = pd.DataFrame(\n",
    "    reduce.transform(reduced_train),\n",
    "    columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "reduced_test = pd.DataFrame(\n",
    "    reduce.transform(reduced_test),\n",
    "    columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "    cv=4,estimator=linear_model.LinearRegression(),\n",
    "    X=reduced_train,y=train.deal_probability))\n",
    "#########\n",
    "# Last save to disk\n",
    "joblib.dump(reduced_train,'train_{}_{}.sav'.format(varname,compname))\n",
    "joblib.dump(reduced_test,'test_{}_{}.sav'.format(varname,compname))\n",
    "#########\n",
    "print('Minutes:',(time.time()-t)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> RESTART THE KERNEL\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title var- IDF voc- Upper Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn import preprocessing, model_selection, metrics, feature_selection, ensemble, linear_model, cross_decomposition, feature_extraction, decomposition\n",
    "import time\n",
    "from sklearn.externals import joblib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../../../train.pkl',compression='zip')\n",
    "test = pd.read_pickle('../../../test.pkl',compression='zip')\n",
    "# Russian stopwords\n",
    "ru_stop = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define discrete target boundaries\n",
    "i_0 = train[train.deal_probability==0].index.tolist()\n",
    "i_low = train[(train.deal_probability>0)&(train.deal_probability<0.65)].index.tolist()\n",
    "i_up = train[train.deal_probability>=0.65].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variable, index-range, discrete-range, voc-origin, and component-name\n",
    "var = 'title' # title or description\n",
    "irange = i_low # pick from above cell\n",
    "rnge = 'up' # zero, low or up\n",
    "voc_kind = 'idf' # count or idf\n",
    "compname = 'upidf' # custom name for component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the desired vocabulary onto a list of limited length\n",
    "voc = pd.read_pickle('{}_vocabs_{}.pkl'.format(var[:5],voc_kind))['{}_voc'.format(rnge)].dropna()[:67000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N tokens: 44922\n"
     ]
    }
   ],
   "source": [
    "vec = feature_extraction.text.TfidfVectorizer(\n",
    "    stop_words=ru_stop,\n",
    "    lowercase=False,\n",
    "    vocabulary=voc)\n",
    "# Fitting on train and test as merged lists\n",
    "vec.fit(train[var].astype(str).tolist() + test[var].astype(str).tolist())\n",
    "print('N tokens:',len(vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### vectors for train and test\n",
    "counts_train = vec.transform(train[var].astype(str).tolist())\n",
    "counts_test = vec.transform(test[var].astype(str).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### To start from zero...\n",
    "reduced_train = pd.DataFrame(index=train.index)\n",
    "reduced_test = pd.DataFrame(index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: 0-1500\n",
      "Prelim score for column range: 0.11858489496328795\n",
      "Aggregate cv: [0.10933183 0.10539125 0.10812998 0.109473  ]\n",
      "Aggregate score for up range: [0.05540884 0.05498032 0.05727032 0.05326966]\n",
      "(1503424, 2) (508438, 2)\n",
      "Columns: 1500-3000\n",
      "Prelim score for column range: 0.02710887262936812\n",
      "Aggregate cv: [0.1210894  0.11648385 0.11984331 0.12060689]\n",
      "Aggregate score for up range: [0.06668404 0.06648953 0.06898359 0.06483578]\n",
      "(1503424, 4) (508438, 4)\n",
      "Columns: 3000-4500\n",
      "Prelim score for column range: 0.01726824714411901\n",
      "Aggregate cv: [0.1264256  0.12211052 0.12501362 0.12600558]\n",
      "Aggregate score for up range: [0.07270501 0.07218492 0.07495111 0.07031932]\n",
      "(1503424, 6) (508438, 6)\n",
      "Columns: 4500-6000\n",
      "Prelim score for column range: 0.013728873916658002\n",
      "Aggregate cv: [0.1291012  0.12516947 0.12816621 0.12895467]\n",
      "Aggregate score for up range: [0.07515278 0.07436282 0.07774894 0.07348382]\n",
      "(1503424, 8) (508438, 8)\n",
      "Columns: 6000-7500\n",
      "Prelim score for column range: 0.012114271209859973\n",
      "Aggregate cv: [0.13109123 0.1272459  0.13015447 0.13113928]\n",
      "Aggregate score for up range: [0.07694794 0.07595585 0.07935919 0.07549061]\n",
      "(1503424, 10) (508438, 10)\n",
      "Interval save to disk...\n",
      "Columns: 7500-9000\n",
      "Prelim score for column range: 0.009630029914630356\n",
      "Aggregate cv: [0.13215352 0.12850653 0.13145972 0.13226517]\n",
      "Aggregate score for up range: [0.07866389 0.07718901 0.08086882 0.07644053]\n",
      "(1503424, 12) (508438, 12)\n",
      "Columns: 9000-10500\n",
      "Prelim score for column range: 0.010161901334214685\n",
      "Aggregate cv: [0.13366922 0.13011477 0.13282115 0.13349945]\n",
      "Aggregate score for up range: [0.07894152 0.07750868 0.08119232 0.07659183]\n",
      "(1503424, 14) (508438, 14)\n",
      "Columns: 10500-12000\n",
      "Prelim score for column range: 0.009331053462045769\n",
      "Aggregate cv: [0.13436084 0.13078109 0.13338948 0.13434607]\n",
      "Aggregate score for up range: [0.07924256 0.07801988 0.08192696 0.07729627]\n",
      "(1503424, 16) (508438, 16)\n",
      "Columns: 12000-13500\n",
      "Prelim score for column range: 0.010022112809596419\n",
      "Aggregate cv: [0.13527815 0.13163607 0.13437845 0.13531591]\n",
      "Aggregate score for up range: [0.07949405 0.07829261 0.08199484 0.07743797]\n",
      "(1503424, 18) (508438, 18)\n",
      "Columns: 13500-15000\n",
      "Prelim score for column range: 0.00718392801759693\n",
      "Aggregate cv: [0.13560353 0.13179711 0.13471601 0.13556093]\n",
      "Aggregate score for up range: [0.07980001 0.07847691 0.08202956 0.07787612]\n",
      "(1503424, 20) (508438, 20)\n",
      "Interval decomposition...\n",
      "(1503424, 20) (508438, 20)\n",
      "Aggregate cv after decomposition: [0.13564917 0.13182793 0.13477147 0.13560519]\n",
      "Interval save to disk...\n",
      "Columns: 15000-16500\n",
      "Prelim score for column range: 0.006627041555811841\n",
      "Aggregate cv: [0.1358475  0.13199723 0.13508612 0.13600942]\n",
      "Aggregate score for up range: [0.07948589 0.07844797 0.08194091 0.07826609]\n",
      "(1503424, 4) (508438, 4)\n",
      "Columns: 16500-18000\n",
      "Prelim score for column range: 0.00713320381710425\n",
      "Aggregate cv: [0.13671776 0.13295012 0.13593151 0.13691226]\n",
      "Aggregate score for up range: [0.07949079 0.07844203 0.08194442 0.07823846]\n",
      "(1503424, 6) (508438, 6)\n",
      "Columns: 18000-19500\n",
      "Prelim score for column range: 0.006977321303054729\n",
      "Aggregate cv: [0.13835622 0.13434143 0.13735684 0.13836458]\n",
      "Aggregate score for up range: [0.07949079 0.07844203 0.08194442 0.07823846]\n",
      "(1503424, 8) (508438, 8)\n",
      "Columns: 19500-21000\n",
      "Prelim score for column range: 0.0066237993077999935\n",
      "Aggregate cv: [0.13969156 0.13567149 0.13872478 0.13982742]\n",
      "Aggregate score for up range: [0.07949079 0.07844203 0.08194442 0.07823846]\n",
      "(1503424, 10) (508438, 10)\n",
      "Columns: 21000-22500\n",
      "Prelim score for column range: 0.007236622388884673\n",
      "Aggregate cv: [0.14126588 0.13698554 0.14028612 0.1412287 ]\n",
      "Aggregate score for up range: [0.07949079 0.07844203 0.08194442 0.07823846]\n",
      "(1503424, 12) (508438, 12)\n",
      "Interval save to disk...\n",
      "Columns: 22500-24000\n",
      "Prelim score for column range: 0.006227818284350262\n",
      "Aggregate cv: [0.14283315 0.13841061 0.14169871 0.14247563]\n",
      "Aggregate score for up range: [0.07949079 0.07844203 0.08194442 0.07823846]\n",
      "(1503424, 14) (508438, 14)\n",
      "Columns: 24000-25500\n",
      "Prelim score for column range: 0.006044226555499632\n",
      "Aggregate cv: [0.1441035  0.13969392 0.14317    0.14372159]\n",
      "Aggregate score for up range: [0.07949079 0.07844203 0.08194442 0.07823846]\n",
      "(1503424, 16) (508438, 16)\n",
      "Columns: 25500-27000\n",
      "Prelim score for column range: 0.005874643830131143\n",
      "Aggregate cv: [0.14524907 0.14097538 0.14394991 0.14488123]\n",
      "Aggregate score for up range: [0.07949079 0.07844203 0.08194442 0.07823846]\n",
      "(1503424, 18) (508438, 18)\n",
      "Columns: 27000-28500\n",
      "Prelim score for column range: 0.005828933011309045\n",
      "Aggregate cv: [0.1464453  0.14212375 0.14535586 0.14628716]\n",
      "Aggregate score for up range: [0.07949079 0.07844203 0.08194442 0.07823846]\n",
      "(1503424, 20) (508438, 20)\n",
      "Columns: 28500-30000\n",
      "Prelim score for column range: 0.006499619551840863\n",
      "Aggregate cv: [0.14757354 0.14366901 0.14668804 0.14775543]\n",
      "Aggregate score for up range: [0.07949079 0.07844203 0.08194442 0.07823846]\n",
      "(1503424, 22) (508438, 22)\n",
      "Interval decomposition...\n",
      "(1503424, 22) (508438, 22)\n",
      "Aggregate cv after decomposition: [0.14759393 0.14368908 0.14709638 0.14778612]\n",
      "Interval save to disk...\n",
      "Columns: 30000-31500\n",
      "Prelim score for column range: 0.005947175831938933\n",
      "Aggregate cv: [0.14784194 0.14388859 0.14730552 0.14790351]\n",
      "Aggregate score for up range: [0.07751053 0.03297234 0.08005642 0.07610987]\n",
      "(1503424, 4) (508438, 4)\n",
      "Columns: 31500-33000\n",
      "Prelim score for column range: 0.0058596719825942944\n",
      "Aggregate cv: [0.14792349 0.14411178 0.14753605 0.14811486]\n",
      "Aggregate score for up range: [0.07762405 0.03239251 0.08038922 0.07620055]\n",
      "(1503424, 6) (508438, 6)\n",
      "Columns: 33000-34500\n",
      "Prelim score for column range: 0.005540002611140182\n",
      "Aggregate cv: [0.14798398 0.14434427 0.14770915 0.14827853]\n",
      "Aggregate score for up range: [0.07760897 0.0374557  0.08025379 0.07625423]\n",
      "(1503424, 8) (508438, 8)\n",
      "Columns: 34500-36000\n",
      "Prelim score for column range: 0.005535535220856591\n",
      "Aggregate cv: [0.14806621 0.14457792 0.14784545 0.14844192]\n",
      "Aggregate score for up range: [0.07771742 0.03661855 0.08032559 0.07629683]\n",
      "(1503424, 10) (508438, 10)\n",
      "Columns: 36000-37500\n",
      "Prelim score for column range: 0.005160309608899127\n",
      "Aggregate cv: [0.14823869 0.1446024  0.14804686 0.14856313]\n",
      "Aggregate score for up range: [0.07782069 0.03368094 0.08039246 0.07637895]\n",
      "(1503424, 12) (508438, 12)\n",
      "Interval save to disk...\n",
      "Columns: 37500-39000\n",
      "Prelim score for column range: 0.005435709774063158\n",
      "Aggregate cv: [0.14837909 0.1448366  0.14811993 0.14872707]\n",
      "Aggregate score for up range: [-0.07456893  0.0334912   0.08042416  0.07642143]\n",
      "(1503424, 14) (508438, 14)\n",
      "Columns: 39000-40500\n",
      "Prelim score for column range: 0.004546975417754395\n",
      "Aggregate cv: [0.14849101 0.14493104 0.14818682 0.14881366]\n",
      "Aggregate score for up range: [-0.06999656  0.03463085  0.08055408  0.07660721]\n",
      "(1503424, 16) (508438, 16)\n",
      "Columns: 40500-42000\n",
      "Prelim score for column range: 0.004632765856448451\n",
      "Aggregate cv: [0.14850867 0.14505265 0.14820459 0.14889717]\n",
      "Aggregate score for up range: [-0.07412043  0.03426544  0.08080286  0.07684552]\n",
      "(1503424, 18) (508438, 18)\n",
      "Columns: 42000-43500\n",
      "Prelim score for column range: 0.004499403374632505\n",
      "Aggregate cv: [0.14851827 0.14514538 0.14825083 0.14894925]\n",
      "Aggregate score for up range: [-0.07269894  0.03226617  0.08099564  0.07695293]\n",
      "(1503424, 20) (508438, 20)\n",
      "Columns: 43500-44922\n",
      "Prelim score for column range: 0.004104030151633631\n",
      "Aggregate cv: [0.14854923 0.14521918 0.14830758 0.14899019]\n",
      "Aggregate score for up range: [-0.07332437  0.03271174  0.08098909  0.07711739]\n",
      "(1503424, 22) (508438, 22)\n",
      "Final decomposition...\n",
      "Aggregate cv after decomposition: [0.14861952 0.14530396 0.14835486 0.14901805]\n",
      "Minutes: 22.070170601209004\n"
     ]
    }
   ],
   "source": [
    "# Reduce all CSR values in batches\n",
    "t = time.time()\n",
    "start_col = 0\n",
    "varname = var[:5]\n",
    "##########################################\n",
    "n_cols = counts_train.shape[1]\n",
    "col_step = 1500\n",
    "col_end = n_cols + col_step\n",
    "##########################################\n",
    "# Start iteration with columns\n",
    "low_col = start_col\n",
    "corrupted = False\n",
    "for col in np.arange(0,col_end,col_step):\n",
    "    # Limiting the edge case of the last values\n",
    "    if col > n_cols:\n",
    "        col = n_cols\n",
    "    up_col = col\n",
    "\n",
    "    if up_col > low_col:\n",
    "        ###########################################\n",
    "        # Train PLSR on a large sample of train vectors\n",
    "        print('Columns: {}-{}'.format(low_col,up_col))\n",
    "        index = np.random.choice(len(train),size=int(4e5))\n",
    "        sample = counts_train[index,low_col:up_col].toarray()\n",
    "        reduce = cross_decomposition.PLSRegression(n_components=2)\n",
    "        reduce.fit(sample,train.iloc[index].deal_probability)\n",
    "        print('Prelim score for column range:',reduce.score(sample,train.iloc[index].deal_probability))\n",
    "        ##########################################\n",
    "        # (TRAIN) Nested indexes iteration\n",
    "        # Initial values:\n",
    "        n_rows = len(train)\n",
    "        row_step = int(2.5e5)\n",
    "        row_end = n_rows + row_step\n",
    "        components = pd.DataFrame()\n",
    "        low_idx = 0\n",
    "        ###########\n",
    "        for idx in np.arange(0,row_end,row_step):\n",
    "            # Limiting the edge case of the last values\n",
    "            if idx > n_rows:\n",
    "                idx = n_rows\n",
    "            up_idx = idx\n",
    "\n",
    "            if up_idx > low_idx:\n",
    "                sample = counts_train[low_idx:up_idx,low_col:up_col].toarray()\n",
    "                sample = reduce.transform(sample)\n",
    "                components = components.append(pd.DataFrame(sample))\n",
    "                low_idx = idx\n",
    "        components.reset_index(drop=True,inplace=True)\n",
    "        components.columns = ['col_{}-{}_{}'.format(low_col,up_col,i) for i in range(0,2)]\n",
    "\n",
    "        # Cross-validate and check for corruptions before joining\n",
    "        cv = model_selection.cross_val_score(\n",
    "            cv=4,estimator=linear_model.LinearRegression(),\n",
    "            X=reduced_train.join(components),y=train.deal_probability)\n",
    "\n",
    "        print('Aggregate cv:',cv)\n",
    "        print('Aggregate score for {} range:'.format(rnge),model_selection.cross_val_score(\n",
    "            cv=4,estimator=linear_model.LinearRegression(),\n",
    "            X=reduced_train.join(components).iloc[irange],y=train.iloc[irange].deal_probability))\n",
    "        \n",
    "        \n",
    "        if sum([score < 0 for score in cv]) > 0:\n",
    "            print('Reached corruption.\\n Final decomposition without joining...')\n",
    "            print(reduced_train.shape,reduced_test.shape)\n",
    "            reduce = cross_decomposition.PLSRegression(n_components=2)\n",
    "            reduce.fit(reduced_train,train.deal_probability)\n",
    "            reduced_train = pd.DataFrame(\n",
    "                reduce.transform(reduced_train),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "            reduced_test = pd.DataFrame(\n",
    "                reduce.transform(reduced_test),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "            print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "                cv=4,estimator=linear_model.LinearRegression(),\n",
    "                X=reduced_train,y=train.deal_probability))\n",
    "            print('Minutes:',(time.time()-t)/60)\n",
    "            break\n",
    "        \n",
    "        # Join if it wasn't corrupted..    \n",
    "        reduced_train = reduced_train.join(components)\n",
    "        ###########################################\n",
    "        # (TEST) Nested indexes iteration\n",
    "        # Initial values:\n",
    "        n_rows = len(test)\n",
    "        row_step = int(2e5)\n",
    "        row_end = n_rows + row_step\n",
    "        components = pd.DataFrame()\n",
    "        low_idx = 0\n",
    "        ###########\n",
    "        for idx in np.arange(0,row_end,row_step):\n",
    "            if idx > n_rows:\n",
    "                idx = n_rows\n",
    "            up_idx = idx\n",
    "\n",
    "            if up_idx > low_idx:\n",
    "                sample = counts_test[low_idx:up_idx,low_col:up_col].toarray()\n",
    "                sample = reduce.transform(sample)\n",
    "                components = components.append(pd.DataFrame(sample))\n",
    "                low_idx = idx\n",
    "        components.reset_index(drop=True,inplace=True)\n",
    "        components.columns = ['col_{}-{}_{}'.format(low_col,up_col,i) for i in range(0,2)]\n",
    "        reduced_test = reduced_test.join(components)\n",
    "        print(reduced_train.shape,reduced_test.shape)\n",
    "        #####################################\n",
    "        # Prepare for next column range\n",
    "        low_col = col     \n",
    "        #####################################    \n",
    "        # Interval decompositions\n",
    "        if up_col%(col_step*10) == 0:\n",
    "            print('Interval decomposition...')\n",
    "            print(reduced_train.shape,reduced_test.shape)\n",
    "            reduce = cross_decomposition.PLSRegression(n_components=2)\n",
    "            reduce.fit(reduced_train,train.deal_probability)\n",
    "            reduced_train = pd.DataFrame(\n",
    "                reduce.transform(reduced_train),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "            reduced_test = pd.DataFrame(\n",
    "                reduce.transform(reduced_test),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "            print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "                cv=4,estimator=linear_model.LinearRegression(),\n",
    "                X=reduced_train,y=train.deal_probability))\n",
    "        #####################################    \n",
    "        # Save progress every n steps\n",
    "        if up_col%(col_step*5) == 0:\n",
    "            print('Interval save to disk...')\n",
    "            joblib.dump(reduced_train,'train_{}_{}.sav'.format(varname,compname))\n",
    "            joblib.dump(reduced_test,'test_{}_{}.sav'.format(varname,compname))   \n",
    "#####################################\n",
    "# Final round of decomposition\n",
    "print('Final decomposition...')\n",
    "reduce = cross_decomposition.PLSRegression(n_components=2)\n",
    "reduce.fit(reduced_train,train.deal_probability)\n",
    "reduced_train = pd.DataFrame(\n",
    "    reduce.transform(reduced_train),\n",
    "    columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "reduced_test = pd.DataFrame(\n",
    "    reduce.transform(reduced_test),\n",
    "    columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,2)])\n",
    "print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "    cv=4,estimator=linear_model.LinearRegression(),\n",
    "    X=reduced_train,y=train.deal_probability))\n",
    "#########\n",
    "# Last save to disk\n",
    "joblib.dump(reduced_train,'train_{}_{}.sav'.format(varname,compname))\n",
    "joblib.dump(reduced_test,'test_{}_{}.sav'.format(varname,compname))\n",
    "#########\n",
    "print('Minutes:',(time.time()-t)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> RESTART THE KERNEL\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
