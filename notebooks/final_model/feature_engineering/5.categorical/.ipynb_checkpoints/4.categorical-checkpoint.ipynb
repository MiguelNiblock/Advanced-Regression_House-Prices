{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary PLSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn import preprocessing, model_selection, metrics, feature_selection, ensemble, linear_model, cross_decomposition, feature_extraction, decomposition\n",
    "import time\n",
    "from sklearn.externals import joblib\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../../train.csv.zip',compression='zip')\n",
    "test = pd.read_csv('../../../test.csv.zip',compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['param_1','param_2','param_3','region','city','parent_category_name','category_name']\n",
    "compnames = ['p1plsr','p2plsr','p3plsr','regplsr','cityplsr','ptcatplsr','catnamplsr'] # custom name for component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param_1 p1plsr\n",
      "N tokens: 387\n",
      "Columns: 0-387\n",
      "Prelim score for column range: 0.1585976928629911\n",
      "Aggregate cv: [0.15679294 0.15318207 0.15617345 0.15660435]\n",
      "(1503424, 3) (508438, 3)\n",
      "Final decomposition...\n",
      "Aggregate cv after decomposition: [0.15679294 0.15318207 0.15617345 0.15660435]\n",
      "Minutes: 0.2599694490432739\n",
      "param_2 p2plsr\n",
      "N tokens: 280\n",
      "Columns: 0-280\n",
      "Prelim score for column range: 0.1236005744489529\n",
      "Aggregate cv: [0.12185657 0.11980314 0.1214753  0.12158728]\n",
      "(1503424, 3) (508438, 3)\n",
      "Final decomposition...\n",
      "Aggregate cv after decomposition: [0.12185657 0.11980314 0.1214753  0.12158728]\n",
      "Minutes: 0.2080145796140035\n",
      "param_3 p3plsr\n",
      "N tokens: 1269\n",
      "Columns: 0-1269\n",
      "Prelim score for column range: 0.06571525287454738\n",
      "Aggregate cv: [0.06351728 0.06121546 0.06336003 0.06263238]\n",
      "(1503424, 3) (508438, 3)\n",
      "Final decomposition...\n",
      "Aggregate cv after decomposition: [0.06351728 0.06121546 0.06336003 0.06263238]\n",
      "Minutes: 0.7164393464724222\n",
      "region regplsr\n",
      "N tokens: 29\n",
      "Columns: 0-29\n",
      "Prelim score for column range: 0.0012931749566543749\n",
      "Aggregate cv: [0.00123863 0.00102418 0.00136165 0.00108523]\n",
      "(1503424, 3) (508438, 3)\n",
      "Final decomposition...\n",
      "Aggregate cv after decomposition: [0.00123863 0.00102418 0.00136165 0.00108523]\n",
      "Minutes: 0.06257619063059489\n",
      "city cityplsr\n",
      "N tokens: 1777\n",
      "Columns: 0-1500\n",
      "Prelim score for column range: 0.006660252262076005\n",
      "Aggregate cv: [0.00247915 0.00260717 0.00326424 0.00264624]\n",
      "(1503424, 3) (508438, 3)\n",
      "Columns: 1500-1777\n",
      "Prelim score for column range: 0.001223238680718497\n",
      "Aggregate cv: [0.00287066 0.002995   0.00369074 0.00302594]\n",
      "(1503424, 6) (508438, 6)\n",
      "Final decomposition...\n",
      "Aggregate cv after decomposition: [0.00289813 0.00300256 0.00369638 0.00304144]\n",
      "Minutes: 1.0420734882354736\n",
      "parent_category_name ptcatplsr\n",
      "N tokens: 9\n",
      "Columns: 0-9\n",
      "Prelim score for column range: 0.09259296328410305\n",
      "Aggregate cv: [0.09424302 0.09221791 0.09434854 0.09511905]\n",
      "(1503424, 3) (508438, 3)\n",
      "Final decomposition...\n",
      "Aggregate cv after decomposition: [0.09424302 0.09221791 0.09434854 0.09511905]\n",
      "Minutes: 0.05015377600987752\n",
      "category_name catnamplsr\n",
      "N tokens: 47\n",
      "Columns: 0-47\n",
      "Prelim score for column range: 0.12419080003688543\n",
      "Aggregate cv: [0.12291549 0.1203705  0.12411626 0.1235096 ]\n",
      "(1503424, 3) (508438, 3)\n",
      "Final decomposition...\n",
      "Aggregate cv after decomposition: [0.12291549 0.1203705  0.12411626 0.1235096 ]\n",
      "Minutes: 0.074923308690389\n"
     ]
    }
   ],
   "source": [
    "for var,compname in zip(variables,compnames):\n",
    "    print(var,compname)\n",
    "    def joiner(string):\n",
    "        pattern = \"[\\[].,/-# <>`~+!*?[\\]]\"\n",
    "        string = string.replace(' ','_').replace(',','_')\n",
    "        return re.sub(pattern, \"\", string)\n",
    "\n",
    "    vec = feature_extraction.text.CountVectorizer(\n",
    "        lowercase=True,\n",
    "        binary=True,\n",
    "        preprocessor=joiner)\n",
    "    # Fitting on train and test as merged lists\n",
    "    vec.fit(train[var].astype(str).tolist() + test[var].astype(str).tolist())\n",
    "    print('N tokens:',len(vec.get_feature_names())) \n",
    "    \n",
    "    counts_train = vec.transform(train[var].astype(str).tolist())\n",
    "    counts_test = vec.transform(test[var].astype(str).tolist())\n",
    "\n",
    "    reduced_train = pd.DataFrame(index=train.index)\n",
    "    reduced_test = pd.DataFrame(index=test.index) \n",
    " \n",
    "    # Reduce all CSR values in batches\n",
    "    t = time.time()\n",
    "    start_col = 0\n",
    "    varname = var[:5]\n",
    "    n_comp = 3\n",
    "    ##########################################\n",
    "    n_cols = counts_train.shape[1]\n",
    "    col_step = 1500\n",
    "    col_end = n_cols + col_step\n",
    "    ##########################################\n",
    "    # Start iteration with columns\n",
    "    low_col = start_col\n",
    "    corrupted = False\n",
    "    for col in np.arange(0,col_end,col_step):\n",
    "        # Limiting the edge case of the last values\n",
    "        if col > n_cols:\n",
    "            col = n_cols\n",
    "        up_col = col\n",
    "\n",
    "        if up_col > low_col:\n",
    "            ###########################################\n",
    "            # Train PLSR on a large sample of train vectors\n",
    "            print('Columns: {}-{}'.format(low_col,up_col))\n",
    "            index = np.random.choice(len(train),size=int(4e5))\n",
    "            sample = counts_train[index,low_col:up_col].toarray()\n",
    "            reduce = cross_decomposition.PLSRegression(n_components=n_comp)\n",
    "            reduce.fit(sample,train.iloc[index].deal_probability)\n",
    "            print('Prelim score for column range:',reduce.score(sample,train.iloc[index].deal_probability))\n",
    "            ##########################################\n",
    "            # (TRAIN) Nested indexes iteration\n",
    "            # Initial values:\n",
    "            n_rows = len(train)\n",
    "            row_step = int(2.5e5)\n",
    "            row_end = n_rows + row_step\n",
    "            components = pd.DataFrame()\n",
    "            low_idx = 0\n",
    "            ###########\n",
    "            for idx in np.arange(0,row_end,row_step):\n",
    "                # Limiting the edge case of the last values\n",
    "                if idx > n_rows:\n",
    "                    idx = n_rows\n",
    "                up_idx = idx\n",
    "\n",
    "                if up_idx > low_idx:\n",
    "                    sample = counts_train[low_idx:up_idx,low_col:up_col].toarray()\n",
    "                    sample = reduce.transform(sample)\n",
    "                    components = components.append(pd.DataFrame(sample))\n",
    "                    low_idx = idx\n",
    "            components.reset_index(drop=True,inplace=True)\n",
    "            components.columns = ['col_{}-{}_{}'.format(low_col,up_col,i) for i in range(0,n_comp)]\n",
    "\n",
    "            # Cross-validate and check for corruptions before joining\n",
    "            cv = model_selection.cross_val_score(\n",
    "                cv=4,estimator=linear_model.LinearRegression(),\n",
    "                X=reduced_train.join(components),y=train.deal_probability)\n",
    "\n",
    "            print('Aggregate cv:',cv)        \n",
    "\n",
    "            if sum([score < 0 for score in cv]) > 0:\n",
    "                print('Reached corruption.\\n Final decomposition without joining...')\n",
    "                print(reduced_train.shape,reduced_test.shape)\n",
    "                reduce = cross_decomposition.PLSRegression(n_components=n_comp)\n",
    "                reduce.fit(reduced_train,train.deal_probability)\n",
    "                reduced_train = pd.DataFrame(\n",
    "                    reduce.transform(reduced_train),\n",
    "                    columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,n_comp)])\n",
    "                reduced_test = pd.DataFrame(\n",
    "                    reduce.transform(reduced_test),\n",
    "                    columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,n_comp)])\n",
    "                print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "                    cv=4,estimator=linear_model.LinearRegression(),\n",
    "                    X=reduced_train,y=train.deal_probability))\n",
    "                print('Minutes:',(time.time()-t)/60)\n",
    "                break\n",
    "\n",
    "            # Join if it wasn't corrupted..    \n",
    "            reduced_train = reduced_train.join(components)\n",
    "            ###########################################\n",
    "            # (TEST) Nested indexes iteration\n",
    "            # Initial values:\n",
    "            n_rows = len(test)\n",
    "            row_step = int(2e5)\n",
    "            row_end = n_rows + row_step\n",
    "            components = pd.DataFrame()\n",
    "            low_idx = 0\n",
    "            ###########\n",
    "            for idx in np.arange(0,row_end,row_step):\n",
    "                if idx > n_rows:\n",
    "                    idx = n_rows\n",
    "                up_idx = idx\n",
    "\n",
    "                if up_idx > low_idx:\n",
    "                    sample = counts_test[low_idx:up_idx,low_col:up_col].toarray()\n",
    "                    sample = reduce.transform(sample)\n",
    "                    components = components.append(pd.DataFrame(sample))\n",
    "                    low_idx = idx\n",
    "            components.reset_index(drop=True,inplace=True)\n",
    "            components.columns = ['col_{}-{}_{}'.format(low_col,up_col,i) for i in range(0,n_comp)]\n",
    "            reduced_test = reduced_test.join(components)\n",
    "            print(reduced_train.shape,reduced_test.shape)\n",
    "            #####################################\n",
    "            # Prepare for next column range\n",
    "            low_col = col     \n",
    "            #####################################    \n",
    "            # Interval decompositions\n",
    "            if up_col%(col_step*10) == 0:\n",
    "                print('Interval decomposition...')\n",
    "                print(reduced_train.shape,reduced_test.shape)\n",
    "                reduce = cross_decomposition.PLSRegression(n_components=n_comp)\n",
    "                reduce.fit(reduced_train,train.deal_probability)\n",
    "                reduced_train = pd.DataFrame(\n",
    "                    reduce.transform(reduced_train),\n",
    "                    columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,n_comp)])\n",
    "                reduced_test = pd.DataFrame(\n",
    "                    reduce.transform(reduced_test),\n",
    "                    columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,n_comp)])\n",
    "                print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "                    cv=4,estimator=linear_model.LinearRegression(),\n",
    "                    X=reduced_train,y=train.deal_probability))\n",
    "            #####################################    \n",
    "            # Save progress every n steps\n",
    "            if up_col%(col_step*5) == 0:\n",
    "                print('Interval save to disk...')\n",
    "                joblib.dump(reduced_train,'train_{}_{}.sav'.format(varname,compname))\n",
    "                joblib.dump(reduced_test,'test_{}_{}.sav'.format(varname,compname))   \n",
    "    #####################################\n",
    "    # Final round of decomposition\n",
    "    print('Final decomposition...')\n",
    "    reduce = cross_decomposition.PLSRegression(n_components=n_comp)\n",
    "    reduce.fit(reduced_train,train.deal_probability)\n",
    "    reduced_train = pd.DataFrame(\n",
    "        reduce.transform(reduced_train),\n",
    "        columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,n_comp)])\n",
    "    reduced_test = pd.DataFrame(\n",
    "        reduce.transform(reduced_test),\n",
    "        columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,n_comp)])\n",
    "    print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "        cv=4,estimator=linear_model.LinearRegression(),\n",
    "        X=reduced_train,y=train.deal_probability))\n",
    "    #########\n",
    "    # Last save to disk\n",
    "    joblib.dump(reduced_train,'train_{}_{}.sav'.format(varname,compname))\n",
    "    joblib.dump(reduced_test,'test_{}_{}.sav'.format(varname,compname))\n",
    "    #########\n",
    "    print('Minutes:',(time.time()-t)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popularity Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn import preprocessing, model_selection, metrics, feature_selection, ensemble, linear_model, cross_decomposition, feature_extraction, decomposition\n",
    "import time\n",
    "from sklearn.externals import joblib\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../../train.csv.zip',compression='zip')\n",
    "test = pd.read_csv('../../../test.csv.zip',compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['param_1','param_2','param_3','region','city','parent_category_name','category_name']\n",
    "\n",
    "train_codes = pd.DataFrame()\n",
    "test_codes = pd.DataFrame()\n",
    "\n",
    "for var in variables:\n",
    "    # Target-sorted unique values of train&test, no empties, no weird types.\n",
    "    train_values = train[[var]].fillna('Blank').astype(str).join(train['deal_probability']).groupby(var).mean().sort_values('deal_probability',ascending=False).reset_index()\n",
    "    # If values are in test only, fill with median\n",
    "    med = train_values.median()[0]\n",
    "    train_unique = train_values[var]\n",
    "    test_unique = pd.Series(test[[var]].fillna('Blank').astype(str)[var].unique())\n",
    "    miss_idx = [i[0] for i in np.argwhere([unique not in train_unique.tolist() for unique in test_unique])]\n",
    "    for idx in miss_idx:\n",
    "        new = pd.DataFrame({var:test_unique[idx],'deal_probability':med},index=[len(train_values)])\n",
    "        train_values = train_values.append(new)\n",
    "    train_values = train_values.sort_values('deal_probability',ascending=False).reset_index(drop=True)\n",
    "    values = train_values[var].tolist()\n",
    "        \n",
    "    # Dict: Keys are sorted-uniques, dict-values are indices.\n",
    "    val_dict = {}\n",
    "    for i,val in enumerate(values):\n",
    "        val_dict[val] = i\n",
    "    \n",
    "    # Train codes ###########\n",
    "    codes = []\n",
    "    for val in train[var].fillna('Blank').astype(str):\n",
    "        codes.append(val_dict[val])\n",
    "    train_codes['code_'+str(var)] = codes\n",
    "    \n",
    "    # Test codes ############\n",
    "    codes = []\n",
    "    for val in test[var].fillna('Blank').astype(str):\n",
    "        codes.append(val_dict[val])\n",
    "    test_codes['code_'+str(var)] = codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_codes.sav']"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(train_codes,'train_codes.sav')\n",
    "joblib.dump(test_codes,'test_codes.sav')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
