{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary PLSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn import preprocessing, model_selection, metrics, feature_selection, ensemble, linear_model, cross_decomposition, feature_extraction, decomposition\n",
    "import time\n",
    "from sklearn.externals import joblib\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../../train.csv.zip',compression='zip')\n",
    "test = pd.read_csv('../../../test.csv.zip',compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['param_1','param_2','param_3','region','city','category_name']\n",
    "compnames = ['p1plsr','p2plsr','p3plsr','regplsr','cityplsr','catnamplsr'] # custom name for component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param_1 p1plsr\n",
      "N tokens: 387\n",
      "Columns: 0-387\n",
      "Prelim score for column range: 0.15655498110674948\n",
      "Aggregate cv: [0.15676329 0.15322856 0.15637188 0.15657073]\n",
      "(1503424, 10) (508438, 10)\n",
      "param_2 p2plsr\n",
      "N tokens: 280\n",
      "Columns: 0-280\n",
      "Prelim score for column range: 0.12558359907063832\n",
      "Aggregate cv: [0.16264688 0.15924979 0.16186965 0.16246216]\n",
      "(1503424, 20) (508438, 20)\n",
      "param_3 p3plsr\n",
      "N tokens: 1269\n",
      "Columns: 0-1000\n",
      "Prelim score for column range: 0.057518731333971\n",
      "Aggregate cv: [0.16334192 0.15999341 0.16264773 0.16316857]\n",
      "(1503424, 30) (508438, 30)\n",
      "Columns: 1000-1269\n",
      "Prelim score for column range: 0.0318208386201323\n",
      "Aggregate cv: [0.16356205 0.16024803 0.16291684 0.16344107]\n",
      "(1503424, 40) (508438, 40)\n",
      "region regplsr\n",
      "N tokens: 29\n",
      "Columns: 0-29\n",
      "Prelim score for column range: 0.0013247104814475552\n",
      "Aggregate cv: [0.16397374 0.16063732 0.16330568 0.16378928]\n",
      "(1503424, 50) (508438, 50)\n",
      "city cityplsr\n",
      "N tokens: 1777\n",
      "Columns: 0-1000\n",
      "Prelim score for column range: 0.004507537758539626\n",
      "Aggregate cv: [0.16433621 0.16109228 0.16390557 0.16429945]\n",
      "(1503424, 60) (508438, 60)\n",
      "Columns: 1000-1777\n",
      "Prelim score for column range: 0.003040355064637179\n",
      "Aggregate cv: [0.16459636 0.16145947 0.16427205 0.16453333]\n",
      "(1503424, 70) (508438, 70)\n",
      "category_name catnamplsr\n",
      "N tokens: 47\n",
      "Columns: 0-47\n",
      "Prelim score for column range: 0.12200873017978597\n",
      "Aggregate cv: [0.16557944 0.16237641 0.16524544 0.1654224 ]\n",
      "(1503424, 80) (508438, 80)\n",
      "Minutes: 6.435929834842682 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "reduced_train = pd.DataFrame(index=train.index)\n",
    "reduced_test = pd.DataFrame(index=test.index) \n",
    "\n",
    "for var,compname in zip(variables,compnames):\n",
    "    print(var,compname)\n",
    "    def joiner(string):\n",
    "        pattern = \"[\\[].,/-# <>`~+!*?[\\]]\"\n",
    "        string = string.replace(' ','_').replace(',','_')\n",
    "        return re.sub(pattern, \"_\", string)\n",
    "\n",
    "    vec = feature_extraction.text.CountVectorizer(\n",
    "        lowercase=True,\n",
    "        binary=True,\n",
    "        preprocessor=joiner)\n",
    "    # Fitting on train and test as merged lists\n",
    "    vec.fit(train[var].astype(str).tolist() + test[var].astype(str).tolist())\n",
    "    print('N tokens:',len(vec.get_feature_names())) \n",
    "    \n",
    "    counts_train = vec.transform(train[var].astype(str).tolist())\n",
    "    counts_test = vec.transform(test[var].astype(str).tolist())\n",
    "\n",
    "    # Reduce all CSR values in batches\n",
    "    start_col = 0\n",
    "    varname = var[:5]\n",
    "    n_comp = 10\n",
    "    ##########################################\n",
    "    n_cols = counts_train.shape[1]\n",
    "    col_step = 1000\n",
    "    col_end = n_cols + col_step\n",
    "    ##########################################\n",
    "    # Start iteration with columns\n",
    "    low_col = start_col\n",
    "    for col in np.arange(0,col_end,col_step):\n",
    "        # Limiting the edge case of the last values\n",
    "        if col > n_cols:\n",
    "            col = n_cols\n",
    "        up_col = col\n",
    "\n",
    "        if up_col > low_col:\n",
    "            ###########################################\n",
    "            # Train PLSR on a large sample of train vectors\n",
    "            print('Columns: {}-{}'.format(low_col,up_col))\n",
    "            index = np.random.choice(len(train),size=int(4e5))\n",
    "            sample = counts_train[index,low_col:up_col].toarray()\n",
    "            reduce = cross_decomposition.PLSRegression(n_components=n_comp)\n",
    "            reduce.fit(sample,train.iloc[index].deal_probability)\n",
    "            print('Prelim score for column range:',reduce.score(sample,train.iloc[index].deal_probability))\n",
    "            ##########################################\n",
    "            # (TRAIN) Nested indexes iteration\n",
    "            # Initial values:\n",
    "            n_rows = len(train)\n",
    "            row_step = int(2.5e5)\n",
    "            row_end = n_rows + row_step\n",
    "            components = pd.DataFrame()\n",
    "            low_idx = 0\n",
    "            ###########\n",
    "            for idx in np.arange(0,row_end,row_step):\n",
    "                # Limiting the edge case of the last values\n",
    "                if idx > n_rows:\n",
    "                    idx = n_rows\n",
    "                up_idx = idx\n",
    "\n",
    "                if up_idx > low_idx:\n",
    "                    sample = counts_train[low_idx:up_idx,low_col:up_col].toarray()\n",
    "                    sample = reduce.transform(sample)\n",
    "                    components = components.append(pd.DataFrame(sample))\n",
    "                    low_idx = idx\n",
    "            components.reset_index(drop=True,inplace=True)\n",
    "            components.columns = ['{}_col_{}-{}_{}'.format(compname,low_col,up_col,i) for i in range(0,n_comp)]\n",
    "\n",
    "            # Cross-validate and check for corruptions before joining\n",
    "            cv = model_selection.cross_val_score(\n",
    "                cv=4,estimator=linear_model.LinearRegression(),\n",
    "                X=reduced_train.join(components),y=train.deal_probability)\n",
    "            print('Aggregate cv:',cv) \n",
    "            \n",
    "            reduced_train = reduced_train.join(components)\n",
    "            ###########################################\n",
    "            # (TEST) Nested indexes iteration\n",
    "            # Initial values:\n",
    "            n_rows = len(test)\n",
    "            row_step = int(2e5)\n",
    "            row_end = n_rows + row_step\n",
    "            components = pd.DataFrame()\n",
    "            low_idx = 0\n",
    "            ###########\n",
    "            for idx in np.arange(0,row_end,row_step):\n",
    "                if idx > n_rows:\n",
    "                    idx = n_rows\n",
    "                up_idx = idx\n",
    "\n",
    "                if up_idx > low_idx:\n",
    "                    sample = counts_test[low_idx:up_idx,low_col:up_col].toarray()\n",
    "                    sample = reduce.transform(sample)\n",
    "                    components = components.append(pd.DataFrame(sample))\n",
    "                    low_idx = idx\n",
    "            components.reset_index(drop=True,inplace=True)\n",
    "            components.columns = ['{}_col_{}-{}_{}'.format(compname,low_col,up_col,i) for i in range(0,n_comp)]\n",
    "            reduced_test = reduced_test.join(components)\n",
    "            print(reduced_train.shape,reduced_test.shape)\n",
    "            #####################################\n",
    "            # Prepare for next column range\n",
    "            low_col = col     \n",
    "            #####################################    \n",
    "# Last save to disk\n",
    "joblib.dump(reduced_train,'train_categorical.sav')\n",
    "joblib.dump(reduced_test,'test_categorical.sav')\n",
    "#########\n",
    "print('Minutes:',(time.time()-t)/60,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scores:\n",
      " [0.23745429 0.23745653 0.2377352  0.23809592 0.23817744]\n",
      "Mean CV score:\n",
      " 0.23778387528549189\n",
      "Less than zero: 1121\n",
      "Over one: 0\n",
      "RMSE without modification: 0.23746276229960936\n",
      "RMSE fit-to-range: 0.23745955927295329\n",
      "TEST preds:\n",
      "Less than zero: 4574\n",
      "Over one: 0\n"
     ]
    }
   ],
   "source": [
    "y = train.deal_probability\n",
    "X_dev,X_val,y_dev,y_val = model_selection.train_test_split(reduced_train,y)\n",
    "\n",
    "def rmse(y,pred):\n",
    "    return metrics.mean_squared_error(y,pred)**0.5\n",
    "\n",
    "score = metrics.make_scorer(rmse)\n",
    "linear = linear_model.LinearRegression()\n",
    "\n",
    "cv = model_selection.cross_val_score(\n",
    "    X=X_dev,y=y_dev,estimator=linear,\n",
    "    cv=5,scoring=score\n",
    ")\n",
    "print('CV Scores:\\n',cv)\n",
    "print('Mean CV score:\\n',cv.mean())\n",
    "\n",
    "linear = linear.fit(X_dev,y_dev)\n",
    "\n",
    "pred = linear.predict(X_val)\n",
    "print('Less than zero:',sum(pred<0))\n",
    "print('Over one:',sum(pred>1))\n",
    "print('RMSE without modification:',metrics.mean_squared_error(y_val,pred)**0.5)\n",
    "pred[pred>1] = 1\n",
    "pred[pred<0] = 0\n",
    "print('RMSE fit-to-range:',metrics.mean_squared_error(y_val,pred)**0.5)\n",
    "\n",
    "print('TEST preds:')\n",
    "pred = linear.predict(reduced_train)\n",
    "print('Less than zero:',sum(pred<0))\n",
    "print('Over one:',sum(pred>1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popularity Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn import preprocessing, model_selection, metrics, feature_selection, ensemble, linear_model, cross_decomposition, feature_extraction, decomposition\n",
    "import time\n",
    "from sklearn.externals import joblib\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../../train.csv.zip',compression='zip')\n",
    "test = pd.read_csv('../../../test.csv.zip',compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['param_1','param_2','param_3','region','city','parent_category_name','category_name']\n",
    "\n",
    "train_codes = pd.DataFrame()\n",
    "test_codes = pd.DataFrame()\n",
    "\n",
    "for var in variables:\n",
    "    # Target-sorted unique values of train&test, no empties, no weird types.\n",
    "    train_values = train[[var]].fillna('Blank').astype(str).join(train['deal_probability']).groupby(var).mean().sort_values('deal_probability',ascending=False).reset_index()\n",
    "    # If values are in test only, fill with median\n",
    "    med = train_values.median()[0]\n",
    "    train_unique = train_values[var]\n",
    "    test_unique = pd.Series(test[[var]].fillna('Blank').astype(str)[var].unique())\n",
    "    miss_idx = [i[0] for i in np.argwhere([unique not in train_unique.tolist() for unique in test_unique])]\n",
    "    for idx in miss_idx:\n",
    "        new = pd.DataFrame({var:test_unique[idx],'deal_probability':med},index=[len(train_values)])\n",
    "        train_values = train_values.append(new)\n",
    "    train_values = train_values.sort_values('deal_probability',ascending=False).reset_index(drop=True)\n",
    "    values = train_values[var].tolist()\n",
    "        \n",
    "    # Dict: Keys are sorted-uniques, dict-values are indices.\n",
    "    val_dict = {}\n",
    "    for i,val in enumerate(values):\n",
    "        val_dict[val] = i\n",
    "    \n",
    "    # Train codes ###########\n",
    "    codes = []\n",
    "    for val in train[var].fillna('Blank').astype(str):\n",
    "        codes.append(val_dict[val])\n",
    "    train_codes['code_'+str(var)] = codes\n",
    "    \n",
    "    # Test codes ############\n",
    "    codes = []\n",
    "    for val in test[var].fillna('Blank').astype(str):\n",
    "        codes.append(val_dict[val])\n",
    "    test_codes['code_'+str(var)] = codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_codes.sav']"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(train_codes,'train_codes.sav')\n",
    "joblib.dump(test_codes,'test_codes.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_f = joblib.load('feature_dumps1/train_codes.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_f = joblib.load('feature_dumps1/test_codes.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scores:\n",
      " [0.23893316 0.23920064 0.23952821 0.23933496 0.23907038]\n",
      "Mean CV score:\n",
      " 0.23921346870628413\n",
      "Less than zero: 672\n",
      "Over one: 0\n",
      "RMSE without modification: 0.2386217698214643\n",
      "RMSE fit-to-range: 0.23862049461858315\n",
      "TEST preds:\n",
      "Less than zero: 673\n",
      "Over one: 0\n"
     ]
    }
   ],
   "source": [
    "y = train.deal_probability\n",
    "X_dev,X_val,y_dev,y_val = model_selection.train_test_split(train_f,y)\n",
    "\n",
    "def rmse(y,pred):\n",
    "    return metrics.mean_squared_error(y,pred)**0.5\n",
    "\n",
    "score = metrics.make_scorer(rmse)\n",
    "linear = linear_model.LinearRegression()\n",
    "\n",
    "cv = model_selection.cross_val_score(\n",
    "    X=X_dev,y=y_dev,estimator=linear,\n",
    "    cv=5,scoring=score\n",
    ")\n",
    "print('CV Scores:\\n',cv)\n",
    "print('Mean CV score:\\n',cv.mean())\n",
    "\n",
    "linear = linear.fit(X_dev,y_dev)\n",
    "\n",
    "pred = linear.predict(X_val)\n",
    "print('Less than zero:',sum(pred<0))\n",
    "print('Over one:',sum(pred>1))\n",
    "print('RMSE without modification:',metrics.mean_squared_error(y_val,pred)**0.5)\n",
    "pred[pred>1] = 1\n",
    "pred[pred<0] = 0\n",
    "print('RMSE fit-to-range:',metrics.mean_squared_error(y_val,pred)**0.5)\n",
    "\n",
    "print('TEST preds:')\n",
    "pred = linear.predict(test_f)\n",
    "print('Less than zero:',sum(pred<0))\n",
    "print('Over one:',sum(pred>1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
