{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Decomposition of TF-IDF Vectors with BiGrams\n",
    "\n",
    "- Apply to train and test data.\n",
    "- Retain separate sets of components for titles and descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn import preprocessing, model_selection, metrics, feature_selection, ensemble, linear_model, cross_decomposition, feature_extraction, decomposition\n",
    "import time\n",
    "from sklearn.externals import joblib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../../train.pkl',compression='zip')\n",
    "test = pd.read_pickle('../../test.pkl',compression='zip')\n",
    "# Russian stopwords\n",
    "ru_stop = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'title'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N tokens: 67138\n"
     ]
    }
   ],
   "source": [
    "vec = feature_extraction.text.TfidfVectorizer(\n",
    "    stop_words=ru_stop,\n",
    "    lowercase=False,\n",
    "    ngram_range=(1,2),\n",
    "    min_df=0.000005)\n",
    "# Fitting on train and test as merged lists\n",
    "vec.fit(train[var].astype(str).tolist() + test[var].astype(str).tolist())\n",
    "print('N tokens:',len(vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### vectors for train and test\n",
    "counts_train = vec.transform(train[var].astype(str).tolist())\n",
    "counts_test = vec.transform(test[var].astype(str).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### To start from zero...\n",
    "reduced_train = pd.DataFrame(index=train.index)\n",
    "reduced_test = pd.DataFrame(index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: 0-1500\n",
      "Prelim score for column range: 0.015922152095400155\n",
      "Aggregate cv: [0.00961826 0.00964545 0.01001544 0.01021694]\n",
      "Columns: 1500-3000\n",
      "Prelim score for column range: 0.017240286526295057\n",
      "Aggregate cv: [0.01927139 0.01884747 0.01922021 0.01977584]\n",
      "Columns: 3000-4500\n",
      "Prelim score for column range: 0.012670055620140896\n",
      "Aggregate cv: [0.02343053 0.0230405  0.02309524 0.02424821]\n",
      "Columns: 4500-6000\n",
      "Prelim score for column range: 0.01141446209742647\n",
      "Aggregate cv: [0.02615848 0.02588908 0.0255398  0.0272025 ]\n",
      "Columns: 6000-7500\n",
      "Prelim score for column range: 0.025564026021270836\n",
      "Aggregate cv: [0.04117682 0.03930076 0.04083533 0.04204329]\n",
      "Columns: 7500-9000\n",
      "Prelim score for column range: 0.023159940202099882\n",
      "Aggregate cv: [0.05146448 0.04927356 0.05082396 0.05175504]\n",
      "Columns: 9000-10500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-42805105eb5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounts_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlow_col\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mup_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mreduce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_decomposition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPLSRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mreduce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeal_probability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Prelim score for column range:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeal_probability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m##########################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/cross_decomposition/pls_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mx_loadings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_scores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0;31m# - subtract rank-one approximations to obtain remainder matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0mXk\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_loadings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeflation_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"canonical\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0;31m# - regress Yk's on y_score, then subtract rank-one approx.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Reduce all CSR values in batches\n",
    "t = time.time()\n",
    "start_col = 0\n",
    "varname = var[:5]\n",
    "compname = 'idfngram'\n",
    "##########################################\n",
    "n_cols = counts_train.shape[1]\n",
    "col_step = 1500\n",
    "col_end = n_cols + col_step\n",
    "##########################################\n",
    "# Start iteration with columns\n",
    "low_col = start_col\n",
    "for col in np.arange(0,col_end,col_step):\n",
    "    # Limiting the edge case of the last values\n",
    "    if col > n_cols:\n",
    "        col = n_cols\n",
    "    up_col = col\n",
    "    \n",
    "    if up_col > low_col:\n",
    "        ###########################################\n",
    "        # Train PLSR on a large sample of train vectors\n",
    "        print('Columns: {}-{}'.format(low_col,up_col))\n",
    "        index = np.random.choice(len(train),size=int(4e5))\n",
    "        sample = counts_train[index,low_col:up_col].toarray()\n",
    "        reduce = cross_decomposition.PLSRegression(n_components=5)\n",
    "        reduce.fit(sample,train.iloc[index].deal_probability)\n",
    "        print('Prelim score for column range:',reduce.score(sample,train.iloc[index].deal_probability))\n",
    "        ##########################################\n",
    "        # (TRAIN) Nested indexes iteration\n",
    "        # Initial values:\n",
    "        n_rows = len(train)\n",
    "        row_step = int(2.5e5)\n",
    "        row_end = n_rows + row_step\n",
    "        components = pd.DataFrame()\n",
    "        low_idx = 0\n",
    "        ###########\n",
    "        for idx in np.arange(0,row_end,row_step):\n",
    "            # Limiting the edge case of the last values\n",
    "            if idx > n_rows:\n",
    "                idx = n_rows\n",
    "            up_idx = idx\n",
    "\n",
    "            if up_idx > low_idx:\n",
    "                sample = counts_train[low_idx:up_idx,low_col:up_col].toarray()\n",
    "                sample = reduce.transform(sample)\n",
    "                components = components.append(pd.DataFrame(sample))\n",
    "                low_idx = idx\n",
    "        components.reset_index(drop=True,inplace=True)\n",
    "        components.columns = ['col_{}-{}_{}'.format(low_col,up_col,i) for i in range(0,5)]\n",
    "        reduced_train = reduced_train.join(components)\n",
    "        print('Aggregate cv:',model_selection.cross_val_score(\n",
    "            cv=4,estimator=linear_model.LinearRegression(),\n",
    "            X=reduced_train,y=train.deal_probability))\n",
    "        ###########################################\n",
    "        # (TEST) Nested indexes iteration\n",
    "        # Initial values:\n",
    "        n_rows = len(test)\n",
    "        row_step = int(2e5)\n",
    "        row_end = n_rows + row_step\n",
    "        components = pd.DataFrame()\n",
    "        low_idx = 0\n",
    "        ###########\n",
    "        for idx in np.arange(0,row_end,row_step):\n",
    "            if idx > n_rows:\n",
    "                idx = n_rows\n",
    "            up_idx = idx\n",
    "\n",
    "            if up_idx > low_idx:\n",
    "                sample = counts_test[low_idx:up_idx,low_col:up_col].toarray()\n",
    "                sample = reduce.transform(sample)\n",
    "                components = components.append(pd.DataFrame(sample))\n",
    "                low_idx = idx\n",
    "        components.reset_index(drop=True,inplace=True)\n",
    "        components.columns = ['col_{}-{}_{}'.format(low_col,up_col,i) for i in range(0,5)]\n",
    "        reduced_test = reduced_test.join(components)\n",
    "        #####################################\n",
    "        # Prepare for next column range\n",
    "        low_col = col     \n",
    "        #####################################    \n",
    "        # Decompose aggregate every n steps\n",
    "        if up_col%(col_step*10) == 0:\n",
    "            print('Decomposing Aggregate...')\n",
    "            reduce = cross_decomposition.PLSRegression(n_components=5)\n",
    "            reduce.fit(reduced_train,train.deal_probability)\n",
    "            reduced_train = pd.DataFrame(\n",
    "                reduce.transform(reduced_train),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,5)])\n",
    "            reduced_test = pd.DataFrame(\n",
    "                reduce.transform(reduced_test),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,5)])\n",
    "            print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "                cv=4,estimator=linear_model.LinearRegression(),\n",
    "                X=reduced_train,y=train.deal_probability))\n",
    "        #####################################    \n",
    "        # Save progress every n steps\n",
    "        if up_col%(col_step*5) == 0:\n",
    "            joblib.dump(reduced_train,'train_{}_{}.sav'.format(varname,compname))\n",
    "            joblib.dump(reduced_test,'test_{}_{}.sav'.format(varname,compname))   \n",
    "#####################################\n",
    "# One final round of decomposition to n components\n",
    "if reduced_train.shape[1] > 5:\n",
    "    print('Decomposing Aggregate...')\n",
    "    reduce = cross_decomposition.PLSRegression(n_components=5)\n",
    "    reduce.fit(reduced_train,train.deal_probability)\n",
    "    reduced_train = pd.DataFrame(\n",
    "        reduce.transform(reduced_train),\n",
    "        columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,5)])\n",
    "    reduced_test = pd.DataFrame(\n",
    "        reduce.transform(reduced_test),\n",
    "        columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,5)])\n",
    "    print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "        cv=4,estimator=linear_model.LinearRegression(),\n",
    "        X=reduced_train,y=train.deal_probability))\n",
    "#########\n",
    "# Last save to disk\n",
    "joblib.dump(reduced_train,'train_{}_{}.sav'.format(varname,compname))\n",
    "joblib.dump(reduced_test,'test_{}_{}.sav'.format(varname,compname))\n",
    "#########\n",
    "print('Minutes:',(time.time()-t)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    ">**YOU MAY RESTART THE KERNEL AT THIS POINT**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn import preprocessing, model_selection, metrics, feature_selection, ensemble, linear_model, cross_decomposition, feature_extraction, decomposition\n",
    "import time\n",
    "from sklearn.externals import joblib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../../train.pkl',compression='zip')\n",
    "test = pd.read_pickle('../../test.pkl',compression='zip')\n",
    "# Russian stopwords\n",
    "ru_stop = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'description'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N tokens: 67138\n"
     ]
    }
   ],
   "source": [
    "vec = feature_extraction.text.TfidfVectorizer(\n",
    "    stop_words=ru_stop,\n",
    "    lowercase=False,\n",
    "    ngram_range=(1,2),\n",
    "    min_df=0.00005)\n",
    "# Fitting on train and test as merged lists\n",
    "vec.fit(train[var].astype(str).tolist() + test[var].astype(str).tolist())\n",
    "print('N tokens:',len(vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### vectors for train and test\n",
    "counts_train = vec.transform(train[var].astype(str).tolist())\n",
    "counts_test = vec.transform(test[var].astype(str).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### To start from zero...\n",
    "reduced_train = pd.DataFrame(index=train.index)\n",
    "reduced_test = pd.DataFrame(index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: 0-1500\n",
      "Prelim score for column range: 0.015922152095400155\n",
      "Aggregate cv: [0.00961826 0.00964545 0.01001544 0.01021694]\n",
      "Columns: 1500-3000\n",
      "Prelim score for column range: 0.017240286526295057\n",
      "Aggregate cv: [0.01927139 0.01884747 0.01922021 0.01977584]\n",
      "Columns: 3000-4500\n",
      "Prelim score for column range: 0.012670055620140896\n",
      "Aggregate cv: [0.02343053 0.0230405  0.02309524 0.02424821]\n",
      "Columns: 4500-6000\n",
      "Prelim score for column range: 0.01141446209742647\n",
      "Aggregate cv: [0.02615848 0.02588908 0.0255398  0.0272025 ]\n",
      "Columns: 6000-7500\n",
      "Prelim score for column range: 0.025564026021270836\n",
      "Aggregate cv: [0.04117682 0.03930076 0.04083533 0.04204329]\n",
      "Columns: 7500-9000\n",
      "Prelim score for column range: 0.023159940202099882\n",
      "Aggregate cv: [0.05146448 0.04927356 0.05082396 0.05175504]\n",
      "Columns: 9000-10500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-42805105eb5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounts_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlow_col\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mup_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mreduce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_decomposition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPLSRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mreduce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeal_probability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Prelim score for column range:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeal_probability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m##########################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/cross_decomposition/pls_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mx_loadings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_scores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0;31m# - subtract rank-one approximations to obtain remainder matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0mXk\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_loadings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeflation_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"canonical\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0;31m# - regress Yk's on y_score, then subtract rank-one approx.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Reduce all CSR values in batches\n",
    "t = time.time()\n",
    "start_col = 0\n",
    "varname = var[:5]\n",
    "compname = 'idfngram'\n",
    "##########################################\n",
    "n_cols = counts_train.shape[1]\n",
    "col_step = 1500\n",
    "col_end = n_cols + col_step\n",
    "##########################################\n",
    "# Start iteration with columns\n",
    "low_col = start_col\n",
    "for col in np.arange(0,col_end,col_step):\n",
    "    # Limiting the edge case of the last values\n",
    "    if col > n_cols:\n",
    "        col = n_cols\n",
    "    up_col = col\n",
    "    \n",
    "    if up_col > low_col:\n",
    "        ###########################################\n",
    "        # Train PLSR on a large sample of train vectors\n",
    "        print('Columns: {}-{}'.format(low_col,up_col))\n",
    "        index = np.random.choice(len(train),size=int(4e5))\n",
    "        sample = counts_train[index,low_col:up_col].toarray()\n",
    "        reduce = cross_decomposition.PLSRegression(n_components=5)\n",
    "        reduce.fit(sample,train.iloc[index].deal_probability)\n",
    "        print('Prelim score for column range:',reduce.score(sample,train.iloc[index].deal_probability))\n",
    "        ##########################################\n",
    "        # (TRAIN) Nested indexes iteration\n",
    "        # Initial values:\n",
    "        n_rows = len(train)\n",
    "        row_step = int(2.5e5)\n",
    "        row_end = n_rows + row_step\n",
    "        components = pd.DataFrame()\n",
    "        low_idx = 0\n",
    "        ###########\n",
    "        for idx in np.arange(0,row_end,row_step):\n",
    "            # Limiting the edge case of the last values\n",
    "            if idx > n_rows:\n",
    "                idx = n_rows\n",
    "            up_idx = idx\n",
    "\n",
    "            if up_idx > low_idx:\n",
    "                sample = counts_train[low_idx:up_idx,low_col:up_col].toarray()\n",
    "                sample = reduce.transform(sample)\n",
    "                components = components.append(pd.DataFrame(sample))\n",
    "                low_idx = idx\n",
    "        components.reset_index(drop=True,inplace=True)\n",
    "        components.columns = ['col_{}-{}_{}'.format(low_col,up_col,i) for i in range(0,5)]\n",
    "        reduced_train = reduced_train.join(components)\n",
    "        print('Aggregate cv:',model_selection.cross_val_score(\n",
    "            cv=4,estimator=linear_model.LinearRegression(),\n",
    "            X=reduced_train,y=train.deal_probability))\n",
    "        ###########################################\n",
    "        # (TEST) Nested indexes iteration\n",
    "        # Initial values:\n",
    "        n_rows = len(test)\n",
    "        row_step = int(2e5)\n",
    "        row_end = n_rows + row_step\n",
    "        components = pd.DataFrame()\n",
    "        low_idx = 0\n",
    "        ###########\n",
    "        for idx in np.arange(0,row_end,row_step):\n",
    "            if idx > n_rows:\n",
    "                idx = n_rows\n",
    "            up_idx = idx\n",
    "\n",
    "            if up_idx > low_idx:\n",
    "                sample = counts_test[low_idx:up_idx,low_col:up_col].toarray()\n",
    "                sample = reduce.transform(sample)\n",
    "                components = components.append(pd.DataFrame(sample))\n",
    "                low_idx = idx\n",
    "        components.reset_index(drop=True,inplace=True)\n",
    "        components.columns = ['col_{}-{}_{}'.format(low_col,up_col,i) for i in range(0,5)]\n",
    "        reduced_test = reduced_test.join(components)\n",
    "        #####################################\n",
    "        # Prepare for next column range\n",
    "        low_col = col     \n",
    "        #####################################    \n",
    "        # Decompose aggregate every n steps\n",
    "        if up_col%(col_step*10) == 0:\n",
    "            print('Decomposing Aggregate...')\n",
    "            reduce = cross_decomposition.PLSRegression(n_components=5)\n",
    "            reduce.fit(reduced_train,train.deal_probability)\n",
    "            reduced_train = pd.DataFrame(\n",
    "                reduce.transform(reduced_train),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,5)])\n",
    "            reduced_test = pd.DataFrame(\n",
    "                reduce.transform(reduced_test),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,5)])\n",
    "            print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "                cv=4,estimator=linear_model.LinearRegression(),\n",
    "                X=reduced_train,y=train.deal_probability))\n",
    "        #####################################    \n",
    "        # Save progress every n steps\n",
    "        if up_col%(col_step*5) == 0:\n",
    "            joblib.dump(reduced_train,'train_{}_{}.sav'.format(varname,compname))\n",
    "            joblib.dump(reduced_test,'test_{}_{}.sav'.format(varname,compname))   \n",
    "#####################################\n",
    "# One final round of decomposition to n components\n",
    "if reduced_train.shape[1] > 5:\n",
    "    print('Decomposing Aggregate...')\n",
    "    reduce = cross_decomposition.PLSRegression(n_components=5)\n",
    "    reduce.fit(reduced_train,train.deal_probability)\n",
    "    reduced_train = pd.DataFrame(\n",
    "        reduce.transform(reduced_train),\n",
    "        columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,5)])\n",
    "    reduced_test = pd.DataFrame(\n",
    "        reduce.transform(reduced_test),\n",
    "        columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,5)])\n",
    "    print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "        cv=4,estimator=linear_model.LinearRegression(),\n",
    "        X=reduced_train,y=train.deal_probability))\n",
    "#########\n",
    "# Last save to disk\n",
    "joblib.dump(reduced_train,'train_{}_{}.sav'.format(varname,compname))\n",
    "joblib.dump(reduced_test,'test_{}_{}.sav'.format(varname,compname))\n",
    "#########\n",
    "print('Minutes:',(time.time()-t)/60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
