{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Vocabularies for Target Ranges\n",
    "\n",
    "- Include Count and IDF vocabularies.\n",
    "- Get vocabs for titles and descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title Vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn import preprocessing, model_selection, metrics, feature_selection, ensemble, linear_model, cross_decomposition, feature_extraction, decomposition\n",
    "import time\n",
    "from sklearn.externals import joblib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../../train.pkl',compression='zip')\n",
    "test = pd.read_pickle('../../test.pkl',compression='zip')\n",
    "# Russian stopwords\n",
    "ru_stop = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define discrete target boundaries\n",
    "i_0 = train[train.deal_probability==0].index.tolist()\n",
    "i_low = train[(train.deal_probability>0)&(train.deal_probability<0.65)].index.tolist()\n",
    "i_up = train[train.deal_probability>=0.65].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make count and idf vocabularies for target ranges on a given variable\n",
    "var = 'title'\n",
    "#####################################\n",
    "upper_str = ' '.join(train.loc[i_up,var].astype(str).values)\n",
    "lower_str = ' '.join(train.loc[i_low,var].astype(str).values)\n",
    "zeroes_str = ' '.join(train.loc[i_0,var].astype(str).values)\n",
    "\n",
    "# Get dictionaries from both count and idf vectorizers.\n",
    "count = feature_extraction.text.CountVectorizer(\n",
    "    stop_words=ru_stop,\n",
    "    lowercase=False)\n",
    "idf = feature_extraction.text.TfidfVectorizer(\n",
    "    stop_words=ru_stop,\n",
    "    lowercase=False)\n",
    "\n",
    "vecs ={'count':count,'idf':idf}\n",
    "#####################\n",
    "for key in vecs:\n",
    "    vec = vecs[key]\n",
    "    vec.fit([zeroes_str,lower_str,upper_str])\n",
    "    counts = vec.transform([zeroes_str,lower_str,upper_str])\n",
    "\n",
    "    # Convert CSR into DataFrame and Transpose. Now terms are on the index\n",
    "    counts = pd.DataFrame(counts.toarray()).T\n",
    "\n",
    "    # Extract terms from vocabulary, sort by index and add to df index\n",
    "    vocab = vec.vocabulary_\n",
    "    terms = [f for f in vocab]\n",
    "    terms = pd.DataFrame(terms)\n",
    "    terms['index'] = [vocab[k] for k in vocab]\n",
    "    terms.sort_values(by='index',inplace=True)\n",
    "    terms = terms[0].values.tolist()\n",
    "    counts.index = terms\n",
    "    \n",
    "    # Make an indicator of where the highest frequency is for each term\n",
    "    group = []\n",
    "    for i in np.arange(len(counts)):\n",
    "        group.append(np.argmax(counts.iloc[i].values))      \n",
    "    counts['group'] = group\n",
    "    \n",
    "    zero_vocab = counts[counts.group == 0].sort_values(by=0,ascending=False).index.tolist()\n",
    "    lower_vocab = counts[counts.group == 1].sort_values(by=1,ascending=False).index.tolist()\n",
    "    upper_vocab = counts[counts.group == 2].sort_values(by=2,ascending=False).index.tolist()\n",
    "\n",
    "    vocabs = [zero_vocab,lower_vocab,upper_vocab]\n",
    "    vocabs = pd.DataFrame(vocabs)\n",
    "    vocabs = vocabs.T\n",
    "    vocabs.columns=['zero_voc','low_voc','up_voc']\n",
    "    \n",
    "    vocabs.to_pickle('{}_vocabs_{}.pkl'.format(var[:5],key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> RESTART KERNEL\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description Vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn import preprocessing, model_selection, metrics, feature_selection, ensemble, linear_model, cross_decomposition, feature_extraction, decomposition\n",
    "import time\n",
    "from sklearn.externals import joblib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../../train.pkl',compression='zip')\n",
    "test = pd.read_pickle('../../test.pkl',compression='zip')\n",
    "# Russian stopwords\n",
    "ru_stop = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define discrete target boundaries\n",
    "i_0 = train[train.deal_probability==0].index.tolist()\n",
    "i_low = train[(train.deal_probability>0)&(train.deal_probability<0.65)].index.tolist()\n",
    "i_up = train[train.deal_probability>=0.65].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make count and idf vocabularies for target ranges on a given variable\n",
    "var = 'description'\n",
    "#####################################\n",
    "upper_str = ' '.join(train.loc[i_up,var].astype(str).values)\n",
    "lower_str = ' '.join(train.loc[i_low,var].astype(str).values)\n",
    "zeroes_str = ' '.join(train.loc[i_0,var].astype(str).values)\n",
    "\n",
    "# Get dictionaries from both count and idf vectorizers.\n",
    "count = feature_extraction.text.CountVectorizer(\n",
    "    stop_words=ru_stop,\n",
    "    lowercase=False)\n",
    "idf = feature_extraction.text.TfidfVectorizer(\n",
    "    stop_words=ru_stop,\n",
    "    lowercase=False)\n",
    "\n",
    "vecs ={'count':count,'idf':idf}\n",
    "#####################\n",
    "for key in vecs:\n",
    "    vec = vecs[key]\n",
    "    vec.fit([zeroes_str,lower_str,upper_str])\n",
    "    counts = vec.transform([zeroes_str,lower_str,upper_str])\n",
    "\n",
    "    # Convert CSR into DataFrame and Transpose. Now terms are on the index\n",
    "    counts = pd.DataFrame(counts.toarray()).T\n",
    "\n",
    "    # Extract terms from vocabulary, sort by index and add to df index\n",
    "    vocab = vec.vocabulary_\n",
    "    terms = [f for f in vocab]\n",
    "    terms = pd.DataFrame(terms)\n",
    "    terms['index'] = [vocab[k] for k in vocab]\n",
    "    terms.sort_values(by='index',inplace=True)\n",
    "    terms = terms[0].values.tolist()\n",
    "    counts.index = terms\n",
    "    \n",
    "    # Make an indicator of where the highest frequency is for each term\n",
    "    group = []\n",
    "    for i in np.arange(len(counts)):\n",
    "        group.append(np.argmax(counts.iloc[i].values))      \n",
    "    counts['group'] = group\n",
    "    \n",
    "    zero_vocab = counts[counts.group == 0].sort_values(by=0,ascending=False).index.tolist()\n",
    "    lower_vocab = counts[counts.group == 1].sort_values(by=1,ascending=False).index.tolist()\n",
    "    upper_vocab = counts[counts.group == 2].sort_values(by=2,ascending=False).index.tolist()\n",
    "\n",
    "    vocabs = [zero_vocab,lower_vocab,upper_vocab]\n",
    "    vocabs = pd.DataFrame(vocabs)\n",
    "    vocabs = vocabs.T\n",
    "    vocabs.columns=['zero_voc','low_voc','up_voc']\n",
    "    \n",
    "    vocabs.to_pickle('{}_vocabs_{}.pkl'.format(var[:5],key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> RESTART THE KERNEL\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Decomposition per Vocab\n",
    "\n",
    "I can only decompose one vocabulary at a time, for memory limitation reasons. So every time define:\n",
    "- the variable\n",
    "- the vocabulary kind\n",
    "- the target range it represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title var- Count voc- Zero Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn import preprocessing, model_selection, metrics, feature_selection, ensemble, linear_model, cross_decomposition, feature_extraction, decomposition\n",
    "import time\n",
    "from sklearn.externals import joblib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../../train.pkl',compression='zip')\n",
    "test = pd.read_pickle('../../test.pkl',compression='zip')\n",
    "# Russian stopwords\n",
    "ru_stop = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define discrete target boundaries\n",
    "i_0 = train[train.deal_probability==0].index.tolist()\n",
    "i_low = train[(train.deal_probability>0)&(train.deal_probability<0.65)].index.tolist()\n",
    "i_up = train[train.deal_probability>=0.65].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variable, index-range, discrete-range, voc-origin, and component-name\n",
    "var = 'title' # title or description\n",
    "irange = i_0 # pick from above cell\n",
    "rnge = 'zero' # zero, low or up\n",
    "voc_kind = 'count' # count or idf\n",
    "compname = 'zero_cnt_voc' # custom name for component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the desired vocabulary onto a list of limited length\n",
    "voc = pd.read_pickle('{}_vocabs_{}.pkl'.format(var[:5],voc_kind))['{}_voc'.format(rnge)].dropna()[:67000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N tokens: 67000\n"
     ]
    }
   ],
   "source": [
    "vec = feature_extraction.text.TfidfVectorizer(\n",
    "    stop_words=ru_stop,\n",
    "    lowercase=False,\n",
    "    vocabulary=voc)\n",
    "# Fitting on train and test as merged lists\n",
    "vec.fit(train[var].astype(str).tolist() + test[var].astype(str).tolist())\n",
    "print('N tokens:',len(vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### vectors for train and test\n",
    "counts_train = vec.transform(train[var].astype(str).tolist())\n",
    "counts_test = vec.transform(test[var].astype(str).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### To start from zero...\n",
    "reduced_train = pd.DataFrame(index=train.index)\n",
    "reduced_test = pd.DataFrame(index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce all CSR values in batches\n",
    "t = time.time()\n",
    "start_col = 0\n",
    "varname = var[:5]\n",
    "##########################################\n",
    "n_cols = counts_train.shape[1]\n",
    "col_step = 1500\n",
    "col_end = n_cols + col_step\n",
    "##########################################\n",
    "# Start iteration with columns\n",
    "low_col = start_col\n",
    "for col in np.arange(0,col_end,col_step):\n",
    "    # Limiting the edge case of the last values\n",
    "    if col > n_cols:\n",
    "        col = n_cols\n",
    "    up_col = col\n",
    "    \n",
    "    if up_col > low_col:\n",
    "        ###########################################\n",
    "        # Train PLSR on a large sample of train vectors\n",
    "        print('Columns: {}-{}'.format(low_col,up_col))\n",
    "        index = np.random.choice(len(train),size=int(4e5))\n",
    "        sample = counts_train[index,low_col:up_col].toarray()\n",
    "        reduce = cross_decomposition.PLSRegression(n_components=5)\n",
    "        reduce.fit(sample,train.iloc[index].deal_probability)\n",
    "        print('Prelim score for column range:',reduce.score(sample,train.iloc[index].deal_probability))\n",
    "        ##########################################\n",
    "        # (TRAIN) Nested indexes iteration\n",
    "        # Initial values:\n",
    "        n_rows = len(train)\n",
    "        row_step = int(2.5e5)\n",
    "        row_end = n_rows + row_step\n",
    "        components = pd.DataFrame()\n",
    "        low_idx = 0\n",
    "        ###########\n",
    "        for idx in np.arange(0,row_end,row_step):\n",
    "            # Limiting the edge case of the last values\n",
    "            if idx > n_rows:\n",
    "                idx = n_rows\n",
    "            up_idx = idx\n",
    "\n",
    "            if up_idx > low_idx:\n",
    "                sample = counts_train[low_idx:up_idx,low_col:up_col].toarray()\n",
    "                sample = reduce.transform(sample)\n",
    "                components = components.append(pd.DataFrame(sample))\n",
    "                low_idx = idx\n",
    "        components.reset_index(drop=True,inplace=True)\n",
    "        components.columns = ['col_{}-{}_{}'.format(low_col,up_col,i) for i in range(0,5)]\n",
    "        reduced_train = reduced_train.join(components)\n",
    "        print('Aggregate cv:',model_selection.cross_val_score(\n",
    "            cv=4,estimator=linear_model.LinearRegression(),\n",
    "            X=reduced_train,y=train.deal_probability))\n",
    "        ###########################################\n",
    "        # (TEST) Nested indexes iteration\n",
    "        # Initial values:\n",
    "        n_rows = len(test)\n",
    "        row_step = int(2e5)\n",
    "        row_end = n_rows + row_step\n",
    "        components = pd.DataFrame()\n",
    "        low_idx = 0\n",
    "        ###########\n",
    "        for idx in np.arange(0,row_end,row_step):\n",
    "            if idx > n_rows:\n",
    "                idx = n_rows\n",
    "            up_idx = idx\n",
    "\n",
    "            if up_idx > low_idx:\n",
    "                sample = counts_test[low_idx:up_idx,low_col:up_col].toarray()\n",
    "                sample = reduce.transform(sample)\n",
    "                components = components.append(pd.DataFrame(sample))\n",
    "                low_idx = idx\n",
    "        components.reset_index(drop=True,inplace=True)\n",
    "        components.columns = ['col_{}-{}_{}'.format(low_col,up_col,i) for i in range(0,5)]\n",
    "        reduced_test = reduced_test.join(components)\n",
    "        #####################################\n",
    "        # Prepare for next column range\n",
    "        low_col = col     \n",
    "        #####################################    \n",
    "        # Decompose aggregate every n steps\n",
    "        if up_col%(col_step*10) == 0:\n",
    "            print('Decomposing Aggregate...')\n",
    "            reduce = cross_decomposition.PLSRegression(n_components=5)\n",
    "            reduce.fit(reduced_train,train.deal_probability)\n",
    "            reduced_train = pd.DataFrame(\n",
    "                reduce.transform(reduced_train),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,5)])\n",
    "            reduced_test = pd.DataFrame(\n",
    "                reduce.transform(reduced_test),\n",
    "                columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,5)])\n",
    "            print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "                cv=4,estimator=linear_model.LinearRegression(),\n",
    "                X=reduced_train,y=train.deal_probability))\n",
    "            print('Aggregate score for target range:',model_selection.cross_val_score(\n",
    "                cv=4,estimator=linear_model.LinearRegression(),\n",
    "                X=reduced.iloc[trange],y=train.iloc[trange].deal_probability))\n",
    "        #####################################    \n",
    "        # Save progress every n steps\n",
    "        if up_col%(col_step*5) == 0:\n",
    "            joblib.dump(reduced_train,'train_{}_{}.sav'.format(varname,compname))\n",
    "            joblib.dump(reduced_test,'test_{}_{}.sav'.format(varname,compname))   \n",
    "#####################################\n",
    "# One final round of decomposition to n components\n",
    "if reduced_train.shape[1] > 5:\n",
    "    print('Decomposing Aggregate...')\n",
    "    reduce = cross_decomposition.PLSRegression(n_components=5)\n",
    "    reduce.fit(reduced_train,train.deal_probability)\n",
    "    reduced_train = pd.DataFrame(\n",
    "        reduce.transform(reduced_train),\n",
    "        columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,5)])\n",
    "    reduced_test = pd.DataFrame(\n",
    "        reduce.transform(reduced_test),\n",
    "        columns=['{}_{}_{}_{}'.format(varname,compname,i,up_col) for i in range(0,5)])\n",
    "    print('Aggregate cv after decomposition:',model_selection.cross_val_score(\n",
    "        cv=4,estimator=linear_model.LinearRegression(),\n",
    "        X=reduced_train,y=train.deal_probability))\n",
    "#########\n",
    "# Last save to disk\n",
    "joblib.dump(reduced_train,'train_{}_{}.sav'.format(varname,compname))\n",
    "joblib.dump(reduced_test,'test_{}_{}.sav'.format(varname,compname))\n",
    "#########\n",
    "print('Minutes:',(time.time()-t)/60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
