{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "[1. Loading Data](#1.-Loading-Data)\n",
    "\n",
    "[2. Exploratory Data Analysis](#2.-Exploratory-Data-Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading Data\n",
    "\n",
    "- Get data into notebook in the best form possible for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import cyrtranslit\n",
    "from sklearn import preprocessing, model_selection, metrics, feature_selection, ensemble, linear_model, cross_decomposition, feature_extraction, decomposition, compose\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "import time\n",
    "from sklearn.externals import joblib\n",
    "import os\n",
    "color = sns.color_palette()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../train.csv.zip',compression='zip')\n",
    "test = pd.read_csv('../test.csv.zip',compression='zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis\n",
    "\n",
    "- What’s happening with data.\n",
    "    - It's big.\n",
    "    - It's in Russian with Cyrillic alphabet.\n",
    "\n",
    "- Why it’s interesting\n",
    "    - Outcome variable is very non-normal. There's three distinct groups. (Zero Range, Lower Range, Upper Range)\n",
    "    \n",
    "- What features you intend to take advantage of for your modeling.\n",
    "    - Titles and descriptions offer endless NLP opportunities.\n",
    "    - Plenty of categorical data to binarize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pipeline\n",
    "\n",
    "- Linked stages.\n",
    "- Efficient and easy to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and describe engineered features\n",
    "train_features = pd.DataFrame(index=train.index)\n",
    "test_features = pd.DataFrame(index=test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Cross-Decomposition of TF-IDF Vectors with BiGrams\n",
    "\n",
    "This process consists of extracting term frequency vectors using the text in each ad as a document. Tokens for unigrams and bigrams will be included in this stage. Lastly, the resulting matrix will be reduced to the smallest number of components that retain all potential predictive power. Perform onto both titles and descriptions and retain separate components for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and joining feature sets:...\n",
      "train_descr_idfngram.sav\n",
      "train_title_idfngram.sav\n",
      "test_descr_idfngram.sav\n",
      "test_title_idfngram.sav\n"
     ]
    }
   ],
   "source": [
    "path = 'feature_engineering/1.tfidf_ngrams/feature_dumps/'\n",
    "print('Loading and joining feature sets:...')\n",
    "for file in os.listdir(path):\n",
    "    print(file)\n",
    "    if file[:4] == 'test':\n",
    "        test_features = test_features.join(joblib.load(path+file))\n",
    "    else:\n",
    "        train_features = train_features.join(joblib.load(path+file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Discretized Vector Cross-Decomposition\n",
    "\n",
    "This consists of splitting the dependent variable into discrete ranges and creating a vocabulary for each range. Then vectorize and cross-decompose each vocabulary independently. Resulting components for each vocabulary will reflect the presence of terms common in a certain discrete range of target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and joining feature sets:...\n",
      "test_title_zeroidf.sav\n",
      "test_title_lowcnt.sav\n",
      "train_title_zeroidf.sav\n",
      "train_title_lowcnt.sav\n",
      "test_title_lowidf.sav\n",
      "test_title_zerocnt.sav\n",
      "train_title_zerocnt.sav\n",
      "train_title_upidf.sav\n",
      "train_title_upcnt.sav\n",
      "train_title_lowidf.sav\n",
      "test_title_upcnt.sav\n",
      "test_title_upidf.sav\n"
     ]
    }
   ],
   "source": [
    "path = 'feature_engineering/2.discrete-decomp/feature_dumps/'\n",
    "print('Loading and joining feature sets:...')\n",
    "for file in os.listdir(path):\n",
    "    print(file)\n",
    "    if file[:4] == 'test':\n",
    "        test_features = test_features.join(joblib.load(path+file))\n",
    "    else:\n",
    "        train_features = train_features.join(joblib.load(path+file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Discretized Vector Sums\n",
    "\n",
    "Similar to previous procedure, vocabularies are created for discrete ranges of target. However instead of decomposing the vectors of those vocabularies, you simply sum their frequencies along the row axis of the term frequency matrix. This results in a single variable for each vocabulary, which represents the aggregate frequency of a vocabulary's terms per ad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and joining feature sets:...\n",
      "test_sums.pkl\n",
      "train_sums.pkl\n"
     ]
    }
   ],
   "source": [
    "path = 'feature_engineering/3.vector-sums/feature_dumps/'\n",
    "print('Loading and joining feature sets:...')\n",
    "for file in os.listdir(path):\n",
    "    print(file)\n",
    "    if file[:4] == 'test':\n",
    "        test_features = test_features.join(pd.read_pickle(path+file,compression='zip'))\n",
    "    else:\n",
    "        train_features = train_features.join(pd.read_pickle(path+file,compression='zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Sentiment Analysis\n",
    "\n",
    "An NLP library called `polyglot` offers multi-language tools, such as Sentiment-Analysis and Named-Entity-Recognition in Russian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and joining feature sets:...\n",
      "test_title_polarity.sav\n",
      "train_title_polarity.sav\n"
     ]
    }
   ],
   "source": [
    "path = 'feature_engineering/4.sentiment/feature_dumps/'\n",
    "print('Loading and joining feature sets:...')\n",
    "for file in os.listdir(path):\n",
    "    print(file)\n",
    "    if file[:4] == 'test':\n",
    "        test_features = test_features.join(joblib.load(path+file))\n",
    "    else:\n",
    "        train_features = train_features.join(joblib.load(path+file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Categorical Features\n",
    "\n",
    "### 3.5.1 Binary CountVectorizer\n",
    "\n",
    "Several categorical variables in this data have thousands of unique values which would increase the dimensional space unreasonably if binarizing in dense format. A binary `CountVectorizer` does the heavy lifting of populating dummy counts in sparse format, and `PLSR` reduces the numerous columns to a few core components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2. Target-Sorted Label Encodings\n",
    "Additionally, a label encoder of each feature is made with particular considerations. Normally, label encoding isn't recommended for machine learning because the algorithm will interpret the code numbers as meaningful information. However, encodings can convey useful information if categorical values are sorted by their mean outcome value. This way, each label's code will represent an approximation of the target outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and joining feature sets:...\n",
      "train_codes.sav\n",
      "test_categ_catnamplsr.sav\n",
      "train_regio_regplsr.sav\n",
      "test_city_cityplsr.sav\n",
      "train_param_p1plsr.sav\n",
      "test_param_p3plsr.sav\n",
      "test_param_p2plsr.sav\n",
      "train_categ_catnamplsr.sav\n",
      "test_regio_regplsr.sav\n",
      "train_param_p3plsr.sav\n",
      "test_codes.sav\n",
      "train_paren_ptcatplsr.sav\n",
      "train_param_p2plsr.sav\n",
      "test_param_p1plsr.sav\n",
      "train_city_cityplsr.sav\n",
      "test_paren_ptcatplsr.sav\n"
     ]
    }
   ],
   "source": [
    "path = 'feature_engineering/5.categorical/feature_dumps/'\n",
    "print('Loading and joining feature sets:...')\n",
    "for file in os.listdir(path):\n",
    "    print(file)\n",
    "    if file[:4] == 'test':\n",
    "        test_features = test_features.join(joblib.load(path+file))\n",
    "    else:\n",
    "        train_features = train_features.join(joblib.load(path+file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Other Features\n",
    "\n",
    "- Imputations\n",
    "- Missing Indicators\n",
    "- Day-of-Week dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and joining feature sets:...\n",
      "train_othfeat.sav\n",
      "test_othfeat.sav\n"
     ]
    }
   ],
   "source": [
    "path = 'feature_engineering/6.other/feature_dumps/'\n",
    "print('Loading and joining feature sets:...')\n",
    "for file in os.listdir(path):\n",
    "    print(file)\n",
    "    if file[:4] == 'test':\n",
    "        test_features = test_features.join(joblib.load(path+file))\n",
    "    else:\n",
    "        train_features = train_features.join(joblib.load(path+file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation\n",
    "\n",
    "- Evaluation and comparison of multiple models via robust analysis of residuals and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Product\n",
    "\n",
    "- Why chose it.\n",
    "- Why works.\n",
    "- What problem it solves.\n",
    "- How will it run in a production environment.\n",
    "- What to do to maintain it going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
