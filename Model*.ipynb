{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import cyrtranslit\n",
    "from sklearn import preprocessing, model_selection, metrics, feature_selection, ensemble, linear_model\n",
    "from sklearn.pipeline import Pipeline\n",
    "import lightgbm as lgb\n",
    "color = sns.color_palette()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"https://onedrive.live.com/download?cid=62B3CEE436FDB342&resid=62B3CEE436FDB342%21107&authkey=AEh-8Y6p9SC7FK0\",\n",
    "                      compression='zip', parse_dates=[\"activation_date\"])\n",
    "test = pd.read_csv(\"https://onedrive.live.com/download?cid=62B3CEE436FDB342&resid=62B3CEE436FDB342%21106&authkey=AAF_zwBmWjNhNGQ\",\n",
    "                      compression='zip', parse_dates=[\"activation_date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Since the test data has no labels, we'll validate with two subsets of train. Lastly we'll use test to generate a submission file and get a public score.\n",
    "\n",
    "## Basic Feature Engineering\n",
    "\n",
    "- Translate all textual features into latin.\n",
    "- Create dummies from categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1503424 entries, 0 to 1503423\n",
      "Data columns (total 18 columns):\n",
      "item_id                 1503424 non-null object\n",
      "user_id                 1503424 non-null object\n",
      "region                  1503424 non-null object\n",
      "city                    1503424 non-null object\n",
      "parent_category_name    1503424 non-null object\n",
      "category_name           1503424 non-null object\n",
      "param_1                 1441848 non-null object\n",
      "param_2                 848882 non-null object\n",
      "param_3                 640859 non-null object\n",
      "title                   1503424 non-null object\n",
      "description             1387148 non-null object\n",
      "price                   1418062 non-null float64\n",
      "item_seq_number         1503424 non-null int64\n",
      "activation_date         1503424 non-null datetime64[ns]\n",
      "user_type               1503424 non-null object\n",
      "image                   1390836 non-null object\n",
      "image_top_1             1390836 non-null float64\n",
      "deal_probability        1503424 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(3), int64(1), object(13)\n",
      "memory usage: 206.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Title and description have too many unique values, therefore we won't translate them all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique regions: 28\n",
      "Number of unique cities: 1733\n",
      "Number of unique parent categories: 9\n",
      "Number of unique categories: 47\n",
      "Number of unique descriptions: 1317102\n",
      "Number of unique titles: 788377\n",
      "Number of unique param_1: 371\n",
      "Number of unique param_2: 271\n",
      "Number of unique param_3 1219\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique regions:',len(train.region.value_counts()))\n",
    "print('Number of unique cities:',len(train.city.value_counts()))\n",
    "print('Number of unique parent categories:',len(train.parent_category_name.value_counts()))\n",
    "print('Number of unique categories:',len(train.category_name.value_counts()))\n",
    "print('Number of unique descriptions:',len(train.description.value_counts()))\n",
    "print('Number of unique titles:',len(train.title.value_counts()))\n",
    "print('Number of unique param_1:',len(train.param_1.value_counts()))\n",
    "print('Number of unique param_2:',len(train.param_2.value_counts()))\n",
    "print('Number of unique param_3',len(train.param_3.value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description and title have too many unique values,\n",
    "# Therefore this method would take too long.\n",
    "cyr_vars = ['region','city','parent_category_name','category_name',\n",
    "           'param_1','param_2','param_3']\n",
    "\n",
    "for var in cyr_vars:\n",
    "    for dataset in [train,test]:\n",
    "        # Get unique cyrilic vlaues\n",
    "        cyrilic_unique = np.unique(dataset[var].fillna('Blank')).tolist()\n",
    "        # Get unique latin translations\n",
    "        latin_unique = [cyrtranslit.to_latin(string,'ru') for string in cyrilic_unique]\n",
    "\n",
    "        # Put lists in a dictionary\n",
    "        trans_dict = {}\n",
    "        for cyr, lat in zip(cyrilic_unique,latin_unique):\n",
    "            trans_dict[cyr]=lat\n",
    "\n",
    "        # Create a translated list\n",
    "        en_list = []\n",
    "        for key in dataset[var].fillna('Blank'):\n",
    "            en_list.append(trans_dict[key])\n",
    "\n",
    "        # Add english list as column\n",
    "        dataset[str(var)+'_en'] = en_list\n",
    "        dataset.drop(var,axis=1,inplace=True)\n",
    "\n",
    "del cyrilic_unique,latin_unique,trans_dict,en_list,dataset,cyr_vars,var,cyr,lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region_en</th>\n",
       "      <th>city_en</th>\n",
       "      <th>parent_category_name_en</th>\n",
       "      <th>category_name_en</th>\n",
       "      <th>param_1_en</th>\n",
       "      <th>param_2_en</th>\n",
       "      <th>param_3_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sverdlovskaja oblast'</td>\n",
       "      <td>Ekaterinburg</td>\n",
       "      <td>Lichnye veszi</td>\n",
       "      <td>Tovary dlja detej i igrushki</td>\n",
       "      <td>Postel'nye prinadlezhnosti</td>\n",
       "      <td>Blank</td>\n",
       "      <td>Blank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Samarskaja oblast'</td>\n",
       "      <td>Samara</td>\n",
       "      <td>Dlja doma i dachi</td>\n",
       "      <td>Mebel' i inter'er</td>\n",
       "      <td>Drugoe</td>\n",
       "      <td>Blank</td>\n",
       "      <td>Blank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rostovskaja oblast'</td>\n",
       "      <td>Rostov-na-Donu</td>\n",
       "      <td>Bytovaja ehlektronika</td>\n",
       "      <td>Audio i video</td>\n",
       "      <td>Video, DVD i Blu-ray pleery</td>\n",
       "      <td>Blank</td>\n",
       "      <td>Blank</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               region_en         city_en parent_category_name_en  \\\n",
       "0  Sverdlovskaja oblast'    Ekaterinburg           Lichnye veszi   \n",
       "1     Samarskaja oblast'          Samara       Dlja doma i dachi   \n",
       "2    Rostovskaja oblast'  Rostov-na-Donu   Bytovaja ehlektronika   \n",
       "\n",
       "               category_name_en                   param_1_en param_2_en  \\\n",
       "0  Tovary dlja detej i igrushki   Postel'nye prinadlezhnosti      Blank   \n",
       "1             Mebel' i inter'er                       Drugoe      Blank   \n",
       "2                 Audio i video  Video, DVD i Blu-ray pleery      Blank   \n",
       "\n",
       "  param_3_en  \n",
       "0      Blank  \n",
       "1      Blank  \n",
       "2      Blank  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region_en</th>\n",
       "      <th>city_en</th>\n",
       "      <th>parent_category_name_en</th>\n",
       "      <th>category_name_en</th>\n",
       "      <th>param_1_en</th>\n",
       "      <th>param_2_en</th>\n",
       "      <th>param_3_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Volgogradskaja oblast'</td>\n",
       "      <td>Volgograd</td>\n",
       "      <td>Lichnye veszi</td>\n",
       "      <td>Detskaja odezhda i obuv'</td>\n",
       "      <td>Dlja mal'chikov</td>\n",
       "      <td>Obuv'</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sverdlovskaja oblast'</td>\n",
       "      <td>Nizhnjaja Tura</td>\n",
       "      <td>Hobbi i otdyh</td>\n",
       "      <td>Velosipedy</td>\n",
       "      <td>Dorozhnye</td>\n",
       "      <td>Blank</td>\n",
       "      <td>Blank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Novosibirskaja oblast'</td>\n",
       "      <td>Berdsk</td>\n",
       "      <td>Bytovaja ehlektronika</td>\n",
       "      <td>Audio i video</td>\n",
       "      <td>Televizory i proektory</td>\n",
       "      <td>Blank</td>\n",
       "      <td>Blank</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                region_en         city_en parent_category_name_en  \\\n",
       "0  Volgogradskaja oblast'       Volgograd           Lichnye veszi   \n",
       "1   Sverdlovskaja oblast'  Nizhnjaja Tura           Hobbi i otdyh   \n",
       "2  Novosibirskaja oblast'          Berdsk   Bytovaja ehlektronika   \n",
       "\n",
       "           category_name_en              param_1_en param_2_en param_3_en  \n",
       "0  Detskaja odezhda i obuv'         Dlja mal'chikov      Obuv'         25  \n",
       "1                Velosipedy               Dorozhnye      Blank      Blank  \n",
       "2             Audio i video  Televizory i proektory      Blank      Blank  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# See latin translations\n",
    "print('Train Data:')\n",
    "display(train.iloc[:3,-7:])\n",
    "print(\"\")\n",
    "print('Test Data:')\n",
    "display(test.iloc[:3,-7:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummies\n",
    "### City Features\n",
    "- The number of unique cities in train is too large (1700) and leads to `MemoryError`. Let's pick a subset of those cities. The most useful might be those which appear in both train and test. Cities appearing only in train won't help predict anything in test that regards the city variable, and listings from cities which appear only in test can't be predicted with train information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique comon cities: 1625\n",
      "N Listings in both sets from unique common cities: 2011447\n"
     ]
    }
   ],
   "source": [
    "# Find which cities are in both train and test sets\n",
    "test_unique = test.city_en.unique()\n",
    "train_unique = train.city_en.unique()\n",
    "\n",
    "common = [city for city in train_unique if city in test_unique]\n",
    "del test_unique,train_unique\n",
    "\n",
    "print('Unique comon cities:',len(common))\n",
    "\n",
    "# Create features from the most popular cities in common set\n",
    "\n",
    "# Get common cities in train and test\n",
    "train_common = train[train.city_en.apply(lambda x: x in common)].city_en\n",
    "test_common = test[test.city_en.apply(lambda x: x in common)].city_en\n",
    "\n",
    "# Merge sets of common cities\n",
    "traintest_common = train_common.append(test_common)\n",
    "print('N Listings in both sets from unique common cities:',traintest_common.shape[0])\n",
    "\n",
    "# Count values among merged set\n",
    "top_common = traintest_common.value_counts()[:150].keys().tolist()\n",
    "del common,train_common,test_common,traintest_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we'll store our machine learning features\n",
    "train_features = pd.DataFrame(index=train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features from top_common\n",
    "for city in top_common:\n",
    "    train_features[str(city)] = np.where(train.city_en == city,1,0)\n",
    "del top_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection of Cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features above 0.01 Variance: 29\n"
     ]
    }
   ],
   "source": [
    "selector = feature_selection.VarianceThreshold(0.01)\n",
    "n_features = selector.fit_transform(train_features).shape[1]\n",
    "print('Number of Features above 0.01 Variance:', n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Variance Threshold doesn't take into account the target variable. At 0.01 variance, it removes all but 29 of cities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Feature Selection Techniques\n",
    "- Feature Importances, Lasso L1, F Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "estimator1 = ensemble.ExtraTreesRegressor()\n",
    "selector1 = feature_selection.SelectFromModel(\n",
    "    estimator1,\n",
    "    threshold=-np.inf,\n",
    "    max_features=n_features\n",
    ")\n",
    "\n",
    "estimator2 = linear_model.Lasso()\n",
    "selector2 = feature_selection.SelectFromModel(\n",
    "    estimator2,\n",
    "    threshold=-np.inf,\n",
    "    max_features=n_features\n",
    ")\n",
    "\n",
    "estimator3 = feature_selection.f_regression\n",
    "selector3 = feature_selection.SelectKBest(\n",
    "    estimator3,\n",
    "    k=n_features\n",
    ")\n",
    "\n",
    "model = linear_model.LinearRegression()\n",
    "\n",
    "pipe = Pipeline([('reduce_dim', None),('regression', model)])\n",
    "\n",
    "grid = {'reduce_dim': [selector1,selector2,selector3]}\n",
    "\n",
    "search = model_selection.GridSearchCV(pipe, cv=4, n_jobs=-1, param_grid=grid)\n",
    "\n",
    "index = np.random.choice(len(train),size=int(len(train)/7))\n",
    "search.fit(train_features.iloc[index],train.iloc[index].deal_probability)\n",
    "\n",
    "print(search.best_estimator_.named_steps['reduce_dim'].estimator)\n",
    "del index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lasso returned the best features, based on the LinearRegression default score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ekaterinburg</th>\n",
       "      <th>Krasnodar</th>\n",
       "      <th>Novosibirsk</th>\n",
       "      <th>Nizhnij Novgorod</th>\n",
       "      <th>Rostov-na-Donu</th>\n",
       "      <th>CHeljabinsk</th>\n",
       "      <th>Kazan'</th>\n",
       "      <th>Perm'</th>\n",
       "      <th>Samara</th>\n",
       "      <th>Ufa</th>\n",
       "      <th>...</th>\n",
       "      <th>Irkutsk</th>\n",
       "      <th>Orenburg</th>\n",
       "      <th>Izhevsk</th>\n",
       "      <th>Sochi</th>\n",
       "      <th>Tol'jatti</th>\n",
       "      <th>Kemerovo</th>\n",
       "      <th>Belgorod</th>\n",
       "      <th>Tula</th>\n",
       "      <th>Naberezhnye CHelny</th>\n",
       "      <th>Stavropol'</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ekaterinburg  Krasnodar  Novosibirsk  Nizhnij Novgorod  Rostov-na-Donu  \\\n",
       "0             1          0            0                 0               0   \n",
       "1             0          0            0                 0               0   \n",
       "2             0          0            0                 0               1   \n",
       "3             0          0            0                 0               0   \n",
       "4             0          0            0                 0               0   \n",
       "\n",
       "   CHeljabinsk  Kazan'  Perm'  Samara  Ufa     ...      Irkutsk  Orenburg  \\\n",
       "0            0       0      0       0    0     ...            0         0   \n",
       "1            0       0      0       1    0     ...            0         0   \n",
       "2            0       0      0       0    0     ...            0         0   \n",
       "3            0       0      0       0    0     ...            0         0   \n",
       "4            0       0      0       0    0     ...            0         0   \n",
       "\n",
       "   Izhevsk  Sochi  Tol'jatti  Kemerovo  Belgorod  Tula  Naberezhnye CHelny  \\\n",
       "0        0      0          0         0         0     0                   0   \n",
       "1        0      0          0         0         0     0                   0   \n",
       "2        0      0          0         0         0     0                   0   \n",
       "3        0      0          0         0         0     0                   1   \n",
       "4        0      0          0         0         0     0                   0   \n",
       "\n",
       "   Stavropol'  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best = search.best_estimator_.named_steps['reduce_dim']\n",
    "# Now fit Lasso with all the data\n",
    "best.fit(train_features,train.deal_probability)\n",
    "# Keep the best features\n",
    "train_features = train_features.loc[:,best.get_support()]\n",
    "# City variable no longer needed\n",
    "train.drop('city_en',axis=1,inplace=True)\n",
    "\n",
    "display(train_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = pd.DataFrame(index=test.index)\n",
    "features = train_features.columns.tolist()\n",
    "\n",
    "# Create features \n",
    "for city in features:\n",
    "    test_features[str(city)] = np.where(test.city_en == city,1,0)\n",
    "test.drop('city_en',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features: Param_1, 2 & 3\n",
    "- Features from these are challenging because some values repeat in more than one of these variables. Therefore when creating dummies, column names overlap. Adding prefixes helps but some values exist only in train or test set, therefore each set ends with a different number of features. The solution is to gather a custom list of values to create features from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Param_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features above 0.01 Variance: 18\n"
     ]
    }
   ],
   "source": [
    "# Find popular common values, create dummies, and get n_features\n",
    "\n",
    "# Find unique common values in both sets\n",
    "var = 'param_1_en'\n",
    "test_unique = test[var].unique()\n",
    "train_unique = train[var].unique()\n",
    "common = [unique for unique in train_unique if unique in test_unique]\n",
    "del test_unique,train_unique\n",
    "\n",
    "# Find the top popular of those common values\n",
    "train_common = train[train[var].apply(lambda x: x in common)][var]\n",
    "test_common = test[test[var].apply(lambda x: x in common)][var]\n",
    "traintest_common = train_common.append(test_common)\n",
    "top_common = traintest_common.value_counts()[:150].keys().tolist()\n",
    "del common,train_common,test_common,traintest_common\n",
    "\n",
    "# Make dummies from top common values\n",
    "new_features = pd.DataFrame(index=train.index)\n",
    "for common in top_common:\n",
    "    new_features[str(common)] = np.where(train[var] == common,1,0)\n",
    "del top_common\n",
    "\n",
    "# How many dummies > 1% variance?\n",
    "selector = feature_selection.VarianceThreshold(0.01)\n",
    "n_features = selector.fit_transform(new_features).shape[1]\n",
    "print('Number of Features above 0.01 Variance:', n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectFromModel(estimator=ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None,\n",
      "          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "          min_samples_leaf=1, min_samples_split=2,\n",
      "          min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
      "          oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
      "        max_features=18, norm_order=1, prefit=False, threshold=-inf)\n"
     ]
    }
   ],
   "source": [
    "# Compare feature selection methods\n",
    "estimator1 = ensemble.ExtraTreesRegressor()\n",
    "selector1 = feature_selection.SelectFromModel(\n",
    "    estimator1,\n",
    "    threshold=-np.inf,\n",
    "    max_features=n_features\n",
    ")\n",
    "estimator2 = linear_model.Lasso()\n",
    "selector2 = feature_selection.SelectFromModel(\n",
    "    estimator2,\n",
    "    threshold=-np.inf,\n",
    "    max_features=n_features\n",
    ")\n",
    "estimator3 = feature_selection.f_regression\n",
    "selector3 = feature_selection.SelectKBest(\n",
    "    estimator3,\n",
    "    k=n_features\n",
    ")\n",
    "# Based on generic linearregression, which feature-set has the best score?\n",
    "model = linear_model.LinearRegression()\n",
    "pipe = Pipeline([('reduce_dim', None),('regression', model)])\n",
    "grid = {'reduce_dim': [selector1,selector2,selector3]}\n",
    "search = model_selection.GridSearchCV(pipe, cv=4, n_jobs=-1, param_grid=grid)\n",
    "\n",
    "# Undersample data for quick testing\n",
    "index = np.random.choice(len(train),size=int(len(train)/7))\n",
    "search.fit(new_features.iloc[index],train.iloc[index].deal_probability)\n",
    "best = search.best_estimator_.named_steps['reduce_dim']\n",
    "print(best)\n",
    "del index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extra trees regressor gave the best 18 features.\n",
    "- In the next step, I won't fit all the data again because ensemble methods are time consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the best features\n",
    "new_features = new_features.loc[:,best.get_support()]\n",
    "# Add new features to train_features\n",
    "train_features = train_features.join(new_features)\n",
    "# Variable no longer needed\n",
    "train.drop(var,axis=1,inplace=True)\n",
    "\n",
    "features = new_features.columns.tolist()\n",
    "\n",
    "# Create features \n",
    "for f in features:\n",
    "    test_features[str(f)] = np.where(test[var] == f,1,0)\n",
    "test.drop(var,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Param_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features above 0.01 Variance: 13\n"
     ]
    }
   ],
   "source": [
    "# Find popular common values, create dummies, and get n_features\n",
    "\n",
    "# Find unique common values in both sets\n",
    "var = 'param_2_en'\n",
    "test_unique = test[var].unique()\n",
    "train_unique = train[var].unique()\n",
    "common = [unique for unique in train_unique if unique in test_unique]\n",
    "del test_unique,train_unique\n",
    "\n",
    "# Find the top popular of those common values\n",
    "train_common = train[train[var].apply(lambda x: x in common)][var]\n",
    "test_common = test[test[var].apply(lambda x: x in common)][var]\n",
    "traintest_common = train_common.append(test_common)\n",
    "top_common = traintest_common.value_counts()[:50].keys().tolist()\n",
    "del common,train_common,test_common,traintest_common\n",
    "\n",
    "# Make dummies from top common values\n",
    "new_features = pd.DataFrame(index=train.index)\n",
    "for common in top_common:\n",
    "    new_features[str(common)] = np.where(train[var] == common,1,0)\n",
    "del top_common\n",
    "\n",
    "# How many dummies > 1% variance?\n",
    "selector = feature_selection.VarianceThreshold(0.01)\n",
    "n_features = selector.fit_transform(new_features).shape[1]\n",
    "print('Number of Features above 0.01 Variance:', n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectKBest(k=13, score_func=<function f_regression at 0x7fbd61ef6620>)\n"
     ]
    }
   ],
   "source": [
    "# Compare feature selection methods\n",
    "estimator1 = ensemble.ExtraTreesRegressor()\n",
    "selector1 = feature_selection.SelectFromModel(\n",
    "    estimator1,\n",
    "    threshold=-np.inf,\n",
    "    max_features=n_features\n",
    ")\n",
    "estimator2 = linear_model.Lasso()\n",
    "selector2 = feature_selection.SelectFromModel(\n",
    "    estimator2,\n",
    "    threshold=-np.inf,\n",
    "    max_features=n_features\n",
    ")\n",
    "estimator3 = feature_selection.f_regression\n",
    "selector3 = feature_selection.SelectKBest(\n",
    "    estimator3,\n",
    "    k=n_features\n",
    ")\n",
    "# Based on generic linearregression, which feature-set has the best score?\n",
    "model = linear_model.LinearRegression()\n",
    "pipe = Pipeline([('reduce_dim', None),('regression', model)])\n",
    "grid = {'reduce_dim': [selector1,selector2,selector3]}\n",
    "search = model_selection.GridSearchCV(pipe, cv=4, n_jobs=-1, param_grid=grid)\n",
    "\n",
    "# Undersample data for quick testing\n",
    "index = np.random.choice(len(train),size=int(len(train)/7))\n",
    "search.fit(new_features.iloc[index],train.iloc[index].deal_probability)\n",
    "best = search.best_estimator_.named_steps['reduce_dim']\n",
    "print(best)\n",
    "del index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the best features\n",
    "new_features = new_features.loc[:,best.get_support()]\n",
    "# Add new features to train_features\n",
    "train_features = train_features.join(new_features)\n",
    "# Variable no longer needed\n",
    "train.drop(var,axis=1,inplace=True)\n",
    "\n",
    "features = new_features.columns.tolist()\n",
    "\n",
    "# Create features \n",
    "for f in features:\n",
    "    test_features[str(f)] = np.where(test[var] == f,1,0)\n",
    "test.drop(var,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Param_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features above 0.01 Variance: 12\n"
     ]
    }
   ],
   "source": [
    "# Find popular common values, create dummies, and get n_features\n",
    "\n",
    "# Find unique common values in both sets\n",
    "var = 'param_3_en'\n",
    "test_unique = test[var].unique()\n",
    "train_unique = train[var].unique()\n",
    "common = [unique for unique in train_unique if unique in test_unique]\n",
    "del test_unique,train_unique\n",
    "\n",
    "# Find the top popular of those common values\n",
    "train_common = train[train[var].apply(lambda x: x in common)][var]\n",
    "test_common = test[test[var].apply(lambda x: x in common)][var]\n",
    "traintest_common = train_common.append(test_common)\n",
    "top_common = traintest_common.value_counts()[:50].keys().tolist()\n",
    "del common,train_common,test_common,traintest_common\n",
    "\n",
    "# Make dummies from top common values\n",
    "new_features = pd.DataFrame(index=train.index)\n",
    "for common in top_common:\n",
    "    new_features[str(common)] = np.where(train[var] == common,1,0)\n",
    "del top_common\n",
    "\n",
    "# How many dummies > 1% variance?\n",
    "selector = feature_selection.VarianceThreshold(0.01)\n",
    "n_features = selector.fit_transform(new_features).shape[1]\n",
    "print('Number of Features above 0.01 Variance:', n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectKBest(k=12, score_func=<function f_regression at 0x7fbd61ef6620>)\n"
     ]
    }
   ],
   "source": [
    "# Compare feature selection methods\n",
    "estimator1 = ensemble.ExtraTreesRegressor()\n",
    "selector1 = feature_selection.SelectFromModel(\n",
    "    estimator1,\n",
    "    threshold=-np.inf,\n",
    "    max_features=n_features\n",
    ")\n",
    "estimator2 = linear_model.Lasso()\n",
    "selector2 = feature_selection.SelectFromModel(\n",
    "    estimator2,\n",
    "    threshold=-np.inf,\n",
    "    max_features=n_features\n",
    ")\n",
    "estimator3 = feature_selection.f_regression\n",
    "selector3 = feature_selection.SelectKBest(\n",
    "    estimator3,\n",
    "    k=n_features\n",
    ")\n",
    "# Based on generic linearregression, which feature-set has the best score?\n",
    "model = linear_model.LinearRegression()\n",
    "pipe = Pipeline([('reduce_dim', None),('regression', model)])\n",
    "grid = {'reduce_dim': [selector1,selector2,selector3]}\n",
    "search = model_selection.GridSearchCV(pipe, cv=4, n_jobs=-1, param_grid=grid)\n",
    "\n",
    "# Undersample data for quick testing\n",
    "index = np.random.choice(len(train),size=int(len(train)/7))\n",
    "search.fit(new_features.iloc[index],train.iloc[index].deal_probability)\n",
    "best = search.best_estimator_.named_steps['reduce_dim']\n",
    "print(best)\n",
    "del index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the best features\n",
    "new_features = new_features.loc[:,best.get_support()]\n",
    "# Add new features to train_features\n",
    "train_features = train_features.join(new_features,rsuffix=var)\n",
    "# Variable no longer needed\n",
    "train.drop(var,axis=1,inplace=True)\n",
    "\n",
    "features = new_features.columns.tolist()\n",
    "\n",
    "# Create features \n",
    "for f in features:\n",
    "    test_features[str(f)] = np.where(test[var] == f,1,0)\n",
    "test.drop(var,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features: Region, Parent Category, Category\n",
    "- Since these have very few unique values compared to `city` or `param_x`, I'll filter them out at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN\n",
      "Unique values for region_en:  28\n",
      "TEST\n",
      "Unique values for region_en:  28\n",
      "\n",
      "TRAIN\n",
      "Unique values for parent_category_name_en:  9\n",
      "TEST\n",
      "Unique values for parent_category_name_en:  9\n",
      "\n",
      "TRAIN\n",
      "Unique values for category_name_en:  47\n",
      "TEST\n",
      "Unique values for category_name_en:  47\n"
     ]
    }
   ],
   "source": [
    "cat_vars = ['region_en','parent_category_name_en','category_name_en']\n",
    "for var in cat_vars:\n",
    "    print('\\nTRAIN')\n",
    "    print('Unique values for {}: '.format(var),len(np.unique(train[var])))\n",
    "    print('TEST')\n",
    "    print('Unique values for {}: '.format(var),len(np.unique(test[var])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data in train and test is the same, therefore it's not necessary to find common unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features above 0.01 Variance: 56\n"
     ]
    }
   ],
   "source": [
    "# Categorical variables for dummies, except city.\n",
    "cat_vars = ['region_en','parent_category_name_en','category_name_en']\n",
    "\n",
    "new_features = pd.DataFrame(index=train.index)\n",
    "\n",
    "for var in cat_vars:\n",
    "    for f in np.unique(test[var]):\n",
    "        # Make dummies\n",
    "        new_features[str(f)] = np.where(train[var] == f,1,0)\n",
    "\n",
    "# How many dummies > 1% variance?\n",
    "selector = feature_selection.VarianceThreshold(0.01)\n",
    "n_features = selector.fit_transform(new_features).shape[1]\n",
    "print('Number of Features above 0.01 Variance:', n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 56 out of 84 new features will be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectKBest(k=56, score_func=<function f_regression at 0x7fbd61ef6620>)\n"
     ]
    }
   ],
   "source": [
    "# Compare feature selection methods\n",
    "selector1.max_features,selector2.max_features,selector3.k = np.repeat(n_features,3)\n",
    "\n",
    "# Based on generic linearregression, which feature-set has the best score?\n",
    "model = linear_model.LinearRegression()\n",
    "pipe = Pipeline([('reduce_dim', None),('regression', model)])\n",
    "grid = {'reduce_dim': [selector1,selector2,selector3]}\n",
    "search = model_selection.GridSearchCV(pipe, cv=4, n_jobs=-1, param_grid=grid)\n",
    "\n",
    "# Undersample data for quick testing\n",
    "index = np.random.choice(len(train),size=int(len(train)/7))\n",
    "search.fit(new_features.iloc[index],train.iloc[index].deal_probability)\n",
    "best = search.best_estimator_.named_steps['reduce_dim']\n",
    "print(best)\n",
    "del index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the best features\n",
    "new_features = new_features.loc[:,best.get_support()]\n",
    "# Add new features to train_features\n",
    "train_features = train_features.join(new_features)\n",
    "# Variable no longer needed\n",
    "for var in cat_vars:\n",
    "    train.drop(var,axis=1,inplace=True)\n",
    "\n",
    "features = new_features.columns.tolist()\n",
    "\n",
    "for var in cat_vars:\n",
    "    for f in features:\n",
    "        # Make dummies\n",
    "        test_features[str(f)] = np.where(test[var] == f,1,0)\n",
    "    test.drop(var,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('train_features', 1539506280),\n",
       " ('train', 1377250683),\n",
       " ('new_features', 673534056),\n",
       " ('test_features', 516573112),\n",
       " ('test', 478668977),\n",
       " ('Pipeline', 1056),\n",
       " ('features', 512),\n",
       " ('grid', 240),\n",
       " ('color', 176),\n",
       " ('estimator3', 136),\n",
       " ('city', 103),\n",
       " ('cat_vars', 88),\n",
       " ('ensemble', 80),\n",
       " ('feature_selection', 80),\n",
       " ('lgb', 80),\n",
       " ('linear_model', 80),\n",
       " ('metrics', 80),\n",
       " ('model_selection', 80),\n",
       " ('np', 80),\n",
       " ('pd', 80),\n",
       " ('plt', 80),\n",
       " ('preprocessing', 80),\n",
       " ('sns', 80),\n",
       " ('f', 67),\n",
       " ('var', 65),\n",
       " ('common', 63),\n",
       " ('best', 56),\n",
       " ('estimator1', 56),\n",
       " ('estimator2', 56),\n",
       " ('model', 56),\n",
       " ('pipe', 56),\n",
       " ('search', 56),\n",
       " ('selector', 56),\n",
       " ('selector1', 56),\n",
       " ('selector2', 56),\n",
       " ('selector3', 56),\n",
       " ('key', 54),\n",
       " ('a', 32),\n",
       " ('b', 32),\n",
       " ('c', 32),\n",
       " ('d', 28),\n",
       " ('n_features', 28)]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# These are the usual ipython objects, including this one you are creating\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Get a sorted list of the objects and their sizes\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['item_id', 'user_id', 'title', 'description', 'price',\n",
       "       'item_seq_number', 'activation_date', 'user_type', 'image',\n",
       "       'image_top_1', 'deal_probability'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lgb(train_X, train_y, val_X, val_y, test_X):\n",
    "    params = {\n",
    "        \"objective\" : \"regression\",\n",
    "        \"metric\" : \"rmse\",\n",
    "        \"num_leaves\" : 30,\n",
    "        \"learning_rate\" : 0.1,\n",
    "        \"bagging_fraction\" : 0.7,\n",
    "        \"feature_fraction\" : 0.7,\n",
    "        \"bagging_frequency\" : 5,\n",
    "        \"bagging_seed\" : 2018,\n",
    "        \"verbosity\" : -1\n",
    "    }\n",
    "    \n",
    "    lgtrain = lgb.Dataset(train_X, label=train_y)\n",
    "    lgval = lgb.Dataset(val_X, label=val_y)\n",
    "    evals_result = {}\n",
    "    model = lgb.train(params, lgtrain, 1000, valid_sets=[lgval], early_stopping_rounds=100, verbose_eval=20, evals_result=evals_result)\n",
    "    \n",
    "    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "    return pred_test_y, model, evals_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add more Features\n",
    "\n",
    "- (Feature) has description\n",
    "- (Feature) has photo\n",
    "- (Feature) has param 1,2,3\n",
    "- (Feature) has price\n",
    "- (Feature) word count in title, description,\n",
    "- (Feature) population of region\n",
    "- (Feature) string value is unique. title, description, param1, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New variable on weekday #\n",
    "#train[\"activation_weekday\"] = train[\"activation_date\"].dt.weekday\n",
    "#test[\"activation_weekday\"] = test[\"activation_date\"].dt.weekday\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
