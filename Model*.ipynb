{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import cyrtranslit\n",
    "from sklearn import preprocessing, model_selection, metrics, feature_selection, ensemble, linear_model\n",
    "from sklearn.pipeline import Pipeline\n",
    "import lightgbm as lgb\n",
    "color = sns.color_palette()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"https://onedrive.live.com/download?cid=62B3CEE436FDB342&resid=62B3CEE436FDB342%21107&authkey=AEh-8Y6p9SC7FK0\",\n",
    "                      compression='zip', parse_dates=[\"activation_date\"])\n",
    "test = pd.read_csv(\"https://onedrive.live.com/download?cid=62B3CEE436FDB342&resid=62B3CEE436FDB342%21106&authkey=AAF_zwBmWjNhNGQ\",\n",
    "                      compression='zip', parse_dates=[\"activation_date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Since the test data has no labels, we'll validate with two subsets of train. Lastly we'll use test to generate a submission file and get a public score.\n",
    "\n",
    "## Basic Feature Engineering\n",
    "\n",
    "- Translate all textual features into latin.\n",
    "- Create dummies from categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1503424 entries, 0 to 1503423\n",
      "Data columns (total 18 columns):\n",
      "item_id                 1503424 non-null object\n",
      "user_id                 1503424 non-null object\n",
      "region                  1503424 non-null object\n",
      "city                    1503424 non-null object\n",
      "parent_category_name    1503424 non-null object\n",
      "category_name           1503424 non-null object\n",
      "param_1                 1441848 non-null object\n",
      "param_2                 848882 non-null object\n",
      "param_3                 640859 non-null object\n",
      "title                   1503424 non-null object\n",
      "description             1387148 non-null object\n",
      "price                   1418062 non-null float64\n",
      "item_seq_number         1503424 non-null int64\n",
      "activation_date         1503424 non-null datetime64[ns]\n",
      "user_type               1503424 non-null object\n",
      "image                   1390836 non-null object\n",
      "image_top_1             1390836 non-null float64\n",
      "deal_probability        1503424 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(3), int64(1), object(13)\n",
      "memory usage: 206.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Title and description have too many unique values, therefore we won't translate them all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique regions: 28\n",
      "Number of unique cities: 1733\n",
      "Number of unique parent categories: 9\n",
      "Number of unique categories: 47\n",
      "Number of unique descriptions: 1317102\n",
      "Number of unique titles: 788377\n",
      "Number of unique param_1: 371\n",
      "Number of unique param_2: 271\n",
      "Number of unique param_3 1219\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique regions:',len(train.region.value_counts()))\n",
    "print('Number of unique cities:',len(train.city.value_counts()))\n",
    "print('Number of unique parent categories:',len(train.parent_category_name.value_counts()))\n",
    "print('Number of unique categories:',len(train.category_name.value_counts()))\n",
    "print('Number of unique descriptions:',len(train.description.value_counts()))\n",
    "print('Number of unique titles:',len(train.title.value_counts()))\n",
    "print('Number of unique param_1:',len(train.param_1.value_counts()))\n",
    "print('Number of unique param_2:',len(train.param_2.value_counts()))\n",
    "print('Number of unique param_3',len(train.param_3.value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description and title have too many unique values,\n",
    "# Therefore this method would take too long.\n",
    "cyr_vars = ['region','city','parent_category_name','category_name',\n",
    "           'param_1','param_2','param_3']\n",
    "\n",
    "for var in cyr_vars:\n",
    "    for dataset in [train,test]:\n",
    "        # Get unique cyrilic vlaues\n",
    "        cyrilic_unique = np.unique(dataset[var].fillna('Blank')).tolist()\n",
    "        # Get unique latin translations\n",
    "        latin_unique = [cyrtranslit.to_latin(string,'ru') for string in cyrilic_unique]\n",
    "\n",
    "        # Put lists in a dictionary\n",
    "        trans_dict = {}\n",
    "        for cyr, lat in zip(cyrilic_unique,latin_unique):\n",
    "            trans_dict[cyr]=lat\n",
    "\n",
    "        # Create a translated list\n",
    "        en_list = []\n",
    "        for key in dataset[var].fillna('Blank'):\n",
    "            en_list.append(trans_dict[key])\n",
    "\n",
    "        # Add english list as column\n",
    "        dataset[str(var)+'_en'] = en_list\n",
    "        dataset.drop(var,axis=1,inplace=True)\n",
    "\n",
    "del cyrilic_unique,latin_unique,trans_dict,en_list,dataset,cyr_vars,var,cyr,lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region_en</th>\n",
       "      <th>city_en</th>\n",
       "      <th>parent_category_name_en</th>\n",
       "      <th>category_name_en</th>\n",
       "      <th>param_1_en</th>\n",
       "      <th>param_2_en</th>\n",
       "      <th>param_3_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sverdlovskaja oblast'</td>\n",
       "      <td>Ekaterinburg</td>\n",
       "      <td>Lichnye veszi</td>\n",
       "      <td>Tovary dlja detej i igrushki</td>\n",
       "      <td>Postel'nye prinadlezhnosti</td>\n",
       "      <td>Blank</td>\n",
       "      <td>Blank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Samarskaja oblast'</td>\n",
       "      <td>Samara</td>\n",
       "      <td>Dlja doma i dachi</td>\n",
       "      <td>Mebel' i inter'er</td>\n",
       "      <td>Drugoe</td>\n",
       "      <td>Blank</td>\n",
       "      <td>Blank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rostovskaja oblast'</td>\n",
       "      <td>Rostov-na-Donu</td>\n",
       "      <td>Bytovaja ehlektronika</td>\n",
       "      <td>Audio i video</td>\n",
       "      <td>Video, DVD i Blu-ray pleery</td>\n",
       "      <td>Blank</td>\n",
       "      <td>Blank</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               region_en         city_en parent_category_name_en  \\\n",
       "0  Sverdlovskaja oblast'    Ekaterinburg           Lichnye veszi   \n",
       "1     Samarskaja oblast'          Samara       Dlja doma i dachi   \n",
       "2    Rostovskaja oblast'  Rostov-na-Donu   Bytovaja ehlektronika   \n",
       "\n",
       "               category_name_en                   param_1_en param_2_en  \\\n",
       "0  Tovary dlja detej i igrushki   Postel'nye prinadlezhnosti      Blank   \n",
       "1             Mebel' i inter'er                       Drugoe      Blank   \n",
       "2                 Audio i video  Video, DVD i Blu-ray pleery      Blank   \n",
       "\n",
       "  param_3_en  \n",
       "0      Blank  \n",
       "1      Blank  \n",
       "2      Blank  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region_en</th>\n",
       "      <th>city_en</th>\n",
       "      <th>parent_category_name_en</th>\n",
       "      <th>category_name_en</th>\n",
       "      <th>param_1_en</th>\n",
       "      <th>param_2_en</th>\n",
       "      <th>param_3_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Volgogradskaja oblast'</td>\n",
       "      <td>Volgograd</td>\n",
       "      <td>Lichnye veszi</td>\n",
       "      <td>Detskaja odezhda i obuv'</td>\n",
       "      <td>Dlja mal'chikov</td>\n",
       "      <td>Obuv'</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sverdlovskaja oblast'</td>\n",
       "      <td>Nizhnjaja Tura</td>\n",
       "      <td>Hobbi i otdyh</td>\n",
       "      <td>Velosipedy</td>\n",
       "      <td>Dorozhnye</td>\n",
       "      <td>Blank</td>\n",
       "      <td>Blank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Novosibirskaja oblast'</td>\n",
       "      <td>Berdsk</td>\n",
       "      <td>Bytovaja ehlektronika</td>\n",
       "      <td>Audio i video</td>\n",
       "      <td>Televizory i proektory</td>\n",
       "      <td>Blank</td>\n",
       "      <td>Blank</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                region_en         city_en parent_category_name_en  \\\n",
       "0  Volgogradskaja oblast'       Volgograd           Lichnye veszi   \n",
       "1   Sverdlovskaja oblast'  Nizhnjaja Tura           Hobbi i otdyh   \n",
       "2  Novosibirskaja oblast'          Berdsk   Bytovaja ehlektronika   \n",
       "\n",
       "           category_name_en              param_1_en param_2_en param_3_en  \n",
       "0  Detskaja odezhda i obuv'         Dlja mal'chikov      Obuv'         25  \n",
       "1                Velosipedy               Dorozhnye      Blank      Blank  \n",
       "2             Audio i video  Televizory i proektory      Blank      Blank  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# See latin translations\n",
    "print('Train Data:')\n",
    "display(train.iloc[:3,-7:])\n",
    "print(\"\")\n",
    "print('Test Data:')\n",
    "display(test.iloc[:3,-7:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummies\n",
    "### City Features\n",
    "- The number of unique cities in train is too large (1700) and leads to `MemoryError`. Let's pick a subset of those cities. The most useful might be those which appear in both train and test. Cities appearing only in train won't help predict anything in test that regards the city variable, and listings from cities which appear only in test can't be predicted with train information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique comon cities: 1625\n",
      "N Listings in both sets from unique common cities: 2011447\n"
     ]
    }
   ],
   "source": [
    "# Find which cities are in both train and test sets\n",
    "test_unique = test.city_en.unique()\n",
    "train_unique = train.city_en.unique()\n",
    "\n",
    "common = [city for city in train_unique if city in test_unique]\n",
    "del test_unique,train_unique\n",
    "\n",
    "print('Unique comon cities:',len(common))\n",
    "\n",
    "# Create features from the most popular cities in common set\n",
    "\n",
    "# Get common cities in train and test\n",
    "train_common = train[train.city_en.apply(lambda x: x in common)].city_en\n",
    "test_common = test[test.city_en.apply(lambda x: x in common)].city_en\n",
    "\n",
    "# Merge sets of common cities\n",
    "traintest_common = train_common.append(test_common)\n",
    "print('N Listings in both sets from unique common cities:',traintest_common.shape[0])\n",
    "\n",
    "# Count values among merged set\n",
    "top_common = traintest_common.value_counts()[:150].keys().tolist()\n",
    "del common,train_common,test_common,traintest_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1625 is still too many cities for feature purposes. Let's find which of those are the most popular among both sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we'll store our machine learning features\n",
    "train_features = pd.DataFrame(index=train.index)\n",
    "#test_features = pd.DataFrame(index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in top_common:\n",
    "    train_features[str(city)] = np.where(train.city_en == city,1,0)\n",
    "    \n",
    "del top_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection of Cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features above 0.01 Variance: 29\n"
     ]
    }
   ],
   "source": [
    "selector = feature_selection.VarianceThreshold(0.01)\n",
    "n_features = selector.fit_transform(train_features).shape[1]\n",
    "print('Number of Features above 0.01 Variance:', n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Variance Threshold doesn't take into account the target variable. At 0.01 variance, it removes all but 29 of cities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Feature Selection Techniques\n",
    "- Feature Importances, Lasso L1, F Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/user/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/user/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/user/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/user/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/user/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_reduce_dim</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112.951528</td>\n",
       "      <td>1.562672</td>\n",
       "      <td>0.024918</td>\n",
       "      <td>0.005180</td>\n",
       "      <td>SelectFromModel(estimator=ExtraTreesRegressor(...</td>\n",
       "      <td>{'reduce_dim': SelectFromModel(estimator=Extra...</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.001330</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>0.002429</td>\n",
       "      <td>0.002279</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.000082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.159754</td>\n",
       "      <td>0.157788</td>\n",
       "      <td>0.033348</td>\n",
       "      <td>0.016042</td>\n",
       "      <td>SelectFromModel(estimator=Lasso(alpha=1.0, cop...</td>\n",
       "      <td>{'reduce_dim': SelectFromModel(estimator=Lasso...</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.001633</td>\n",
       "      <td>0.001651</td>\n",
       "      <td>0.001602</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.755045</td>\n",
       "      <td>0.164839</td>\n",
       "      <td>0.020064</td>\n",
       "      <td>0.007867</td>\n",
       "      <td>SelectKBest(k=29, score_func=&lt;function f_regre...</td>\n",
       "      <td>{'reduce_dim': SelectKBest(k=29, score_func=&lt;f...</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.001160</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>3</td>\n",
       "      <td>0.002184</td>\n",
       "      <td>0.002225</td>\n",
       "      <td>0.002417</td>\n",
       "      <td>0.002243</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0     112.951528      1.562672         0.024918        0.005180   \n",
       "1       1.159754      0.157788         0.033348        0.016042   \n",
       "2       0.755045      0.164839         0.020064        0.007867   \n",
       "\n",
       "                                    param_reduce_dim  \\\n",
       "0  SelectFromModel(estimator=ExtraTreesRegressor(...   \n",
       "1  SelectFromModel(estimator=Lasso(alpha=1.0, cop...   \n",
       "2  SelectKBest(k=29, score_func=<function f_regre...   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'reduce_dim': SelectFromModel(estimator=Extra...           0.001398   \n",
       "1  {'reduce_dim': SelectFromModel(estimator=Lasso...           0.001468   \n",
       "2  {'reduce_dim': SelectKBest(k=29, score_func=<f...           0.001384   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
       "0           0.000977           0.000657           0.001330         0.001090   \n",
       "1           0.001270           0.001236           0.001134         0.001277   \n",
       "2           0.001010           0.000583           0.001160         0.001034   \n",
       "\n",
       "   std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0        0.000297                2            0.002203            0.002274   \n",
       "1        0.000121                1            0.001529            0.001595   \n",
       "2        0.000293                3            0.002184            0.002225   \n",
       "\n",
       "   split2_train_score  split3_train_score  mean_train_score  std_train_score  \n",
       "0            0.002429            0.002279          0.002296         0.000082  \n",
       "1            0.001633            0.001651          0.001602         0.000047  \n",
       "2            0.002417            0.002243          0.002267         0.000089  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "estimator1 = ensemble.ExtraTreesRegressor()\n",
    "selector1 = feature_selection.SelectFromModel(\n",
    "    estimator1,\n",
    "    threshold=-np.inf,\n",
    "    max_features=n_features\n",
    ")\n",
    "\n",
    "estimator2 = linear_model.Lasso()\n",
    "selector2 = feature_selection.SelectFromModel(\n",
    "    estimator2,\n",
    "    threshold=-np.inf,\n",
    "    max_features=n_features\n",
    ")\n",
    "\n",
    "estimator3 = feature_selection.f_regression\n",
    "selector3 = feature_selection.SelectKBest(\n",
    "    estimator3,\n",
    "    k=n_features\n",
    ")\n",
    "\n",
    "model = linear_model.LinearRegression()\n",
    "\n",
    "pipe = Pipeline([('reduce_dim', None),('regression', model)])\n",
    "\n",
    "grid = {'reduce_dim': [selector1,selector2,selector3]}\n",
    "\n",
    "search = GridSearchCV(pipe, cv=4, n_jobs=-1, param_grid=grid)\n",
    "\n",
    "index = np.random.choice(len(train),size=int(len(train)/7))\n",
    "search.fit(train_features.iloc[index],train.iloc[index].deal_probability)\n",
    "\n",
    "print(search.best_estimator_.named_steps['reduce_dim'].estimator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lasso returned the best features, based on the LinearRegression default score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = search.best_estimator_.named_steps['reduce_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1503424, 29)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.fit_transform(train_features,train.deal_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del var_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('train', 2177612018),\n",
       " ('train_features', 1804108904),\n",
       " ('index', 1718288),\n",
       " ('GridSearchCV', 1464),\n",
       " ('top_common', 1264),\n",
       " ('Pipeline', 1056),\n",
       " ('var_f', 296),\n",
       " ('grid', 240),\n",
       " ('color', 176),\n",
       " ('estimator3', 136),\n",
       " ('city', 83),\n",
       " ('ensemble', 80),\n",
       " ('feature_selection', 80),\n",
       " ('lgb', 80),\n",
       " ('linear_model', 80),\n",
       " ('metrics', 80),\n",
       " ('model_selection', 80),\n",
       " ('np', 80),\n",
       " ('pd', 80),\n",
       " ('plt', 80),\n",
       " ('preprocessing', 80),\n",
       " ('sns', 80),\n",
       " ('best', 56),\n",
       " ('estimator', 56),\n",
       " ('estimator1', 56),\n",
       " ('estimator2', 56),\n",
       " ('model', 56),\n",
       " ('pipe', 56),\n",
       " ('search', 56),\n",
       " ('selector', 56),\n",
       " ('selector1', 56),\n",
       " ('selector2', 56),\n",
       " ('selector3', 56),\n",
       " ('key', 54),\n",
       " ('n_features', 28)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# These are the usual ipython objects, including this one you are creating\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Get a sorted list of the objects and their sizes\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features: Param_1, 2 & 3\n",
    "- Features from these are challenging because some values repeat in more than one of these variables. Therefore when creating dummies, column names overlap. Adding prefixes helps but some values exist only in train or test set, therefore each set ends with a different number of features. The solution is to gather a custom list of values to create features from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features: Region, Parent Category, Category\n",
    "- These can be done without as much juggling as `city` or `param_x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Categorical variables for dummies, except city.\n",
    "cat_vars = train.columns[-7:].drop('city_en')\n",
    "\n",
    "# Get dummies in both train and test\n",
    "for var in cat_vars:\n",
    "    for dataset in [train,test]:\n",
    "        dummies = pd.get_dummies(dataset[var],prefix=var)\n",
    "        if len(dataset) == len(train):\n",
    "            train_features = train_features.join(dummies)\n",
    "        elif len(dataset) == len(test):\n",
    "            test_features = test_features.join(dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train[['price_fill','item_seq_number','activation_date''image_top_1','deal_probability']].copy()\n",
    "test_features = test[['price_fill','item_seq_number','activation_date''image_top_1']].copy()\n",
    "\n",
    "cat_vars = [\"region\", \"city\", \"parent_category_name\", \"category_name\", \"user_type\", \"param_1\", \"param_2\", \"param_3\"]\n",
    "for col in cat_vars:\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(train[col].values.astype('str')) + list(test[col].values.astype('str')))\n",
    "    train_features[col] = lbl.transform(list(train[col].values.astype('str')))\n",
    "    test_features[col] = lbl.transform(list(test[col].values.astype('str')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train[\"deal_probability\"].values\n",
    "\n",
    "# Label encode the categorical variables #\n",
    "cat_vars = [\"region\", \"city\", \"parent_category_name\", \"category_name\", \"user_type\", \"param_1\", \"param_2\", \"param_3\"]\n",
    "for col in cat_vars:\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(train_df[col].values.astype('str')) + list(test_df[col].values.astype('str')))\n",
    "    train_df[col] = lbl.transform(list(train_df[col].values.astype('str')))\n",
    "    test_df[col] = lbl.transform(list(test_df[col].values.astype('str')))\n",
    "\n",
    "cols_to_drop = [\"item_id\", \"user_id\", \"title\", \"description\", \"activation_date\", \"image\"]\n",
    "train_X = train_df.drop(cols_to_drop + [\"region_en\", \"parent_category_name_en\", \"category_name_en\", \"price_new\", \"deal_probability\"], axis=1)\n",
    "test_X = test_df.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lgb(train_X, train_y, val_X, val_y, test_X):\n",
    "    params = {\n",
    "        \"objective\" : \"regression\",\n",
    "        \"metric\" : \"rmse\",\n",
    "        \"num_leaves\" : 30,\n",
    "        \"learning_rate\" : 0.1,\n",
    "        \"bagging_fraction\" : 0.7,\n",
    "        \"feature_fraction\" : 0.7,\n",
    "        \"bagging_frequency\" : 5,\n",
    "        \"bagging_seed\" : 2018,\n",
    "        \"verbosity\" : -1\n",
    "    }\n",
    "    \n",
    "    lgtrain = lgb.Dataset(train_X, label=train_y)\n",
    "    lgval = lgb.Dataset(val_X, label=val_y)\n",
    "    evals_result = {}\n",
    "    model = lgb.train(params, lgtrain, 1000, valid_sets=[lgval], early_stopping_rounds=100, verbose_eval=20, evals_result=evals_result)\n",
    "    \n",
    "    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "    return pred_test_y, model, evals_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add more Features\n",
    "\n",
    "- (Feature) has description\n",
    "- (Feature) has photo\n",
    "- (Feature) has param 1,2,3\n",
    "- (Feature) has price\n",
    "- (Feature) word count in title, description,\n",
    "- (Feature) population of region\n",
    "- (Feature) string value is unique. title, description, param1, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New variable on weekday #\n",
    "#train[\"activation_weekday\"] = train[\"activation_date\"].dt.weekday\n",
    "#test[\"activation_weekday\"] = test[\"activation_date\"].dt.weekday\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
